{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180eaf69-31e6-4aec-a341-0e4a3497f572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 09:31:36.404407: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/packages/AI/tensorflow_23.02-2.10.0-py3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_probability as tfp; tfp = tfp.substrates.jax\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "from jaxlie import SO3\n",
    "from so3dm.distributions.isotropic_gaussian import IsotropicGaussianSO3\n",
    "from so3dm.plotting import visualize_so3_probabilities, visualize_so3_density\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "import jraph\n",
    "from jraph import GraphConvolution\n",
    "from jraph._src import utils as jraph_utils\n",
    "from jraph._src import graph as gn_graph\n",
    "\n",
    "import networkx as nx\n",
    "import jax.tree_util as tree\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb694466-e71a-4d8f-aa2a-b10ca6462380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_force_compilation_parallelism=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99054b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36109/1757885639.py:5: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0594215-29ad-4b41-a9a0-ad0c4fabf896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.inv(jnp.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a121dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = hk.PRNGSequence(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6263e366-af61-41c1-988c-63edddf0aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from halotools_ia.correlation_functions import  ed_3d, ee_3d,gi_plus_3d, gi_plus_projected, ii_minus_3d, ii_minus_projected, ii_plus_3d, ii_plus_projected, ed_projected, ed_3d_one_two_halo_decomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee030ed-0c33-4497-958c-23ecb05d73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tng = pickle.load(  open('/jet/home/yjagvara/SO3Diffusion_Tidal/TNG100-1_99_non-reduced_galaxy_shapes_multi_scale_1024_MLP_only_cent.pkl', \"rb\" ) )\n",
    "tng = tng[tng['dm_mass']>0]\n",
    "tng = tng[log10(tng['dm_mass']*10**10)>9]\n",
    "tng = tng[log10(tng['mass']*10**10)>9]\n",
    "\n",
    "tng['mass'] = log10(tng['mass']*10**10)\n",
    "tng['mass'] = (tng['mass']  -jnp.mean(tng['mass']))/ jnp.std(tng['mass']) \n",
    "tng['dm_mass'] = log10(tng['dm_mass']*10**10)\n",
    "tng['dm_mass'] = (tng['dm_mass']  -jnp.mean(tng['dm_mass']))/ jnp.std(tng['dm_mass']) \n",
    "\n",
    "#tng = tng[tng['central_bool']==1.0]\n",
    "n_scalar = 2 #number of scalar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9538258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=17457</i>\n",
       "<table id=\"table23118742419248\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>gal_id</th><th>a</th><th>b</th><th>c</th><th>av_x</th><th>av_y</th><th>av_z</th><th>bv_x</th><th>bv_y</th><th>bv_z</th><th>cv_x</th><th>cv_y</th><th>cv_z</th><th>tid_av_x_0.1_1024</th><th>tid_av_y_0.1_1024</th><th>tid_av_z_0.1_1024</th><th>tid_bv_x_0.1_1024</th><th>tid_bv_y_0.1_1024</th><th>tid_bv_z_0.1_1024</th><th>tid_cv_x_0.1_1024</th><th>tid_cv_y_0.1_1024</th><th>tid_cv_z_0.1_1024</th><th>tid_a_0.1_1024</th><th>tid_b_0.1_1024</th><th>tid_c_0.1_1024</th><th>tid_av_x_0.5_1024</th><th>tid_av_y_0.5_1024</th><th>tid_av_z_0.5_1024</th><th>tid_bv_x_0.5_1024</th><th>tid_bv_y_0.5_1024</th><th>tid_bv_z_0.5_1024</th><th>tid_cv_x_0.5_1024</th><th>tid_cv_y_0.5_1024</th><th>tid_cv_z_0.5_1024</th><th>tid_a_0.5_1024</th><th>tid_b_0.5_1024</th><th>tid_c_0.5_1024</th><th>tid_av_x_1.0_1024</th><th>tid_av_y_1.0_1024</th><th>tid_av_z_1.0_1024</th><th>tid_bv_x_1.0_1024</th><th>tid_bv_y_1.0_1024</th><th>tid_bv_z_1.0_1024</th><th>tid_cv_x_1.0_1024</th><th>tid_cv_y_1.0_1024</th><th>tid_cv_z_1.0_1024</th><th>tid_a_1.0_1024</th><th>tid_b_1.0_1024</th><th>tid_c_1.0_1024</th><th>tid_av_x_2.0_1024</th><th>tid_av_y_2.0_1024</th><th>tid_av_z_2.0_1024</th><th>tid_bv_x_2.0_1024</th><th>tid_bv_y_2.0_1024</th><th>tid_bv_z_2.0_1024</th><th>tid_cv_x_2.0_1024</th><th>tid_cv_y_2.0_1024</th><th>tid_cv_z_2.0_1024</th><th>tid_a_2.0_1024</th><th>tid_b_2.0_1024</th><th>tid_c_2.0_1024</th><th>mass</th><th>GroupID</th><th>tot_mass</th><th>dm_mass</th><th>central_bool</th><th>group_mass</th><th>group_x</th><th>group_y</th><th>group_z</th><th>gal_pos_x</th><th>gal_pos_y</th><th>gal_pos_z</th><th>dm_av_x</th><th>dm_av_y</th><th>dm_av_z</th><th>dm_bv_x</th><th>dm_bv_y</th><th>dm_bv_z</th><th>dm_cv_x</th><th>dm_cv_y</th><th>dm_cv_z</th><th>dm_a</th><th>dm_b</th><th>dm_c</th><th>mlp_av_x</th><th>mlp_av_y</th><th>mlp_av_z</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>0.0</td><td>0.24562844247483243</td><td>0.11233008826530698</td><td>0.09435347356932028</td><td>0.15057356009311798</td><td>-0.36432524666271543</td><td>0.9190183445639349</td><td>-0.6040801058750838</td><td>0.7019725376704176</td><td>0.37725559246020707</td><td>0.7825693762771655</td><td>0.611965416507098</td><td>0.11438312949234405</td><td>0.8275753855705261</td><td>0.5501617810833461</td><td>-0.11153932645318682</td><td>-0.4364806852752775</td><td>0.7555949692944677</td><td>0.4884269175204382</td><td>0.3529923768186143</td><td>-0.355525325415916</td><td>0.8654467776217399</td><td>1054.244087225318</td><td>991.9027130436707</td><td>668.0261137005616</td><td>-0.9183833599090576</td><td>0.33972494317589014</td><td>0.20287679547321114</td><td>0.33534699111399274</td><td>0.9403888675675559</td><td>-0.05666721544068841</td><td>-0.2100343464963115</td><td>0.015991896046057438</td><td>-0.9775632115381181</td><td>59.10935058615099</td><td>55.377614125575626</td><td>37.975054282341716</td><td>0.9371282458305359</td><td>0.1784244452709745</td><td>-0.29992566920893826</td><td>-0.2053100234179685</td><td>0.9768323454844315</td><td>-0.06038512316371721</td><td>0.2822029128212493</td><td>0.11816634946180396</td><td>0.9520494891811419</td><td>13.122050507836377</td><td>11.700347539646893</td><td>9.282237732126015</td><td>-0.8248880505561829</td><td>0.33217499988277654</td><td>0.4574051124728157</td><td>0.5063412900419841</td><td>0.7939398479662237</td><td>0.3365679958195495</td><td>-0.2513526714837356</td><td>0.5092340201363678</td><td>-0.8231054290148635</td><td>2.5250096232145673</td><td>2.341083724345102</td><td>1.7858375325765965</td><td>4.7994876</td><td>0.0</td><td>27477.935546875</td><td>4.9791627</td><td>1.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8490914106369019</td><td>26.326995849609375</td><td>18.306934356689453</td><td>0.20913785733198287</td><td>-0.2376266091458571</td><td>0.9485752217175131</td><td>-0.0291868731156354</td><td>0.9680772186306577</td><td>0.24894703293283385</td><td>0.9774505015950008</td><td>0.07975019369363229</td><td>-0.19552601754628252</td><td>0.7422561814889702</td><td>0.380580342545896</td><td>0.2953753041796042</td><td>-0.8295962810516357</td><td>0.51895672082901</td><td>0.5298088788986206</td></tr>\n",
       "<tr><td>1.0</td><td>0.08088470650058953</td><td>0.04497213832247513</td><td>0.03777383880658522</td><td>-0.9840750455762398</td><td>-0.006859759853817557</td><td>0.17762108086843106</td><td>0.07866214907516236</td><td>0.8792771126813137</td><td>0.46977018361927686</td><td>-0.15940066178344028</td><td>0.4762611707976621</td><td>-0.8647350612837673</td><td>-0.25285616517066956</td><td>-0.542943046543665</td><td>-0.8007974784346137</td><td>-0.0015899779481896464</td><td>-0.8274600252025045</td><td>0.5615223759228073</td><td>0.9675025711735737</td><td>-0.14325765215776112</td><td>-0.20836511193280413</td><td>340.0297044488053</td><td>304.0921077455601</td><td>251.82406385313607</td><td>-0.43189844489097595</td><td>-0.6954925306839556</td><td>-0.5742419895334245</td><td>-0.7976906521230697</td><td>0.5916774294264403</td><td>-0.11665094522888154</td><td>-0.4208958853397719</td><td>-0.40768610384724513</td><td>0.810332459200483</td><td>19.602475741375493</td><td>17.40369819653049</td><td>5.434223834232313</td><td>0.46768128871917725</td><td>0.777220230781344</td><td>0.42095478906302564</td><td>0.802480656776426</td><td>-0.5730060672469052</td><td>0.1663996466279646</td><td>0.3705388199238754</td><td>0.25998607609337393</td><td>-0.8916884114795887</td><td>6.901495930274793</td><td>5.321455175040075</td><td>-0.37971657154338395</td><td>-0.3023521304130554</td><td>-0.8591983176887451</td><td>0.4127486479698836</td><td>-0.8679340461933621</td><td>0.4271650525814584</td><td>0.2534176578526006</td><td>-0.3940478232125578</td><td>-0.2816172379914081</td><td>-0.874881731600069</td><td>2.041410478830176</td><td>1.404129183064541</td><td>1.2895440686703488</td><td>3.3239326</td><td>0.0</td><td>3666.622802734375</td><td>3.639929</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.10647333413362503</td><td>24.63332176208496</td><td>16.90055274963379</td><td>-0.894172344445321</td><td>-0.431108424887396</td><td>-0.12083602285852724</td><td>-0.4150634625674986</td><td>0.6969958927253865</td><td>0.5847384437211395</td><td>0.16786345781806322</td><td>-0.5730115631599854</td><td>0.8021718070428016</td><td>0.2554090437812483</td><td>0.18938552774748685</td><td>0.17268245195140966</td><td>-0.6350778937339783</td><td>-0.5722015500068665</td><td>0.8127713799476624</td></tr>\n",
       "<tr><td>2.0</td><td>0.029657668413294556</td><td>0.027907432725562016</td><td>0.025816469838701374</td><td>-0.26077723253654983</td><td>0.4254462113472258</td><td>-0.8665972283828683</td><td>0.9558149884532077</td><td>-0.012390405194009673</td><td>-0.2937076534707986</td><td>0.13569429921250462</td><td>0.9048988888872719</td><td>0.40341673000980943</td><td>-0.880781888961792</td><td>0.44753759161619305</td><td>0.15470422818181326</td><td>0.47337510537536415</td><td>0.8403335008773494</td><td>0.26411288668688704</td><td>-0.011802700445901449</td><td>0.3058589730473757</td><td>-0.9520036685163501</td><td>382.3485118800341</td><td>379.8080753150767</td><td>229.73399493712893</td><td>0.9019741415977478</td><td>0.39395049048399733</td><td>0.17676446381355537</td><td>-0.4301801316453484</td><td>0.8551803839966307</td><td>0.2891566446840985</td><td>0.0372521000410273</td><td>0.3368523740079518</td><td>-0.9408202586933065</td><td>45.55879223874659</td><td>41.7458650873896</td><td>13.936163208857044</td><td>-0.8888373374938965</td><td>-0.36896232896961634</td><td>-0.27172599523478264</td><td>0.43208303553659616</td><td>-0.8722901430712391</td><td>-0.2289413826773946</td><td>-0.15255316150934772</td><td>-0.3208998381147507</td><td>0.934746397057207</td><td>12.192577897497326</td><td>10.585406962788612</td><td>7.440784034136231</td><td>0.8154981732368469</td><td>0.34012826538765034</td><td>-0.46826861664919506</td><td>-0.5509893239287479</td><td>0.7038767719183728</td><td>-0.4482948302962152</td><td>0.17712565926681462</td><td>0.623594622361482</td><td>0.7614172626038479</td><td>2.512468517480205</td><td>2.328128810399225</td><td>1.7195177630093514</td><td>3.4139335</td><td>0.0</td><td>773.4555053710938</td><td>2.6384254</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8531123399734497</td><td>26.72574806213379</td><td>17.510679244995117</td><td>-0.44998482120148936</td><td>0.5216721585493953</td><td>-0.7248253718535784</td><td>0.1483677084347021</td><td>-0.7566904268686213</td><td>-0.6367154945336407</td><td>0.8806251664483017</td><td>0.39405298740118433</td><td>-0.2630999037211969</td><td>0.06412549909992203</td><td>0.06279831222742362</td><td>0.05986077141319621</td><td>-0.5391837954521179</td><td>-0.1834503561258316</td><td>0.2371581792831421</td></tr>\n",
       "<tr><td>3.0</td><td>0.018082525231503666</td><td>0.014158971881358921</td><td>0.01222966521600501</td><td>-0.18327640118347924</td><td>0.8024303076072575</td><td>-0.5679043600841203</td><td>0.9766838307247415</td><td>0.21432650432834124</td><td>-0.0123630232240266</td><td>-0.11179649176104173</td><td>0.5569288562964891</td><td>0.823001697115023</td><td>0.9721372723579407</td><td>0.13241262915181962</td><td>-0.19343225375726192</td><td>-0.2253100707952795</td><td>0.7555232597061101</td><td>-0.6151584966829927</td><td>0.06468781300011002</td><td>0.6416007322970851</td><td>0.7643062129572853</td><td>87.04871759916425</td><td>84.99975852487485</td><td>62.34963110912915</td><td>0.9601742029190063</td><td>0.09534201450948858</td><td>-0.26263176314282344</td><td>-0.11330683354213822</td><td>0.9920863922003901</td><td>-0.05409391725016924</td><td>0.2553959753302503</td><td>0.08169755614512844</td><td>0.9633786405692335</td><td>11.497520319686519</td><td>9.693721223067465</td><td>-3.8285132677798916</td><td>0.9453354477882385</td><td>0.18561979935204662</td><td>-0.2681158910993191</td><td>-0.207397898455762</td><td>0.9767054732929888</td><td>-0.05506841341142483</td><td>0.2516484704654678</td><td>0.1076647970695349</td><td>0.9618114881744583</td><td>6.208022837439617</td><td>5.142239415879281</td><td>-1.1519118757588624</td><td>0.9223912358283997</td><td>-0.07469076297867584</td><td>-0.3789666305250492</td><td>-0.341641069588688</td><td>-0.6155109176150187</td><td>-0.7102307300216</td><td>0.18021042338372953</td><td>-0.7845811622913088</td><td>0.593259305094725</td><td>2.089374436125829</td><td>1.9721288558379273</td><td>0.8042911796768404</td><td>2.4028754</td><td>0.0</td><td>339.1330871582031</td><td>2.074516</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.24545502662658691</td><td>26.517372131347656</td><td>15.888749122619629</td><td>0.07192887265149081</td><td>-0.602036263783214</td><td>0.7952223427249978</td><td>-0.06875971805707189</td><td>0.7923976582805746</td><td>0.6061171935559767</td><td>0.9950368528108882</td><td>0.09827659050562189</td><td>-0.015600426493936224</td><td>0.11019573989878745</td><td>0.08198181434064328</td><td>0.06754900596157654</td><td>-0.36452817916870117</td><td>-0.13807687163352966</td><td>0.38148462772369385</td></tr>\n",
       "<tr><td>4.0</td><td>0.01801164473834766</td><td>0.0168904028181911</td><td>0.013904951566977544</td><td>-0.1142639508855221</td><td>-0.979945945153804</td><td>-0.16324734026822255</td><td>-0.9934460645980511</td><td>0.1131971410280455</td><td>0.01585320149754536</td><td>-0.0029438516735695247</td><td>-0.16398887718285846</td><td>0.9864577952946744</td><td>0.7485275268554688</td><td>0.6600091975474459</td><td>0.06398781308005926</td><td>0.09984522831373491</td><td>-0.016784453435366945</td><td>-0.9948614036667893</td><td>-0.65554367623597</td><td>0.7510699975872106</td><td>-0.0784623939946852</td><td>95.48307820950194</td><td>84.8429631454686</td><td>74.73061250881207</td><td>-0.9893072247505188</td><td>-0.04843766873430373</td><td>0.137568123101957</td><td>0.08901581515380513</td><td>0.546642973525605</td><td>0.8326209486600661</td><td>0.1155308655672284</td><td>-0.8359636665206632</td><td>0.536486129698258</td><td>8.999897029588341</td><td>8.000785662629548</td><td>-3.7907557456801593</td><td>0.9832291603088379</td><td>-0.16407630175885704</td><td>-0.07962015991931036</td><td>-0.17963702753860372</td><td>-0.9466587978466704</td><td>-0.26752132400351225</td><td>-0.031479215389444815</td><td>0.2773374977543313</td><td>-0.9602567215790949</td><td>4.977716213338439</td><td>4.500357897673841</td><td>-1.9989919689628635</td><td>0.951259195804596</td><td>0.08308734405975979</td><td>-0.296988995835959</td><td>-0.30827944117927625</td><td>0.28227597115623293</td><td>-0.9084514638955671</td><td>-0.008352037880026354</td><td>-0.955728397278694</td><td>-0.2941317291594872</td><td>1.9237631510049509</td><td>1.8755596258083822</td><td>0.5238074617832662</td><td>2.2063403</td><td>0.0</td><td>325.36669921875</td><td>2.0665267</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.7687843441963196</td><td>26.51828956604004</td><td>15.530523300170898</td><td>-0.7731495452799411</td><td>-0.025394237333196023</td><td>-0.6337151673612411</td><td>-0.38358190855097435</td><td>0.8144575707517007</td><td>0.4353432954320422</td><td>0.5050789047919271</td><td>0.5796671442780793</td><td>-0.6394382704989464</td><td>0.07512998409324027</td><td>0.07145462279941757</td><td>0.06208312340477497</td><td>-0.3841070532798767</td><td>0.06337937712669373</td><td>0.3301573693752289</td></tr>\n",
       "<tr><td>5.0</td><td>0.020714045769373805</td><td>0.015366443958998062</td><td>0.011101178523383154</td><td>0.4543514501412507</td><td>0.4368184773811964</td><td>0.7763725765203948</td><td>0.0989417327520005</td><td>0.8413858812125629</td><td>-0.5313006045697589</td><td>0.885310845564751</td><td>-0.3182128481292561</td><td>-0.3390653182071158</td><td>0.7768359780311584</td><td>0.15880068292379038</td><td>-0.6093506734152345</td><td>0.12258095443257752</td><td>-0.9873034088378564</td><td>-0.10102419763376748</td><td>0.6176567086165746</td><td>-0.0037844418460261536</td><td>0.7864387250770741</td><td>150.6719051410386</td><td>144.29760338145712</td><td>32.579406564684625</td><td>0.8302285671234131</td><td>0.06331149379261101</td><td>-0.5538160138923615</td><td>-0.010964905134195126</td><td>-0.9914818694743903</td><td>-0.12978240774067612</td><td>0.5573152549009173</td><td>-0.11382160277081475</td><td>0.8224623695935326</td><td>30.224059047691718</td><td>27.72795609148564</td><td>-5.601407306425997</td><td>0.8750212788581848</td><td>-0.013376262889044099</td><td>-0.4838995982078433</td><td>-0.07984628679003236</td><td>-0.9899146088434846</td><td>-0.11701981748445146</td><td>0.4774539936375365</td><td>-0.14103241751233508</td><td>0.8672644009586659</td><td>10.214945917008441</td><td>9.040170727109352</td><td>3.5761973690223474</td><td>-0.8614780902862549</td><td>0.16273267542655348</td><td>0.4810130892316824</td><td>0.32598990654872145</td><td>0.9035240204531997</td><td>0.2781634866269143</td><td>-0.3893405918884474</td><td>0.39643715871825347</td><td>-0.8314153490857331</td><td>2.3269508673959876</td><td>2.003771712772918</td><td>1.3924634090971475</td><td>2.491318</td><td>0.0</td><td>298.01812744140625</td><td>2.0118546</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>1.4265923500061035</td><td>26.341442108154297</td><td>19.04120445251465</td><td>0.3918429756654348</td><td>0.3654932744319969</td><td>0.8443185114437765</td><td>0.011168222931565</td><td>0.9157482681432102</td><td>-0.4015972860830387</td><td>0.9199643217093848</td><td>-0.1667926129589919</td><td>-0.35474761598085974</td><td>0.062249062652563976</td><td>0.04554774917427718</td><td>0.038746376495772214</td><td>-0.9575876593589783</td><td>0.2240697145462036</td><td>0.34571993350982666</td></tr>\n",
       "<tr><td>6.0</td><td>0.014119734858107792</td><td>0.011844378746852749</td><td>0.007982621187688364</td><td>-0.22290355721159702</td><td>0.2984316518790147</td><td>-0.9280369353313361</td><td>0.9394537387980854</td><td>-0.18840792189093264</td><td>-0.2862326459840651</td><td>-0.26027039178653266</td><td>-0.9356500436196766</td><td>-0.238365935137095</td><td>-0.9105023145675659</td><td>-0.0988293550637392</td><td>0.40151996746038326</td><td>0.41098426612009176</td><td>-0.10925613157905309</td><td>0.9050718373223812</td><td>0.04557914757245563</td><td>-0.9890883966006611</td><td>-0.1400952712139218</td><td>335.3875231376624</td><td>294.41755624107986</td><td>-42.357889451741656</td><td>0.8969227075576782</td><td>0.3906390273337473</td><td>0.20719759058429238</td><td>-0.4393241654870583</td><td>0.8404695553081605</td><td>0.3171832344547916</td><td>0.05023911662531079</td><td>0.37551574790392966</td><td>-0.9254533776678657</td><td>54.147985187340716</td><td>50.0636499904205</td><td>27.867718833217868</td><td>0.9127143025398254</td><td>-0.27453073013282103</td><td>0.30263098525806825</td><td>-0.3416022979844858</td><td>-0.919069329745677</td><td>0.1965182870181554</td><td>0.22418854796186125</td><td>-0.2827444877156255</td><td>-0.9326280339069702</td><td>12.92535772244448</td><td>11.247231530013353</td><td>9.113070946326427</td><td>-0.8143582344055176</td><td>0.3390340504565971</td><td>0.4710377874775135</td><td>0.5378165677834383</td><td>0.745882999514194</td><td>0.39295278400000105</td><td>-0.21811470381054063</td><td>0.5733362570662202</td><td>-0.789754083443028</td><td>2.539618561436438</td><td>2.3344824030547318</td><td>1.80247327951178</td><td>2.8106654</td><td>0.0</td><td>206.8306121826172</td><td>1.7277424</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.9047555923461914</td><td>26.611112594604492</td><td>17.832447052001953</td><td>0.11291700663663792</td><td>-0.287765607234945</td><td>0.9510208751152229</td><td>0.978800736446974</td><td>-0.13239059141012582</td><td>-0.15627491685148434</td><td>0.17087676244323766</td><td>0.9485060287624851</td><td>0.26671603899678925</td><td>0.03274124656447151</td><td>0.02647658482026913</td><td>0.021759721217229116</td><td>-0.6157798171043396</td><td>0.1805431842803955</td><td>0.8364304900169373</td></tr>\n",
       "<tr><td>7.0</td><td>0.014264607282143125</td><td>0.012930660989750504</td><td>0.010384966439804553</td><td>-0.32911445366610587</td><td>0.2580640586333649</td><td>0.9083427866338432</td><td>0.9287466042611946</td><td>-0.08535148538246126</td><td>0.3607559687881924</td><td>-0.17062655555733508</td><td>-0.9623502821658456</td><td>0.2115857106563602</td><td>-0.5996925234794617</td><td>-0.7457408138106868</td><td>0.2902404295058079</td><td>0.09643116278398547</td><td>0.29270824713300736</td><td>0.9513269222010118</td><td>0.7943990805299755</td><td>-0.5984918718556634</td><td>0.10362229574678217</td><td>106.19326795531998</td><td>81.73599148245327</td><td>-10.20633132785529</td><td>0.778051495552063</td><td>0.5637893944151302</td><td>-0.2770873032260318</td><td>-0.2447324288495145</td><td>-0.13420077738774477</td><td>-0.960258397317067</td><td>-0.57856883180264</td><td>0.8149427403772278</td><td>0.0335624309754635</td><td>28.03821793521729</td><td>22.953089681994513</td><td>-3.2529257228048234</td><td>0.8548998832702637</td><td>0.47899385711488035</td><td>-0.19927645448558487</td><td>-0.37472315456571015</td><td>0.3044741498035521</td><td>-0.8757157355750365</td><td>-0.35878792886391675</td><td>0.8233227659597501</td><td>0.4397849987822771</td><td>9.766195040270658</td><td>8.692339146168745</td><td>5.119134099857715</td><td>0.8400160074234009</td><td>0.20962384116815538</td><td>0.5004307851097664</td><td>-0.5422037651656503</td><td>0.3578287100487087</td><td>0.7602458097911947</td><td>0.019702855424065132</td><td>0.9099540974570008</td><td>-0.41424067643020446</td><td>2.3679617471799905</td><td>2.1642857947847527</td><td>1.5947039668881267</td><td>2.4935412</td><td>0.0</td><td>137.8258819580078</td><td>1.4610503</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>1.4113867282867432</td><td>26.66329574584961</td><td>17.27666473388672</td><td>-0.6442798631117556</td><td>0.13761416014325387</td><td>0.7523069858221204</td><td>0.7423171156837277</td><td>-0.12418326258128562</td><td>0.6584404430604626</td><td>-0.18403466453820141</td><td>-0.9826702703464344</td><td>0.02214457101813702</td><td>0.03617064514392236</td><td>0.0322852986267902</td><td>0.028433050752495505</td><td>-0.487853080034256</td><td>-0.006067287176847458</td><td>0.6665339469909668</td></tr>\n",
       "<tr><td>8.0</td><td>0.012384133540565926</td><td>0.011480578764441723</td><td>0.008352121662922892</td><td>-0.09489200944034107</td><td>0.9845018255922565</td><td>-0.14748444646771572</td><td>-0.6070729329106967</td><td>-0.1746455075092532</td><td>-0.7752170024154785</td><td>0.7889600501112708</td><td>-0.015971916364344224</td><td>-0.6142368738655077</td><td>0.7868995070457458</td><td>0.6145145813034041</td><td>0.05622298256882669</td><td>0.6145898622580878</td><td>-0.7886416891792428</td><td>0.017988532405250793</td><td>0.05539400340305002</td><td>0.020398908157355643</td><td>-0.9982561740019295</td><td>297.00356951122933</td><td>154.70036110379576</td><td>106.36466728451252</td><td>-0.9773894548416138</td><td>-0.19233808125036161</td><td>-0.08784018444010429</td><td>0.20084339955830172</td><td>-0.9743790274684</td><td>-0.10122963836544814</td><td>-0.06611931907851548</td><td>-0.11658290325199669</td><td>0.9909776295729025</td><td>45.70994895845578</td><td>41.547306769246795</td><td>14.042110243323462</td><td>0.9484310746192932</td><td>-0.21409147544254528</td><td>0.23375905418496223</td><td>-0.25863785674287965</td><td>-0.9490173863896809</td><td>0.18020116367423786</td><td>0.1832618736400553</td><td>-0.2313673280152882</td><td>-0.9554497606870821</td><td>12.333956344572897</td><td>10.806718608133647</td><td>7.680217240448437</td><td>0.8317590355873108</td><td>-0.28411894710134616</td><td>0.4769206318809015</td><td>-0.5212707518563224</td><td>-0.695197018785786</td><td>0.4949524303713436</td><td>0.19092843809867752</td><td>-0.6602859448522115</td><td>-0.7263393164050955</td><td>2.5267100202069264</td><td>2.377367265966651</td><td>1.7441043594413514</td><td>2.6079419</td><td>0.0</td><td>118.54483032226562</td><td>1.3346524</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8358985781669617</td><td>26.50518798828125</td><td>17.452577590942383</td><td>0.03019337371364592</td><td>-0.9973178994724258</td><td>0.06667358979158695</td><td>0.6088277874201042</td><td>0.0712522137829101</td><td>0.7900961000385804</td><td>0.7927276237453368</td><td>-0.016737067334008347</td><td>-0.6093461948089348</td><td>0.02507435545710483</td><td>0.023336459324641515</td><td>0.017664686905375673</td><td>-0.478249192237854</td><td>0.572478175163269</td><td>0.7418162822723389</td></tr>\n",
       "<tr><td>9.0</td><td>0.011825359956376158</td><td>0.010521218654665525</td><td>0.008978701775802936</td><td>-0.7609710684566479</td><td>-0.6463192607270818</td><td>0.05651943192518953</td><td>0.3723984291927872</td><td>-0.3637957790608959</td><td>0.8537985951453777</td><td>-0.5312649460548777</td><td>0.670763776862372</td><td>0.5175263401437973</td><td>0.411751389503479</td><td>-0.8524676327996239</td><td>0.3221175630061307</td><td>-0.4447841681946456</td><td>0.12051648053501705</td><td>0.8874924346961213</td><td>0.7953790499449934</td><td>0.5086990396574953</td><td>0.3295412780823392</td><td>267.7468038439064</td><td>172.7403347699184</td><td>-32.640283025697485</td><td>0.7494412660598755</td><td>0.6344754920526042</td><td>-0.18915235918686385</td><td>-0.43950295814616697</td><td>0.26309741857929136</td><td>-0.8588462598845511</td><td>0.49515140591754225</td><td>-0.7267878633559586</td><td>-0.47602992226997876</td><td>49.09953130616094</td><td>41.87076494607203</td><td>29.394175175203834</td><td>0.8856460452079773</td><td>0.3705045218806867</td><td>-0.2799240623581826</td><td>-0.29344113471123306</td><td>0.9137431830026912</td><td>0.28100835570405097</td><td>0.3598935702128169</td><td>-0.16673270459653186</td><td>0.9179743042903749</td><td>12.130659111205967</td><td>11.040164845071411</td><td>9.127062973829954</td><td>0.8107178211212158</td><td>0.3727685313728518</td><td>-0.4514201993941928</td><td>-0.5305426667923021</td><td>0.7938056589848028</td><td>-0.2973164214714939</td><td>0.24750970307419756</td><td>0.4805374051848933</td><td>0.8413220246150055</td><td>2.4672608847176476</td><td>2.348442180176855</td><td>1.8129556375128915</td><td>2.5268881</td><td>0.0</td><td>129.09141540527344</td><td>1.4090705</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.4719037413597107</td><td>26.57210350036621</td><td>18.048410415649414</td><td>-0.010990762544732776</td><td>-0.937025356616335</td><td>0.34908836158874756</td><td>0.8748738199638653</td><td>0.16004706438643404</td><td>0.45714410892312607</td><td>-0.4842261891724978</td><td>0.31043263075797217</td><td>0.8180199138652835</td><td>0.03507434720214762</td><td>0.027104034426882407</td><td>0.02378750381252945</td><td>-0.7521599531173706</td><td>0.16906972229480743</td><td>0.4688679873943329</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>752194.0</td><td>0.0039049782377542867</td><td>0.0036617101517045876</td><td>0.00323593098287949</td><td>-0.8969425341511473</td><td>0.15923472866925656</td><td>0.41247835290612045</td><td>0.4385604363122114</td><td>0.20181889799140837</td><td>0.8757476098255571</td><td>-0.056203506404498144</td><td>-0.9663919668535962</td><td>0.25085400588525136</td><td>0.8813514113426208</td><td>-0.14239274624984932</td><td>-0.45049302620831444</td><td>-0.454290413917188</td><td>0.006483625748448637</td><td>-0.8908300524904308</td><td>0.1297685658002307</td><td>0.9897890019658634</td><td>-0.05897330682241867</td><td>5.970796756331006</td><td>3.5133344809792804</td><td>-2.9648340086428737</td><td>-0.87455153465271</td><td>-0.055496988388769694</td><td>-0.4817465506771193</td><td>0.45307553636173487</td><td>0.2606185793915024</td><td>-0.8525259611451617</td><td>-0.17286472503102496</td><td>0.9638454442262679</td><td>0.20278004459072446</td><td>3.5582254513528113</td><td>1.9449147384840677</td><td>-3.678877401717923</td><td>-0.8715538382530212</td><td>-0.09990647198693227</td><td>0.4800131555179455</td><td>0.40938610145545484</td><td>0.3904636323890103</td><td>0.8245854544658691</td><td>-0.2698091039054739</td><td>0.9151813200872844</td><td>-0.2994097506979705</td><td>2.484524160636765</td><td>1.2301506411263117</td><td>-1.137547397261705</td><td>0.7137887477874756</td><td>0.5192677404498717</td><td>-0.46996452723553467</td><td>-0.14948485572206624</td><td>-0.5426113854083702</td><td>-0.8265755635965549</td><td>0.684222128435925</td><td>-0.6602529046912965</td><td>0.30968723061391806</td><td>1.0579058050113466</td><td>0.6455042915192071</td><td>0.11860523604323951</td><td>-0.6245405</td><td>34218.0</td><td>2.0303726196289062</td><td>-1.3926811</td><td>1.0</td><td>2.135425329208374</td><td>43591.21875</td><td>66893.1015625</td><td>46197.33203125</td><td>43.591217041015625</td><td>66.89310455322266</td><td>46.197330474853516</td><td>-0.33428342418923335</td><td>-0.5364417851953097</td><td>0.7749095453075738</td><td>0.8781455882307229</td><td>-0.4758345354840794</td><td>0.049414782318335675</td><td>-0.3422205694917228</td><td>-0.69700194112866</td><td>-0.6301375848809855</td><td>0.009796140220580327</td><td>0.00909373617652982</td><td>0.008591940835305619</td><td>-0.9059203267097473</td><td>-0.27984654903411865</td><td>0.3170780837535858</td></tr>\n",
       "<tr><td>752818.0</td><td>0.0026186262605587193</td><td>0.0024932728331082256</td><td>0.002279924441617432</td><td>-0.9030752217638134</td><td>-0.42804347083864136</td><td>-0.03512735271335533</td><td>-0.37217532460122005</td><td>0.7391351649699659</td><td>0.561395347026316</td><td>-0.21433775121106394</td><td>0.5200557614114376</td><td>-0.8268018707214889</td><td>-0.20766738057136536</td><td>-0.6820985198038517</td><td>0.7011532425744401</td><td>0.5806113379797836</td><td>0.4909114048286273</td><td>0.6495355777927098</td><td>0.7872513794847126</td><td>-0.5419848723816729</td><td>-0.2940878501550836</td><td>11.468423455048686</td><td>10.651956184244154</td><td>-9.081636927805297</td><td>-0.2167843133211136</td><td>-0.9318600698414162</td><td>-0.2909319059122004</td><td>0.5927500302742935</td><td>-0.36244097476253795</td><td>0.7192245417274118</td><td>0.7756622752541074</td><td>0.016533301192812565</td><td>-0.6309316006504104</td><td>5.571626128709312</td><td>5.305849647593844</td><td>-1.665199122983732</td><td>-0.0744098424911499</td><td>-0.7523948560910872</td><td>0.6544961084557119</td><td>0.5978332116983757</td><td>0.49164441112732254</td><td>0.6331518174953567</td><td>0.798159524435062</td><td>-0.43839223708334535</td><td>-0.41322345047039083</td><td>1.7220040774830347</td><td>1.5588226742413658</td><td>0.7511200728909818</td><td>0.44135501980781555</td><td>-0.5377206255924155</td><td>0.7183747397692141</td><td>0.18492605354786965</td><td>0.8378870880291757</td><td>0.5135636108927497</td><td>0.8780706649496923</td><td>0.09381767704697398</td><td>-0.4692442336654916</td><td>0.38305216297663175</td><td>0.3019758838822295</td><td>0.19838676700390007</td><td>-1.0096263</td><td>34416.0</td><td>2.374236822128296</td><td>-1.2458875</td><td>1.0</td><td>2.6618595123291016</td><td>22884.365234375</td><td>24484.498046875</td><td>55708.71875</td><td>22.88436508178711</td><td>24.484498977661133</td><td>55.708717346191406</td><td>-0.74311389551055</td><td>-0.6587438549240829</td><td>-0.11763618447949593</td><td>0.4915361967780833</td><td>-0.41806821367418684</td><td>-0.7639444586959265</td><td>-0.4540637681604565</td><td>0.6255201853777647</td><td>-0.6344687479528792</td><td>0.009999610624272874</td><td>0.008407642147943567</td><td>0.00785384932863061</td><td>-0.7071279287338257</td><td>0.4234252870082855</td><td>-0.11567516624927521</td></tr>\n",
       "<tr><td>755536.0</td><td>0.0021613513097696055</td><td>0.00207422701044982</td><td>0.0019470556776861182</td><td>0.5680242291524903</td><td>-0.12896409205728354</td><td>-0.8128448425471861</td><td>-0.716978955399259</td><td>-0.5624660205781143</td><td>-0.4117926094645316</td><td>0.4040911439394667</td><td>-0.8167008256729397</td><td>0.41195886777059687</td><td>-0.3111162781715393</td><td>-0.9457294025660034</td><td>-0.093821912995568</td><td>0.700595806725478</td><td>-0.29493411861908686</td><td>0.6497533234028579</td><td>-0.6421621055735647</td><td>0.1364176044230847</td><td>0.7543328624478672</td><td>17.2170062071065</td><td>12.974222181701856</td><td>-16.882677835225092</td><td>0.3025914430618286</td><td>0.9404595212832804</td><td>-0.15483640782516903</td><td>-0.691771241037885</td><td>0.32845478490129515</td><td>0.6430940866998766</td><td>0.6556607159449793</td><td>-0.08748338758340188</td><td>0.749970454393671</td><td>10.921497738375654</td><td>7.986124265775706</td><td>-10.690819231498974</td><td>-0.4161469042301178</td><td>0.8774404260381898</td><td>-0.2385792142164934</td><td>0.6614251737142258</td><td>0.47214744851157725</td><td>0.5827465370477234</td><td>-0.6239699369996484</td><td>-0.08470587711562799</td><td>0.7768438917200332</td><td>6.391702243933468</td><td>5.435520623703876</td><td>-1.974914627062353</td><td>0.5831827521324158</td><td>-0.36280727781932565</td><td>-0.7268210102950337</td><td>-0.6401639153442851</td><td>0.3455485194849024</td><td>-0.6861387484852224</td><td>0.5000880555822523</td><td>0.8654288531376353</td><td>-0.03073819807404793</td><td>2.0490855478253582</td><td>1.9420036362058934</td><td>0.7419671860135247</td><td>-0.820687</td><td>35267.0</td><td>2.1864006519317627</td><td>-1.3186811</td><td>1.0</td><td>2.3315136432647705</td><td>2042.494140625</td><td>25050.94921875</td><td>18758.775390625</td><td>2.042494058609009</td><td>25.050949096679688</td><td>18.75877571105957</td><td>-0.3263830133088009</td><td>-0.8374746126016356</td><td>0.43830400622308613</td><td>0.8027756152309624</td><td>-0.0007882010487470872</td><td>0.5962807143700495</td><td>0.49902448859150866</td><td>-0.5464756645879546</td><td>-0.6725614528049734</td><td>0.005858898899447545</td><td>0.005688195098892562</td><td>0.0056594885769899106</td><td>-0.9588258862495422</td><td>0.663828432559967</td><td>-0.3421096205711365</td></tr>\n",
       "<tr><td>755674.0</td><td>0.004436804083140114</td><td>0.004264740110807067</td><td>0.0025533748600972766</td><td>0.04403848197702088</td><td>-0.1135546899290897</td><td>-0.9925552601746</td><td>0.9505702597764065</td><td>-0.30091247648105296</td><td>0.07660197599706549</td><td>-0.30737077501573706</td><td>-0.9468669462459592</td><td>0.0946899824325525</td><td>-0.11105625331401825</td><td>0.9927880217499921</td><td>0.045149222326187496</td><td>-0.3622912634031452</td><td>-0.08274732416115264</td><td>0.9283845759198729</td><td>-0.9254250638861452</td><td>-0.08674574464436825</td><td>-0.3688680345562874</td><td>13.95188110612573</td><td>11.187302298978205</td><td>-19.386835812314224</td><td>0.16068865358829498</td><td>-0.978925184570336</td><td>0.1260342872967807</td><td>0.37727176411351526</td><td>0.17891567899753882</td><td>0.9086557080718338</td><td>0.9120554668237838</td><td>0.09846147887769592</td><td>-0.3980705497949664</td><td>11.8258019368186</td><td>9.33140442632036</td><td>-11.99921107508282</td><td>0.2940555512905121</td><td>-0.9373061462802995</td><td>0.18705217230597665</td><td>0.3416856022786474</td><td>0.2858612009989778</td><td>0.895284492750154</td><td>0.8926266163490134</td><td>0.19935034965757129</td><td>-0.40432309095167346</td><td>6.167059663873896</td><td>5.286718570452355</td><td>-0.206161498032284</td><td>0.2879871726036072</td><td>-0.8555263496191945</td><td>0.4302767120814721</td><td>0.29795075684016564</td><td>0.5070595174306202</td><td>0.8087743766226767</td><td>0.9101036920872033</td><td>0.10471538039028457</td><td>-0.4009313641497244</td><td>1.6106905869774604</td><td>1.4610559910002954</td><td>0.8732351221659276</td><td>-1.2674142</td><td>35311.0</td><td>2.6895902156829834</td><td>-1.1459374</td><td>1.0</td><td>2.8207802772521973</td><td>41948.828125</td><td>67385.875</td><td>43157.11328125</td><td>41.948829650878906</td><td>67.38587188720703</td><td>43.15711212158203</td><td>-0.7932919083736172</td><td>-0.10996820093798011</td><td>-0.5988279743727813</td><td>0.5489113093915441</td><td>-0.5547038022661512</td><td>-0.6252999809479729</td><td>0.2634090403365174</td><td>0.8247488627054532</td><td>-0.5004048270501155</td><td>0.011458019378859392</td><td>0.01017745813650446</td><td>0.00940806951299503</td><td>-0.996257483959198</td><td>-0.08073781430721283</td><td>0.3829881250858307</td></tr>\n",
       "<tr><td>756143.0</td><td>0.00308361038690644</td><td>0.002863228393033827</td><td>0.0026574845332567217</td><td>-0.8820282465768073</td><td>-0.30503057593444066</td><td>-0.3591413649048887</td><td>0.37735646811831136</td><td>-0.913722593613067</td><td>-0.15070871869365546</td><td>-0.28218481215316443</td><td>-0.2684536639089007</td><td>0.9210343979048611</td><td>0.0015297271311283112</td><td>0.6829034495813674</td><td>0.7305070420501512</td><td>-0.5062166100096855</td><td>0.630523231567244</td><td>-0.5883750489307832</td><td>0.8624050113985278</td><td>0.3688947451562144</td><td>-0.34666159768690435</td><td>10.484103245714167</td><td>9.364777762883662</td><td>-10.484911472825559</td><td>0.0693657174706459</td><td>0.6393084073479804</td><td>0.7658153546009777</td><td>-0.5144573365815175</td><td>0.6806359637661176</td><td>-0.5216017002135052</td><td>0.8547058241792225</td><td>0.3577980507525953</td><td>-0.37610970340016375</td><td>9.155412621463178</td><td>7.249615957388292</td><td>-9.763320299917943</td><td>0.11723411083221436</td><td>-0.664608436684243</td><td>-0.7379375237218778</td><td>-0.45003975370358634</td><td>-0.6979383065317145</td><td>0.5570873722873798</td><td>0.8852798332251028</td><td>-0.26679157813500975</td><td>0.38092239461754607</td><td>5.8597562089345825</td><td>4.904736729885101</td><td>-3.0848186345156816</td><td>0.1956349015235901</td><td>-0.7960476172685534</td><td>-0.5727435517287517</td><td>-0.32617973240589426</td><td>-0.6035965775051984</td><td>0.7275149165423553</td><td>0.9248425634525186</td><td>-0.044490028827478094</td><td>0.3777391562460853</td><td>2.021503591285496</td><td>1.8367751552424527</td><td>0.21233276768052678</td><td>-1.173998</td><td>35460.0</td><td>2.484553813934326</td><td>-1.2053144</td><td>1.0</td><td>2.7357537746429443</td><td>12748.4169921875</td><td>49639.87890625</td><td>47643.5</td><td>12.748416900634766</td><td>49.63987731933594</td><td>47.64350128173828</td><td>-0.955188023683456</td><td>0.29030062106486715</td><td>-0.057804747391940074</td><td>0.26884812485837184</td><td>0.932570293718068</td><td>0.24090108558230686</td><td>-0.12384072501334462</td><td>-0.2145651338963279</td><td>0.9688267534209717</td><td>0.00968518309011414</td><td>0.008876370829769229</td><td>0.008005922377726408</td><td>-0.8064053058624268</td><td>0.02114914357662201</td><td>0.5130857229232788</td></tr>\n",
       "<tr><td>759874.0</td><td>0.002820744583448637</td><td>0.002662717462739102</td><td>0.002319225147659849</td><td>0.04319607310607816</td><td>-0.9530069040905351</td><td>0.2998531974550014</td><td>-0.6074539930611559</td><td>0.2132287320806834</td><td>0.7652013814214669</td><td>0.7931795166179414</td><td>0.2152007169389496</td><td>0.5696972054053719</td><td>-0.1481405794620514</td><td>-0.82326837452263</td><td>-0.5479813409441071</td><td>0.67024069836565</td><td>-0.49101434161756335</td><td>0.5564910804138702</td><td>0.7272082045510493</td><td>0.2848404813869292</td><td>-0.6245271230274163</td><td>7.30261618068809</td><td>6.609193025087626</td><td>-6.649047371244596</td><td>0.22049525380134583</td><td>-0.6521221977135423</td><td>0.7253402541486638</td><td>-0.6843301402915557</td><td>-0.6333280678775361</td><td>-0.3613693644002592</td><td>-0.6950353258128171</td><td>0.41669196979035555</td><td>0.5859127052594989</td><td>5.071516011979067</td><td>4.657164687021211</td><td>-4.639407350910298</td><td>0.22362379729747772</td><td>0.36972846819821875</td><td>0.9018277329587392</td><td>-0.7103837628876122</td><td>0.6953354972725315</td><td>-0.10891949163670296</td><td>-0.6673434719507917</td><td>-0.6162867887895698</td><td>0.4181426603542181</td><td>2.510227887296019</td><td>2.4446278380397635</td><td>-0.29550932599063673</td><td>-0.33222824335098267</td><td>0.0033420409483209644</td><td>0.9431930972938731</td><td>0.7715653706591196</td><td>-0.5742085519298509</td><td>0.27380908986057984</td><td>0.5425046237776824</td><td>0.8187022472504417</td><td>0.1881897009055279</td><td>0.7182822949657577</td><td>0.6025217404174937</td><td>0.2306177146392478</td><td>-1.1865146</td><td>36674.0</td><td>2.4078431129455566</td><td>-1.2269256</td><td>1.0</td><td>2.522681713104248</td><td>56744.86328125</td><td>8870.302734375</td><td>35271.48046875</td><td>56.7448616027832</td><td>8.8703031539917</td><td>35.271480560302734</td><td>-0.49254175439093084</td><td>-0.7410512703662206</td><td>-0.4563393856222735</td><td>-0.3426860771911116</td><td>-0.31684952479878653</td><td>0.8844052414669142</td><td>-0.7999805451889888</td><td>0.5919876631513475</td><td>-0.09788633201696292</td><td>0.010622851462192457</td><td>0.00982369881825927</td><td>0.009650462343434934</td><td>-0.8629167675971985</td><td>0.048090964555740356</td><td>-0.16218844056129456</td></tr>\n",
       "<tr><td>765609.0</td><td>0.004260902628739046</td><td>0.0034987491021631826</td><td>0.0034106950579482743</td><td>-0.7254811450662946</td><td>-0.29035294328760564</td><td>-0.6239970164011381</td><td>-0.6037416947992897</td><td>-0.1667834636273663</td><td>0.7795378388643749</td><td>0.33041348960685485</td><td>-0.9422730201935549</td><td>0.05429991989991641</td><td>0.39381176233291626</td><td>-0.6548285431766336</td><td>-0.6450673468188927</td><td>-0.2761178421602759</td><td>-0.7536412095371603</td><td>0.5964762061709743</td><td>0.8767389806159819</td><td>0.05678473675674944</td><td>0.47760261048272723</td><td>10.481565932624203</td><td>6.6920962085506615</td><td>-7.170262658610948</td><td>0.4483642280101776</td><td>-0.8762701314169516</td><td>-0.17640911969134018</td><td>-0.19613505366352707</td><td>-0.28899686470826963</td><td>0.9370228668037912</td><td>0.87206683313159</td><td>0.38552752038642946</td><td>0.30144314484912554</td><td>4.729470471816316</td><td>4.465591796277699</td><td>-0.3970924795654311</td><td>0.48078492283821106</td><td>0.7879133844221842</td><td>-0.38475751826720556</td><td>-0.000794233791429596</td><td>-0.4384093620450877</td><td>-0.8987750555416539</td><td>0.8768381939716388</td><td>-0.4324230913348613</td><td>0.21015482786021653</td><td>1.7567807726870621</td><td>1.426349191737206</td><td>0.2031151300820342</td><td>0.4790285527706146</td><td>0.7892295986942002</td><td>-0.3842502908566294</td><td>0.13989461687933577</td><td>-0.5007876366108205</td><td>-0.8541903998441644</td><td>0.8665801415025594</td><td>-0.35542704393833213</td><td>0.3503005492297478</td><td>0.682993241841505</td><td>0.39078392445802157</td><td>-0.28050173093942027</td><td>-1.0077474</td><td>38584.0</td><td>1.9691458940505981</td><td>-1.3810116</td><td>1.0</td><td>2.1477677822113037</td><td>69493.40625</td><td>24621.359375</td><td>14227.939453125</td><td>69.493408203125</td><td>24.62135887145996</td><td>14.22793960571289</td><td>-0.6743944499030639</td><td>-0.010403297096392141</td><td>-0.7382979732800758</td><td>-0.717860103919018</td><td>-0.22480331505028345</td><td>0.6588932696148744</td><td>0.17282649432688355</td><td>-0.9743486239287901</td><td>-0.1441379960550045</td><td>0.010279716595123521</td><td>0.008949812636168017</td><td>0.008544749853599294</td><td>-0.6449933052062988</td><td>-0.14330190420150757</td><td>0.025591935962438583</td></tr>\n",
       "<tr><td>768819.0</td><td>0.0021323759568082227</td><td>0.0019531829151296413</td><td>0.00168196992926417</td><td>0.22650844435540055</td><td>0.8825824324248946</td><td>0.4119977847157102</td><td>0.22960971113889792</td><td>0.36268625898340107</td><td>-0.9031821843323399</td><td>0.9465586644188557</td><td>-0.299177083881071</td><td>0.12049799705393582</td><td>0.5274119973182678</td><td>0.05384076766902656</td><td>-0.8479019808409981</td><td>-0.8137323925258569</td><td>0.3189620539130704</td><td>-0.4859041073274638</td><td>0.24428710717396201</td><td>0.9462369575853424</td><td>0.21203638689671114</td><td>19.25813556124239</td><td>5.157875753926299</td><td>-12.949636135332824</td><td>-0.30958840250968933</td><td>0.22900413062259264</td><td>0.9228825088615509</td><td>0.8808795210692636</td><td>-0.2964034887415335</td><td>0.3690477492447729</td><td>0.3580590542991266</td><td>0.9272012079476807</td><td>-0.1099619644003186</td><td>10.042317755658667</td><td>4.536471533194636</td><td>-2.1186116933800037</td><td>-0.13278178870677948</td><td>-0.28319771340635713</td><td>0.9498252751686096</td><td>0.5369329413552438</td><td>0.7849578247187383</td><td>0.3091022968216713</td><td>0.8331098455277792</td><td>-0.5510356327264482</td><td>-0.047830082066023676</td><td>4.374354019368073</td><td>2.568492555061436</td><td>1.041868943709825</td><td>-0.021763809025287628</td><td>-0.18826020958231326</td><td>0.9818780118442816</td><td>0.1488672411019617</td><td>0.9705517113362551</td><td>0.18938827880562378</td><td>0.9886176617792873</td><td>-0.1502912808857845</td><td>-0.006902876774547467</td><td>1.1737398506529195</td><td>1.0210257779570981</td><td>0.46605677495209324</td><td>-1.1968647</td><td>39684.0</td><td>1.8780118227005005</td><td>-1.4170407</td><td>1.0</td><td>2.0431861877441406</td><td>36158.20703125</td><td>74774.9453125</td><td>70913.0546875</td><td>36.158206939697266</td><td>74.77494812011719</td><td>70.91305541992188</td><td>-0.25880737812234933</td><td>0.5373878123052639</td><td>-0.8026413147945962</td><td>0.9001275221184105</td><td>-0.16727014441858473</td><td>-0.4022326972178621</td><td>0.35041287783756714</td><td>0.8265803275979584</td><td>0.4404268123918498</td><td>0.007447231116267198</td><td>0.006435803850254935</td><td>0.005884069490372308</td><td>-0.7623950839042664</td><td>-0.16326552629470825</td><td>-0.24580833315849304</td></tr>\n",
       "<tr><td>770944.0</td><td>0.003327517241708196</td><td>0.002747715967309195</td><td>0.002407341366642528</td><td>0.14292035662607</td><td>0.6694683238880966</td><td>0.72896223151295</td><td>-0.5992812437275111</td><td>-0.5276248767513952</td><td>0.6020581204082236</td><td>-0.7876774483113533</td><td>0.522899754009827</td><td>-0.3257914742257349</td><td>0.05435025691986084</td><td>0.7641332931650193</td><td>0.6427646223097467</td><td>0.9839356278544618</td><td>0.06864069457954995</td><td>-0.16480028909648037</td><td>0.1700491977483439</td><td>-0.6413959505051615</td><td>0.7481273320904169</td><td>16.031065234256847</td><td>13.097033127661414</td><td>-18.57526951137799</td><td>0.042064864188432693</td><td>0.5122840869476556</td><td>-0.8577852652925486</td><td>0.9745662965674106</td><td>0.16810868869776616</td><td>0.1481890764503211</td><td>0.22011606185756488</td><td>-0.8422021628115925</td><td>-0.49218333603220676</td><td>8.338396006391417</td><td>7.324287751073336</td><td>-2.9798515915973374</td><td>-0.08957286179065704</td><td>-0.5715592807471986</td><td>-0.8156572142414509</td><td>0.8270799596506596</td><td>-0.49895586369803363</td><td>0.25880839713118475</td><td>0.5549012911290102</td><td>0.6514315272329814</td><td>-0.5174181311379249</td><td>2.724508155446739</td><td>1.8929122368676128</td><td>1.1767791243203767</td><td>-0.5422424077987671</td><td>-0.4646034656564396</td><td>-0.7000834361301348</td><td>0.09872729271140354</td><td>-0.8626714405853982</td><td>0.4960351875343029</td><td>0.8344016535923378</td><td>-0.19985395992659125</td><td>-0.5136460602243853</td><td>0.7937468924310519</td><td>0.47057788766000547</td><td>0.06254334276224803</td><td>-1.1959074</td><td>40428.0</td><td>1.8275648355484009</td><td>-1.4227736</td><td>1.0</td><td>2.186333417892456</td><td>23255.451171875</td><td>67403.6328125</td><td>73682.65625</td><td>23.255451202392578</td><td>67.40363311767578</td><td>73.68265533447266</td><td>-0.029873393774191622</td><td>0.959348917237283</td><td>0.28063719521842584</td><td>0.9545157855163463</td><td>-0.055948303086403764</td><td>0.29286413672873035</td><td>0.2966600673237379</td><td>0.2766214785178561</td><td>-0.9140423196319035</td><td>0.010272358832846969</td><td>0.008207857856765633</td><td>0.007920067444851958</td><td>-0.7486265897750854</td><td>0.3115461468696594</td><td>0.6876575946807861</td></tr>\n",
       "<tr><td>803349.0</td><td>0.0031508121209186802</td><td>0.0030230735357601266</td><td>0.0021085432756289216</td><td>-0.32068781961542403</td><td>0.8583297912918535</td><td>0.4005362552019344</td><td>0.11450726335093839</td><td>0.4549004614612307</td><td>-0.8831498495738073</td><td>0.9402379533874308</td><td>0.23735108919750747</td><td>0.24416603258146458</td><td>-0.653459370136261</td><td>-0.6424877035768541</td><td>0.40025042264332955</td><td>0.5947728860507294</td><td>-0.10873011102233433</td><td>0.7965067337919722</td><td>-0.46822650938675986</td><td>0.7585428885103405</td><td>0.4531849756974288</td><td>6.6748121996964525</td><td>2.1377852490317872</td><td>-5.148943097278041</td><td>0.7105637788772583</td><td>-0.5531318755114786</td><td>-0.4349071783190374</td><td>-0.484342016331643</td><td>0.06385517200989986</td><td>-0.8725453158566515</td><td>0.5104036997084477</td><td>0.8306429108231407</td><td>-0.2225318359767618</td><td>4.223898005240051</td><td>2.4463229500904813</td><td>-2.6088281377861557</td><td>-0.7002483010292053</td><td>-0.4427910772855235</td><td>0.5599896365228617</td><td>0.42053104198821856</td><td>0.37802810777223383</td><td>0.8247717214226297</td><td>-0.5768933817101604</td><td>0.8130380136318779</td><td>-0.07850614325354002</td><td>2.4838324889453736</td><td>1.8398044371683278</td><td>-1.1980335771197033</td><td>-0.641701877117157</td><td>-0.4435216393631488</td><td>0.6257054349642287</td><td>0.3951264139991685</td><td>0.5080488462491738</td><td>0.7653505646336464</td><td>-0.65733846143924</td><td>0.7383596178296242</td><td>-0.15076876954855106</td><td>0.9848531553910784</td><td>0.7312008380597477</td><td>0.03994652212845551</td><td>-1.2654737</td><td>52574.0</td><td>1.3310412168502808</td><td>-1.6483018</td><td>1.0</td><td>1.3615388870239258</td><td>45466.94140625</td><td>36893.8515625</td><td>72491.796875</td><td>45.466941833496094</td><td>36.89385223388672</td><td>72.4917984008789</td><td>-0.38634791225497644</td><td>0.18658681399575996</td><td>0.9032832620718336</td><td>-0.34642264167820824</td><td>0.8782718458782803</td><td>-0.3295905309171525</td><td>0.8548255050177862</td><td>0.44025438734948485</td><td>0.27468059704066633</td><td>0.007025950808725896</td><td>0.006659610929452984</td><td>0.006428501890477896</td><td>-0.2274145483970642</td><td>-0.12203589081764221</td><td>0.053576335310935974</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=17457>\n",
       " gal_id            a           ...        mlp_av_y             mlp_av_z      \n",
       "float64         float64        ...        float64              float64       \n",
       "-------- --------------------- ... --------------------- --------------------\n",
       "     0.0   0.24562844247483243 ...      0.51895672082901   0.5298088788986206\n",
       "     1.0   0.08088470650058953 ...   -0.5722015500068665   0.8127713799476624\n",
       "     2.0  0.029657668413294556 ...   -0.1834503561258316   0.2371581792831421\n",
       "     3.0  0.018082525231503666 ...  -0.13807687163352966  0.38148462772369385\n",
       "     4.0   0.01801164473834766 ...   0.06337937712669373   0.3301573693752289\n",
       "     5.0  0.020714045769373805 ...    0.2240697145462036  0.34571993350982666\n",
       "     6.0  0.014119734858107792 ...    0.1805431842803955   0.8364304900169373\n",
       "     7.0  0.014264607282143125 ... -0.006067287176847458   0.6665339469909668\n",
       "     8.0  0.012384133540565926 ...     0.572478175163269   0.7418162822723389\n",
       "     9.0  0.011825359956376158 ...   0.16906972229480743   0.4688679873943329\n",
       "     ...                   ... ...                   ...                  ...\n",
       "752194.0 0.0039049782377542867 ...  -0.27984654903411865   0.3170780837535858\n",
       "752818.0 0.0026186262605587193 ...    0.4234252870082855 -0.11567516624927521\n",
       "755536.0 0.0021613513097696055 ...     0.663828432559967  -0.3421096205711365\n",
       "755674.0  0.004436804083140114 ...  -0.08073781430721283   0.3829881250858307\n",
       "756143.0   0.00308361038690644 ...   0.02114914357662201   0.5130857229232788\n",
       "759874.0  0.002820744583448637 ...  0.048090964555740356 -0.16218844056129456\n",
       "765609.0  0.004260902628739046 ...  -0.14330190420150757 0.025591935962438583\n",
       "768819.0 0.0021323759568082227 ...  -0.16326552629470825 -0.24580833315849304\n",
       "770944.0  0.003327517241708196 ...    0.3115461468696594   0.6876575946807861\n",
       "803349.0 0.0031508121209186802 ...  -0.12203589081764221 0.053576335310935974"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e296451",
   "metadata": {},
   "source": [
    "## Let's build the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a62847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_key='GroupID'\n",
    "pos_key=['gal_pos_x', 'gal_pos_y', 'gal_pos_z']\n",
    "scalar_key = ['mass', 'dm_mass']\n",
    "catalog = tng\n",
    "\n",
    "# It takes a minute but we precompute all the graphs and data\n",
    "# Identify the individual groups and pre-extract the relevant data\n",
    "\n",
    "group_ids = catalog[group_key].astype(jnp.int32)\n",
    "gids, idx = jnp.unique(group_ids, return_index=True) # gids are the unique group ids, in other words All the host halo IDS uniquely  extracted     idx = The indices of the first occurrences of the unique values in the original array. i.e index of the central galaxy\n",
    "Position = jnp.array(catalog[pos_key].to_pandas())\n",
    "Scalars = jnp.array(catalog[scalar_key].to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091471bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_radius = 1 #Mpc/h\n",
    "graphs_list = []\n",
    "node_features_list = []\n",
    "positions_list = []\n",
    "direction_to_COM_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de74b5ed-3dae-4a49-bbc6-c8f91b3e5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10292/10292 [00:56<00:00, 181.03it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gid in tqdm(gids):\n",
    "    \n",
    "    g = np.where(group_ids == gid)[0]\n",
    "    Positions_for_group = Position[g]\n",
    "    Features_for_group = Scalars[g]\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    # Compute adjacency matrix for each entry\n",
    "    graph = radius_neighbors_graph(Positions_for_group, graph_radius, mode='connectivity',\n",
    "                               include_self=False)\n",
    "    \n",
    "    Positions_for_group_COM =  (Positions_for_group - Positions_for_group[0])\n",
    "    directions_to_COM = Positions_for_group_COM#/jnp.linalg.norm(Positions_for_group_COM, axis=-1)[..., jnp.newaxis]\n",
    "    \n",
    " \n",
    "    directions_to_COM = directions_to_COM.at[jnp.isnan(directions_to_COM)].set(0.0)\n",
    "    \n",
    "    graphs_list.append(graph)\n",
    "    node_features_list.append(Features_for_group)\n",
    "    \n",
    "    direction_to_COM_list.append(directions_to_COM)\n",
    "    positions_list.append(Positions_for_group_COM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f81f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_node = graphs_list[0].tocoo().shape[0]\n",
    "max_n_edge = graphs_list[0].tocoo().nnz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babd3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "senders_list = []\n",
    "receivers_list = []\n",
    "n_node_list = []\n",
    "n_edge_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62db2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12261171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10292/10292 [00:23<00:00, 439.63it/s]\n"
     ]
    }
   ],
   "source": [
    "for single_graph in tqdm(graphs_list):\n",
    "    single_graph =  single_graph.tocoo()\n",
    "    \n",
    " \n",
    "    senders = single_graph.row.astype(jnp.int64)\n",
    "    receivers = single_graph.col.astype(jnp.int64)\n",
    "    values = single_graph.data.astype(jnp.int64)\n",
    "    n_node = jnp.asarray([single_graph.shape[0]])\n",
    " \n",
    "    values = np.array([0]) if values.size == 0 else values\n",
    "    n_edge = np.array([np.sum(values)])\n",
    "    senders = np.repeat(senders, values)\n",
    "    receivers = np.repeat(receivers, values)\n",
    "    \n",
    "    if len(senders) < max_n_edge:\n",
    "        senders = jnp.concatenate( [senders, -1*jnp.ones( max_n_edge - len(senders)  )] )\n",
    "        receivers = jnp.concatenate( [receivers, -1*jnp.ones(max_n_edge - len(receivers)   )] )\n",
    "        \n",
    "    \n",
    "     \n",
    "    senders_list.append( jnp.asarray(senders).astype(jnp.int32)  )\n",
    "    receivers_list.append( jnp.asarray(receivers).astype(jnp.int32) )\n",
    "    \n",
    "    n_node_list.append( jnp.asarray(n_node).astype(jnp.int32) )\n",
    "    n_edge_list.append( jnp.asarray(n_edge).astype(jnp.int32) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f9da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_list_padded = []\n",
    "positions_list_padded = []\n",
    "direction_to_COM_list_padded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1958f135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10292/10292 [00:07<00:00, 1336.99it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for direction_to_COM in tqdm(direction_to_COM_list ):\n",
    "    if len(direction_to_COM) < max_n_node:\n",
    "        \n",
    "        direction_to_COM = jnp.concatenate( [direction_to_COM, 0*jnp.ones( (max_n_node-len(direction_to_COM), jnp.squeeze(direction_to_COM ).shape[-1])  )] )\n",
    "        \n",
    "    direction_to_COM_list_padded.append( jnp.asarray(direction_to_COM).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280d4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10292/10292 [00:07<00:00, 1418.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for node_features in tqdm(node_features_list ):\n",
    "    if len(node_features) < max_n_node:\n",
    "        \n",
    "        node_features = jnp.concatenate( [node_features, 0*jnp.ones( (max_n_node-len(node_features), jnp.squeeze(node_features ).shape[-1])  )] )\n",
    "        \n",
    "    node_features_list_padded.append( jnp.asarray(node_features).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea28dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10292/10292 [00:04<00:00, 2154.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for positions in tqdm(positions_list ):\n",
    "    if len(positions) < max_n_node:\n",
    "        \n",
    "        positions = jnp.concatenate( [positions, 0*jnp.ones( (max_n_node-len(positions), jnp.squeeze(positions ).shape[-1])  )] )\n",
    "        \n",
    "    positions_list_padded.append( jnp.asarray(positions).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbda61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c75c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b238a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "senders_list = jnp.array(senders_list)\n",
    "receivers_list = jnp.array(receivers_list)\n",
    "\n",
    "n_node_list = jnp.array(n_node_list)\n",
    "n_edge_list = jnp.array(n_edge_list)\n",
    "\n",
    "node_features_list_padded = jnp.array(node_features_list_padded)\n",
    "positions_list_padded = jnp.array(positions_list_padded)\n",
    "direction_to_COM_list_padded = jnp.array(direction_to_COM_list_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9dac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_feats = jnp.concatenate([node_features_list_padded,   direction_to_COM_list_padded, positions_list_padded], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bad61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP =  gn_graph.GraphsTuple(\n",
    "      nodes=concat_feats,  \n",
    "      edges=-1*jnp.ones_like(n_node_list),\n",
    "      receivers= receivers_list   ,\n",
    "      senders= senders_list ,\n",
    "      globals=  -1*jnp.ones_like(n_node_list),\n",
    "      n_node=n_node_list,\n",
    "      n_edge=n_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfc49d1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dset = tf.data.Dataset.from_tensor_slices(GP)\n",
    "\n",
    "dset = dset.repeat()\n",
    "dset = dset.shuffle(buffer_size=10000)\n",
    "dset = dset.batch(batch_size)\n",
    "dset = dset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dset = dset.as_numpy_iterator()\n",
    "_ = next(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4613b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = next(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de2d6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbf1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dset2 = tf.data.Dataset.from_tensor_slices(GP)\n",
    "\n",
    "dset2 = dset2.repeat()\n",
    "dset2 = dset2.shuffle(buffer_size=10000)\n",
    "dset2 = dset2.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dset2 = dset2.as_numpy_iterator()\n",
    "g_init = next(dset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c058acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_e(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim , in_dim ], activation=jax.nn.silu)(feats)\n",
    "  return net\n",
    "\n",
    "def f_inf(m_ij: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP( [1,], activation=jax.nn.sigmoid)(m_ij)\n",
    "  return net\n",
    "\n",
    "def f_h(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim,  ], activation=jax.nn.silu)(feats)\n",
    "  net = hk.Linear(in_dim)(net)\n",
    "  return net\n",
    "\n",
    "\n",
    "def f_x(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim, in_dim  ], activation=jax.nn.silu)(feats)\n",
    "  net = hk.Linear(1)(net) \n",
    "  return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a86f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT modified\n",
    "def GNN(add_self_edges: bool = False ) -> Callable:\n",
    "  def _ApplyGNN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    " \n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    \n",
    "    \n",
    "    # Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    \n",
    "    \n",
    "    # position of the nodes are the last 3 entries in the features matrix\n",
    "    distances = jnp.linalg.norm( nodes[:,-3:][senders] - nodes[:,-3:][receivers], axis=-1)\n",
    "    node_feat = nodes[:,:n_scalar]\n",
    "    \n",
    "    input_dim = nodes[:,:n_scalar].shape[-1]\n",
    "    \n",
    "    \n",
    "    concat_feats = jnp.concatenate([ node_feat[senders], node_feat[receivers], distances.reshape((-1,1))**2 ], axis=-1)\n",
    "    m_ij = f_e(concat_feats, input_dim)\n",
    "    \n",
    "    \n",
    "    # predict edges\n",
    "    e_ij = f_inf(m_ij, input_dim)\n",
    "    \n",
    "    m_i = jraph_utils.segment_sum( e_ij*m_ij, receivers, num_segments=sum_n_node)\n",
    "\n",
    "    concat_for_hl1_i =  jnp.concatenate([node_feat,  m_i], axis=-1)\n",
    "    \n",
    "    \n",
    "    new_node_feat = node_feat + f_h(concat_for_hl1_i, input_dim) #h^(l+1)_i    f_h is the node update func\n",
    "\n",
    "     \n",
    "    position_mlp = f_x(concat_feats, input_dim)\n",
    "    prefac_xi_xj = (nodes[:,-3:][senders] - nodes[:,-3:][receivers])/ (distances.reshape((-1,1))+1)\n",
    "    \n",
    "    sum_xi_xj =  jraph_utils.segment_sum( prefac_xi_xj*position_mlp, receivers, num_segments=sum_n_node)\n",
    "    #sum_xi_xj = jnp.einsum('ij,ij ->i', prefac_xi_xj, position_mlp ) #f_x\n",
    "    \n",
    "    x_Lplus1_i = nodes[:,-3:] + sum_xi_xj\n",
    "    \n",
    "#     x_Lplus1_i = x_Lplus1_i/jnp.linalg.norm(x_Lplus1_i, axis=-1)[..., jnp.newaxis]\n",
    "#     x_Lplus1_i = x_Lplus1_i.at[jnp.isnan(x_Lplus1_i)].set(0.0)\n",
    "    \n",
    "    \n",
    "    nodes = jnp.concatenate([new_node_feat, x_Lplus1_i],axis=-1)\n",
    "    #print(nodes.shape)\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  # pylint: enable=g-long-lambda\n",
    "  return _ApplyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0fb32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_expand_MLP(feats: jnp.ndarray, out_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP( [out_dim] )(feats)\n",
    "  return net\n",
    "\n",
    "\n",
    "def dim_expand(graph: jraph.GraphsTuple, out_dim) -> jraph.GraphsTuple:\n",
    "\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    node_feat = nodes[:,:n_scalar]\n",
    " \n",
    "    new_node = dim_expand_MLP(node_feat, out_dim)\n",
    "    \n",
    "    nodes = jnp.concatenate([new_node, nodes[:,n_scalar:]],axis=-1)\n",
    "\n",
    "     \n",
    "    return graph._replace(nodes=nodes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c0598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569b2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced5484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afca7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def gcn_definition(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a GCN for the karate club task.\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  graph = dim_expand(graph,   64)\n",
    "    \n",
    "  gn = GNN( \n",
    "      add_self_edges=False )\n",
    "  graph = gn(graph)\n",
    "  \n",
    "  \n",
    "\n",
    "  gn = GNN(add_self_edges=False) # output dim is 2 because we have 2 scalars.\n",
    "  graph = gn(graph)\n",
    "    \n",
    "  graph = dim_expand(graph,   0)\n",
    "   \n",
    "\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85f80bce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network = hk.without_apply_rng(hk.transform(gcn_definition ))\n",
    "\n",
    "\n",
    "#params = jax.vmap(lambda x: network.init( next(rng) , x))  (g)\n",
    "\n",
    "params =  network.init( next(rng) , g_init )\n",
    "\n",
    "#out_graph = jax.vmap(lambda x,y: network.apply(x,y))   (params, g)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c6144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e4294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d89a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def prediction_loss(params: hk.Params, graph_batch: jraph.GraphsTuple ) -> jnp.ndarray:\n",
    "    \n",
    "    predicted_graph = jax.vmap ( lambda x: network.apply(params, x)) (graph_batch)\n",
    "\n",
    "    #print(predicted_graph.nodes.shape)\n",
    "    loss =  jnp.linalg.norm(predicted_graph.nodes   -  graph_batch.nodes[:,:,n_scalar:-3], axis=-1) **2\n",
    "    \n",
    "    return jnp.mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d48a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update = optax.adam(1e-3)\n",
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69e1dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params: hk.Params, opt_state, graph_batch: jraph.GraphsTuple) -> Tuple[hk.Params, Any]:\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(prediction_loss)(params, graph_batch)\n",
    "\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    \n",
    "    return optax.apply_updates(params, updates), opt_state, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e3b231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "params_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011be02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 5418/10000 [1:49:28<1:22:05,  1.07s/it]"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(10_000)):\n",
    "    params, opt_state, loss = update(params, opt_state, next(dset))\n",
    "\n",
    "    #print(loss)\n",
    "    if jnp.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    if step%10==0:\n",
    "        pickle.dump( params_list, open( \"/ocean/projects/phy210062p/yjagvara/EGNN_weights.p\", \"wb\" ) )\n",
    "        pickle.dump( losses, open( \"/ocean/projects/phy210062p/yjagvara/EGNN_losses.p\", \"wb\" ) )\n",
    "        losses.append(loss)\n",
    "        params_list.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11fc8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pickle.load(open( \"/ocean/projects/phy210062p/yjagvara/EGNN_losses.p\", \"rb\" ))\n",
    "params_list = pickle.load(open( \"/ocean/projects/phy210062p/yjagvara/EGNN_weights.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b507138f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x150663edde20>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10c96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0252a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_graph = jax.vmap ( lambda x: network.apply(params_list[-1], x)) (GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2ab6ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10292, 227, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_graph.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7911af68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10292, 227, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GP.nodes[:,:,n_scalar:n_scalar+3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39801e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = jnp.einsum('...k,...k->...' ,  predicted_graph.nodes, GP.nodes[:,:,n_scalar:n_scalar+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707a883e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90fd4fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([224.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.]),\n",
       " array([   0.        ,  552.35546875, 1104.7109375 , 1657.06640625,\n",
       "        2209.421875  , 2761.77734375, 3314.1328125 , 3866.48828125,\n",
       "        4418.84375   , 4971.19921875, 5523.5546875 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(t[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2143e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c248f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe60d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd9711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfd166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca68d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45d602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27412dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d999a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2aee7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c213cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592482e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jraph.concatenated_args\n",
    "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.Sequential(\n",
    "      [hk.Linear(128), jax.nn.relu,\n",
    "       hk.Linear(128)])\n",
    "  return net(feats)\n",
    "\n",
    "\n",
    "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  # Add a global paramater for graph classification.\n",
    "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
    "  embedder = jraph.GraphMapFeatures(\n",
    "      hk.Linear(128), hk.Linear(128), hk.Linear(128))\n",
    "  net = jraph.GraphNetwork(\n",
    "      update_node_fn=node_update_fn,\n",
    "      update_edge_fn=edge_update_fn,\n",
    "      update_global_fn=update_global_fn)\n",
    "  return net(embedder(graph)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple, label: jnp.ndarray,\n",
    "                 net: jraph.GraphsTuple) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "  \"\"\"Computes loss and accuracy.\"\"\"\n",
    "  pred_graph = net.apply(params, graph)\n",
    "  preds = jax.nn.log_softmax(pred_graph.globals)\n",
    "  targets = jax.nn.one_hot(label, 2)\n",
    "\n",
    "  # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
    "  # to mask out any loss associated with the dummy graph.\n",
    "  # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
    "  # get_graph_padding_mask.\n",
    "  mask = jraph.get_graph_padding_mask(pred_graph)\n",
    "\n",
    "  # Cross entropy loss.\n",
    "  loss = -jnp.mean(preds * targets * mask[:, None])\n",
    "\n",
    "  # Accuracy taking into account the mask.\n",
    "  accuracy = jnp.sum(\n",
    "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask) / jnp.sum(mask)\n",
    "  return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
    "def train(dataset: List[Dict[str, Any]], num_train_steps: int) -> hk.Params:\n",
    "  \"\"\"Training loop.\"\"\"\n",
    "\n",
    "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
    "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
    "  # Get a candidate graph and label to initialize the network.\n",
    "  graph = dataset[0]['input_graph']\n",
    "\n",
    "  # Initialize the network.\n",
    "  params = net.init(jax.random.PRNGKey(42), graph)\n",
    "  # Initialize the optimizer.\n",
    "  opt_init, opt_update = optax.adam(1e-4)\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
    "  # We jit the computation of our loss, since this is the main computation.\n",
    "  # Using jax.jit means that we will use a single accelerator. If you want\n",
    "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
    "  # found in the jax documentation.\n",
    "  compute_loss_fn = jax.jit(jax.value_and_grad(\n",
    "      compute_loss_fn, has_aux=True))\n",
    "\n",
    "  for idx in range(num_train_steps):\n",
    "    graph = dataset[idx % len(dataset)]['input_graph']\n",
    "    label = dataset[idx % len(dataset)]['target']\n",
    "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
    "    # In the limit, this means a new compilation every training step, which\n",
    "    # will result in *extremely* slow training. To prevent this, pad each\n",
    "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
    "    # of compiled programs, the compilation cost is amortized.\n",
    "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
    "\n",
    "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
    "    # been added to the batch, which means there should be an extra label.\n",
    "    label = jnp.concatenate([label, jnp.array([0])])\n",
    "\n",
    "    (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
    "    updates, opt_state = opt_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if idx % 50 == 0:\n",
    "      print(f'step: {idx}, loss: {loss}, acc: {acc}')\n",
    "  print('Training finished')\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train(train_mutag_ds, num_train_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfa6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28355cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(hk.Module):\n",
    "  def __init__(self, features: jnp.ndarray):\n",
    "    super().__init__()\n",
    "    self.features = features\n",
    "\n",
    "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    layers = []\n",
    "    for feat in self.features[:-1]:\n",
    "      layers.append(hk.Linear(feat))\n",
    "      layers.append(jax.nn.relu)\n",
    "    layers.append(hk.Linear(self.features[-1]))\n",
    "\n",
    "    mlp = hk.Sequential(layers)\n",
    "    return mlp(x)\n",
    "\n",
    "# Use MLP block to define the update node function\n",
    "update_node_fn = lambda x: MLP(features=[8, 4])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18233c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = False) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    # First pass nodes through the node updater.\n",
    "    nodes = update_node_fn(nodes)\n",
    "    # Equivalent to jnp.sum(n_node), but jittable\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
    "      # this case it is not required since a GCN is agnostic to whether\n",
    "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
    "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
    "                                                       total_num_nodes)\n",
    "    else:\n",
    "      conv_senders = senders\n",
    "      conv_receivers = receivers\n",
    "\n",
    "    # pylint: disable=g-long-lambda\n",
    "    if symmetric_normalization:\n",
    "      # Calculate the normalization values.\n",
    "      count_edges = lambda x: jax.ops.segment_sum(\n",
    "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
    "      sender_degree = count_edges(conv_senders)\n",
    "      receiver_degree = count_edges(conv_receivers)\n",
    "\n",
    "      # Pre normalize by sqrt sender degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
    "          nodes,\n",
    "      )\n",
    "      # Aggregate the pre-normalized nodes.\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "      # Post normalize by sqrt receiver degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x:\n",
    "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
    "          nodes,\n",
    "      )\n",
    "    else:\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "    # pylint: enable=g-long-lambda\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(8)(n)),\n",
    "      add_self_edges=False)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(4)(n)),\n",
    "      add_self_edges=False)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=hk.Linear(2))\n",
    "  graph = gn(graph)\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525be495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21324b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = False) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    # First pass nodes through the node updater.\n",
    "    nodes = update_node_fn(nodes)\n",
    "    # Equivalent to jnp.sum(n_node), but jittable\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
    "      # this case it is not required since a GCN is agnostic to whether\n",
    "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
    "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
    "                                                       total_num_nodes)\n",
    "    else:\n",
    "      conv_senders = senders\n",
    "      conv_receivers = receivers\n",
    "\n",
    "    # pylint: disable=g-long-lambda\n",
    "    if symmetric_normalization:\n",
    "      # Calculate the normalization values.\n",
    "      count_edges = lambda x: jax.ops.segment_sum(\n",
    "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
    "      sender_degree = count_edges(conv_senders)\n",
    "      receiver_degree = count_edges(conv_receivers)\n",
    "\n",
    "      # Pre normalize by sqrt sender degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
    "          nodes,\n",
    "      )\n",
    "      # Aggregate the pre-normalized nodes.\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "      # Post normalize by sqrt receiver degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x:\n",
    "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
    "          nodes,\n",
    "      )\n",
    "    else:\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "    # pylint: enable=g-long-lambda\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044347fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT modified\n",
    "def GNN(add_self_edges: bool = False ) -> Callable:\n",
    " \n",
    " \n",
    "\n",
    "  def _ApplyGNN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    " \n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    \n",
    "    \n",
    "    #Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # position of the nodes are the last 3 entries in the features matrix\n",
    "    \n",
    "    distances = jnp.linalg.norm( nodes[:,:3][senders] - nodes[:,:3][receivers], axis=-1)\n",
    "    node_feat = nodes[:,3:]\n",
    "    \n",
    "    input_dim = nodes[:,3:].shape[-1]\n",
    "    \n",
    "    \n",
    "    concat_feats = jnp.concatenate([ node_feat[senders], node_feat[receivers], distances.reshape((-1,1))**2 ], axis=-1)\n",
    "    m_ij = f_e(concat_feats, input_dim)\n",
    "    \n",
    "    \n",
    "    # predict edges\n",
    "    e_ij = f_inf(m_ij, input_dim)\n",
    "    \n",
    "    m_i = jraph_utils.segment_sum( e_ij*m_ij, receivers, num_segments=sum_n_node)\n",
    "\n",
    "    concat_for_hl1_i =  jnp.concatenate([node_feat,  m_i], axis=-1)\n",
    "    \n",
    "    \n",
    "    new_node_feat = node_feat + f_h(concat_for_hl1_i, input_dim) #h^(l+1)_i    f_h is the node update func\n",
    "\n",
    "     \n",
    "    position_mlp = f_x(concat_feats, input_dim)\n",
    "    prefac_xi_xj = (nodes[:,:3][senders] - nodes[:,:3][receivers])/ (distances.reshape((-1,1))+1)\n",
    "    \n",
    "    sum_xi_xj =  jraph_utils.segment_sum( prefac_xi_xj*position_mlp, receivers, num_segments=sum_n_node)\n",
    "    #sum_xi_xj = jnp.einsum('ij,ij ->i', prefac_xi_xj, position_mlp ) #f_x\n",
    "    \n",
    "    x_Lplus1_i = nodes[:,:3] + sum_xi_xj\n",
    "    \n",
    "    nodes = jnp.concatenate([x_Lplus1_i, new_node_feat],axis=-1)\n",
    "    #print(nodes.shape)\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  # pylint: enable=g-long-lambda\n",
    "  return _ApplyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ed5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_expand_MLP(feats: jnp.ndarray, out_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.Linear( out_dim )(feats)\n",
    "  return net\n",
    "\n",
    "\n",
    "def dim_expand(graph: jraph.GraphsTuple, out_dim) -> jraph.GraphsTuple:\n",
    "\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    node_feat = nodes[:,3:]\n",
    "    in_dim = nodes[:,3:].shape[-1]\n",
    "    new_node = dim_expand_MLP(node_feat, out_dim)\n",
    "    \n",
    "    nodes = jnp.concatenate([nodes[:,:3], new_node],axis=-1)\n",
    "\n",
    "     \n",
    "    return graph._replace(nodes=nodes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe45fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
