{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180eaf69-31e6-4aec-a341-0e4a3497f572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 15:22:20.092046: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/packages/AI/tensorflow_23.02-2.10.0-py3/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import tensorflow_probability as tfp; tfp = tfp.substrates.jax\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "from jaxlie import SO3\n",
    "from so3dm.distributions.isotropic_gaussian import IsotropicGaussianSO3\n",
    "from so3dm.plotting import visualize_so3_probabilities, visualize_so3_density\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from flax.metrics import tensorboard\n",
    "import haiku as hk\n",
    "import optax\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8071a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "import jraph\n",
    "from jraph import GraphConvolution\n",
    "from jraph._src import utils as jraph_utils\n",
    "from jraph._src import graph as gn_graph\n",
    "\n",
    "import networkx as nx\n",
    "import jax.tree_util as tree\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb694466-e71a-4d8f-aa2a-b10ca6462380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"]=\"--xla_gpu_force_compilation_parallelism=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99054b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25837/1757885639.py:5: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0594215-29ad-4b41-a9a0-ad0c4fabf896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.inv(jnp.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a121dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = hk.PRNGSequence(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6263e366-af61-41c1-988c-63edddf0aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from halotools_ia.correlation_functions import  ed_3d, ee_3d,gi_plus_3d, gi_plus_projected, ii_minus_3d, ii_minus_projected, ii_plus_3d, ii_plus_projected, ed_projected, ed_3d_one_two_halo_decomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee030ed-0c33-4497-958c-23ecb05d73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tng = pickle.load(  open('/jet/home/yjagvara/SO3Diffusion_Tidal/TNG100-1_99_non-reduced_galaxy_shapes_multi_scale_1024_MLP_only_cent.pkl', \"rb\" ) )\n",
    "tng = tng[tng['dm_mass']>0]\n",
    "tng = tng[log10(tng['dm_mass']*10**10)>9]\n",
    "tng = tng[log10(tng['mass']*10**10)>9]\n",
    "\n",
    "tng['mass'] = log10(tng['mass']*10**10)\n",
    "tng['mass'] = (tng['mass']  -jnp.mean(tng['mass']))/ jnp.std(tng['mass']) \n",
    "tng['dm_mass'] = log10(tng['dm_mass']*10**10)\n",
    "tng['dm_mass'] = (tng['dm_mass']  -jnp.mean(tng['dm_mass']))/ jnp.std(tng['dm_mass']) \n",
    "\n",
    "#tng = tng[tng['central_bool']==1.0]\n",
    "n_scalar = 2 #number of scalar features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9538258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=17457</i>\n",
       "<table id=\"table22903921838112\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>gal_id</th><th>a</th><th>b</th><th>c</th><th>av_x</th><th>av_y</th><th>av_z</th><th>bv_x</th><th>bv_y</th><th>bv_z</th><th>cv_x</th><th>cv_y</th><th>cv_z</th><th>tid_av_x_0.1_1024</th><th>tid_av_y_0.1_1024</th><th>tid_av_z_0.1_1024</th><th>tid_bv_x_0.1_1024</th><th>tid_bv_y_0.1_1024</th><th>tid_bv_z_0.1_1024</th><th>tid_cv_x_0.1_1024</th><th>tid_cv_y_0.1_1024</th><th>tid_cv_z_0.1_1024</th><th>tid_a_0.1_1024</th><th>tid_b_0.1_1024</th><th>tid_c_0.1_1024</th><th>tid_av_x_0.5_1024</th><th>tid_av_y_0.5_1024</th><th>tid_av_z_0.5_1024</th><th>tid_bv_x_0.5_1024</th><th>tid_bv_y_0.5_1024</th><th>tid_bv_z_0.5_1024</th><th>tid_cv_x_0.5_1024</th><th>tid_cv_y_0.5_1024</th><th>tid_cv_z_0.5_1024</th><th>tid_a_0.5_1024</th><th>tid_b_0.5_1024</th><th>tid_c_0.5_1024</th><th>tid_av_x_1.0_1024</th><th>tid_av_y_1.0_1024</th><th>tid_av_z_1.0_1024</th><th>tid_bv_x_1.0_1024</th><th>tid_bv_y_1.0_1024</th><th>tid_bv_z_1.0_1024</th><th>tid_cv_x_1.0_1024</th><th>tid_cv_y_1.0_1024</th><th>tid_cv_z_1.0_1024</th><th>tid_a_1.0_1024</th><th>tid_b_1.0_1024</th><th>tid_c_1.0_1024</th><th>tid_av_x_2.0_1024</th><th>tid_av_y_2.0_1024</th><th>tid_av_z_2.0_1024</th><th>tid_bv_x_2.0_1024</th><th>tid_bv_y_2.0_1024</th><th>tid_bv_z_2.0_1024</th><th>tid_cv_x_2.0_1024</th><th>tid_cv_y_2.0_1024</th><th>tid_cv_z_2.0_1024</th><th>tid_a_2.0_1024</th><th>tid_b_2.0_1024</th><th>tid_c_2.0_1024</th><th>mass</th><th>GroupID</th><th>tot_mass</th><th>dm_mass</th><th>central_bool</th><th>group_mass</th><th>group_x</th><th>group_y</th><th>group_z</th><th>gal_pos_x</th><th>gal_pos_y</th><th>gal_pos_z</th><th>dm_av_x</th><th>dm_av_y</th><th>dm_av_z</th><th>dm_bv_x</th><th>dm_bv_y</th><th>dm_bv_z</th><th>dm_cv_x</th><th>dm_cv_y</th><th>dm_cv_z</th><th>dm_a</th><th>dm_b</th><th>dm_c</th><th>mlp_av_x</th><th>mlp_av_y</th><th>mlp_av_z</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float64</th><th>float32</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>0.0</td><td>0.24562844247483243</td><td>0.11233008826530698</td><td>0.09435347356932028</td><td>0.15057356009311798</td><td>-0.36432524666271543</td><td>0.9190183445639349</td><td>-0.6040801058750838</td><td>0.7019725376704176</td><td>0.37725559246020707</td><td>0.7825693762771655</td><td>0.611965416507098</td><td>0.11438312949234405</td><td>0.8275753855705261</td><td>0.5501617810833461</td><td>-0.11153932645318682</td><td>-0.4364806852752775</td><td>0.7555949692944677</td><td>0.4884269175204382</td><td>0.3529923768186143</td><td>-0.355525325415916</td><td>0.8654467776217399</td><td>1054.244087225318</td><td>991.9027130436707</td><td>668.0261137005616</td><td>-0.9183833599090576</td><td>0.33972494317589014</td><td>0.20287679547321114</td><td>0.33534699111399274</td><td>0.9403888675675559</td><td>-0.05666721544068841</td><td>-0.2100343464963115</td><td>0.015991896046057438</td><td>-0.9775632115381181</td><td>59.10935058615099</td><td>55.377614125575626</td><td>37.975054282341716</td><td>0.9371282458305359</td><td>0.1784244452709745</td><td>-0.29992566920893826</td><td>-0.2053100234179685</td><td>0.9768323454844315</td><td>-0.06038512316371721</td><td>0.2822029128212493</td><td>0.11816634946180396</td><td>0.9520494891811419</td><td>13.122050507836377</td><td>11.700347539646893</td><td>9.282237732126015</td><td>-0.8248880505561829</td><td>0.33217499988277654</td><td>0.4574051124728157</td><td>0.5063412900419841</td><td>0.7939398479662237</td><td>0.3365679958195495</td><td>-0.2513526714837356</td><td>0.5092340201363678</td><td>-0.8231054290148635</td><td>2.5250096232145673</td><td>2.341083724345102</td><td>1.7858375325765965</td><td>4.7994876</td><td>0.0</td><td>27477.935546875</td><td>4.9791627</td><td>1.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8490914106369019</td><td>26.326995849609375</td><td>18.306934356689453</td><td>0.20913785733198287</td><td>-0.2376266091458571</td><td>0.9485752217175131</td><td>-0.0291868731156354</td><td>0.9680772186306577</td><td>0.24894703293283385</td><td>0.9774505015950008</td><td>0.07975019369363229</td><td>-0.19552601754628252</td><td>0.7422561814889702</td><td>0.380580342545896</td><td>0.2953753041796042</td><td>-0.8295962810516357</td><td>0.51895672082901</td><td>0.5298088788986206</td></tr>\n",
       "<tr><td>1.0</td><td>0.08088470650058953</td><td>0.04497213832247513</td><td>0.03777383880658522</td><td>-0.9840750455762398</td><td>-0.006859759853817557</td><td>0.17762108086843106</td><td>0.07866214907516236</td><td>0.8792771126813137</td><td>0.46977018361927686</td><td>-0.15940066178344028</td><td>0.4762611707976621</td><td>-0.8647350612837673</td><td>-0.25285616517066956</td><td>-0.542943046543665</td><td>-0.8007974784346137</td><td>-0.0015899779481896464</td><td>-0.8274600252025045</td><td>0.5615223759228073</td><td>0.9675025711735737</td><td>-0.14325765215776112</td><td>-0.20836511193280413</td><td>340.0297044488053</td><td>304.0921077455601</td><td>251.82406385313607</td><td>-0.43189844489097595</td><td>-0.6954925306839556</td><td>-0.5742419895334245</td><td>-0.7976906521230697</td><td>0.5916774294264403</td><td>-0.11665094522888154</td><td>-0.4208958853397719</td><td>-0.40768610384724513</td><td>0.810332459200483</td><td>19.602475741375493</td><td>17.40369819653049</td><td>5.434223834232313</td><td>0.46768128871917725</td><td>0.777220230781344</td><td>0.42095478906302564</td><td>0.802480656776426</td><td>-0.5730060672469052</td><td>0.1663996466279646</td><td>0.3705388199238754</td><td>0.25998607609337393</td><td>-0.8916884114795887</td><td>6.901495930274793</td><td>5.321455175040075</td><td>-0.37971657154338395</td><td>-0.3023521304130554</td><td>-0.8591983176887451</td><td>0.4127486479698836</td><td>-0.8679340461933621</td><td>0.4271650525814584</td><td>0.2534176578526006</td><td>-0.3940478232125578</td><td>-0.2816172379914081</td><td>-0.874881731600069</td><td>2.041410478830176</td><td>1.404129183064541</td><td>1.2895440686703488</td><td>3.3239326</td><td>0.0</td><td>3666.622802734375</td><td>3.639929</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.10647333413362503</td><td>24.63332176208496</td><td>16.90055274963379</td><td>-0.894172344445321</td><td>-0.431108424887396</td><td>-0.12083602285852724</td><td>-0.4150634625674986</td><td>0.6969958927253865</td><td>0.5847384437211395</td><td>0.16786345781806322</td><td>-0.5730115631599854</td><td>0.8021718070428016</td><td>0.2554090437812483</td><td>0.18938552774748685</td><td>0.17268245195140966</td><td>-0.6350778937339783</td><td>-0.5722015500068665</td><td>0.8127713799476624</td></tr>\n",
       "<tr><td>2.0</td><td>0.029657668413294556</td><td>0.027907432725562016</td><td>0.025816469838701374</td><td>-0.26077723253654983</td><td>0.4254462113472258</td><td>-0.8665972283828683</td><td>0.9558149884532077</td><td>-0.012390405194009673</td><td>-0.2937076534707986</td><td>0.13569429921250462</td><td>0.9048988888872719</td><td>0.40341673000980943</td><td>-0.880781888961792</td><td>0.44753759161619305</td><td>0.15470422818181326</td><td>0.47337510537536415</td><td>0.8403335008773494</td><td>0.26411288668688704</td><td>-0.011802700445901449</td><td>0.3058589730473757</td><td>-0.9520036685163501</td><td>382.3485118800341</td><td>379.8080753150767</td><td>229.73399493712893</td><td>0.9019741415977478</td><td>0.39395049048399733</td><td>0.17676446381355537</td><td>-0.4301801316453484</td><td>0.8551803839966307</td><td>0.2891566446840985</td><td>0.0372521000410273</td><td>0.3368523740079518</td><td>-0.9408202586933065</td><td>45.55879223874659</td><td>41.7458650873896</td><td>13.936163208857044</td><td>-0.8888373374938965</td><td>-0.36896232896961634</td><td>-0.27172599523478264</td><td>0.43208303553659616</td><td>-0.8722901430712391</td><td>-0.2289413826773946</td><td>-0.15255316150934772</td><td>-0.3208998381147507</td><td>0.934746397057207</td><td>12.192577897497326</td><td>10.585406962788612</td><td>7.440784034136231</td><td>0.8154981732368469</td><td>0.34012826538765034</td><td>-0.46826861664919506</td><td>-0.5509893239287479</td><td>0.7038767719183728</td><td>-0.4482948302962152</td><td>0.17712565926681462</td><td>0.623594622361482</td><td>0.7614172626038479</td><td>2.512468517480205</td><td>2.328128810399225</td><td>1.7195177630093514</td><td>3.4139335</td><td>0.0</td><td>773.4555053710938</td><td>2.6384254</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8531123399734497</td><td>26.72574806213379</td><td>17.510679244995117</td><td>-0.44998482120148936</td><td>0.5216721585493953</td><td>-0.7248253718535784</td><td>0.1483677084347021</td><td>-0.7566904268686213</td><td>-0.6367154945336407</td><td>0.8806251664483017</td><td>0.39405298740118433</td><td>-0.2630999037211969</td><td>0.06412549909992203</td><td>0.06279831222742362</td><td>0.05986077141319621</td><td>-0.5391837954521179</td><td>-0.1834503561258316</td><td>0.2371581792831421</td></tr>\n",
       "<tr><td>3.0</td><td>0.018082525231503666</td><td>0.014158971881358921</td><td>0.01222966521600501</td><td>-0.18327640118347924</td><td>0.8024303076072575</td><td>-0.5679043600841203</td><td>0.9766838307247415</td><td>0.21432650432834124</td><td>-0.0123630232240266</td><td>-0.11179649176104173</td><td>0.5569288562964891</td><td>0.823001697115023</td><td>0.9721372723579407</td><td>0.13241262915181962</td><td>-0.19343225375726192</td><td>-0.2253100707952795</td><td>0.7555232597061101</td><td>-0.6151584966829927</td><td>0.06468781300011002</td><td>0.6416007322970851</td><td>0.7643062129572853</td><td>87.04871759916425</td><td>84.99975852487485</td><td>62.34963110912915</td><td>0.9601742029190063</td><td>0.09534201450948858</td><td>-0.26263176314282344</td><td>-0.11330683354213822</td><td>0.9920863922003901</td><td>-0.05409391725016924</td><td>0.2553959753302503</td><td>0.08169755614512844</td><td>0.9633786405692335</td><td>11.497520319686519</td><td>9.693721223067465</td><td>-3.8285132677798916</td><td>0.9453354477882385</td><td>0.18561979935204662</td><td>-0.2681158910993191</td><td>-0.207397898455762</td><td>0.9767054732929888</td><td>-0.05506841341142483</td><td>0.2516484704654678</td><td>0.1076647970695349</td><td>0.9618114881744583</td><td>6.208022837439617</td><td>5.142239415879281</td><td>-1.1519118757588624</td><td>0.9223912358283997</td><td>-0.07469076297867584</td><td>-0.3789666305250492</td><td>-0.341641069588688</td><td>-0.6155109176150187</td><td>-0.7102307300216</td><td>0.18021042338372953</td><td>-0.7845811622913088</td><td>0.593259305094725</td><td>2.089374436125829</td><td>1.9721288558379273</td><td>0.8042911796768404</td><td>2.4028754</td><td>0.0</td><td>339.1330871582031</td><td>2.074516</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.24545502662658691</td><td>26.517372131347656</td><td>15.888749122619629</td><td>0.07192887265149081</td><td>-0.602036263783214</td><td>0.7952223427249978</td><td>-0.06875971805707189</td><td>0.7923976582805746</td><td>0.6061171935559767</td><td>0.9950368528108882</td><td>0.09827659050562189</td><td>-0.015600426493936224</td><td>0.11019573989878745</td><td>0.08198181434064328</td><td>0.06754900596157654</td><td>-0.36452817916870117</td><td>-0.13807687163352966</td><td>0.38148462772369385</td></tr>\n",
       "<tr><td>4.0</td><td>0.01801164473834766</td><td>0.0168904028181911</td><td>0.013904951566977544</td><td>-0.1142639508855221</td><td>-0.979945945153804</td><td>-0.16324734026822255</td><td>-0.9934460645980511</td><td>0.1131971410280455</td><td>0.01585320149754536</td><td>-0.0029438516735695247</td><td>-0.16398887718285846</td><td>0.9864577952946744</td><td>0.7485275268554688</td><td>0.6600091975474459</td><td>0.06398781308005926</td><td>0.09984522831373491</td><td>-0.016784453435366945</td><td>-0.9948614036667893</td><td>-0.65554367623597</td><td>0.7510699975872106</td><td>-0.0784623939946852</td><td>95.48307820950194</td><td>84.8429631454686</td><td>74.73061250881207</td><td>-0.9893072247505188</td><td>-0.04843766873430373</td><td>0.137568123101957</td><td>0.08901581515380513</td><td>0.546642973525605</td><td>0.8326209486600661</td><td>0.1155308655672284</td><td>-0.8359636665206632</td><td>0.536486129698258</td><td>8.999897029588341</td><td>8.000785662629548</td><td>-3.7907557456801593</td><td>0.9832291603088379</td><td>-0.16407630175885704</td><td>-0.07962015991931036</td><td>-0.17963702753860372</td><td>-0.9466587978466704</td><td>-0.26752132400351225</td><td>-0.031479215389444815</td><td>0.2773374977543313</td><td>-0.9602567215790949</td><td>4.977716213338439</td><td>4.500357897673841</td><td>-1.9989919689628635</td><td>0.951259195804596</td><td>0.08308734405975979</td><td>-0.296988995835959</td><td>-0.30827944117927625</td><td>0.28227597115623293</td><td>-0.9084514638955671</td><td>-0.008352037880026354</td><td>-0.955728397278694</td><td>-0.2941317291594872</td><td>1.9237631510049509</td><td>1.8755596258083822</td><td>0.5238074617832662</td><td>2.2063403</td><td>0.0</td><td>325.36669921875</td><td>2.0665267</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.7687843441963196</td><td>26.51828956604004</td><td>15.530523300170898</td><td>-0.7731495452799411</td><td>-0.025394237333196023</td><td>-0.6337151673612411</td><td>-0.38358190855097435</td><td>0.8144575707517007</td><td>0.4353432954320422</td><td>0.5050789047919271</td><td>0.5796671442780793</td><td>-0.6394382704989464</td><td>0.07512998409324027</td><td>0.07145462279941757</td><td>0.06208312340477497</td><td>-0.3841070532798767</td><td>0.06337937712669373</td><td>0.3301573693752289</td></tr>\n",
       "<tr><td>5.0</td><td>0.020714045769373805</td><td>0.015366443958998062</td><td>0.011101178523383154</td><td>0.4543514501412507</td><td>0.4368184773811964</td><td>0.7763725765203948</td><td>0.0989417327520005</td><td>0.8413858812125629</td><td>-0.5313006045697589</td><td>0.885310845564751</td><td>-0.3182128481292561</td><td>-0.3390653182071158</td><td>0.7768359780311584</td><td>0.15880068292379038</td><td>-0.6093506734152345</td><td>0.12258095443257752</td><td>-0.9873034088378564</td><td>-0.10102419763376748</td><td>0.6176567086165746</td><td>-0.0037844418460261536</td><td>0.7864387250770741</td><td>150.6719051410386</td><td>144.29760338145712</td><td>32.579406564684625</td><td>0.8302285671234131</td><td>0.06331149379261101</td><td>-0.5538160138923615</td><td>-0.010964905134195126</td><td>-0.9914818694743903</td><td>-0.12978240774067612</td><td>0.5573152549009173</td><td>-0.11382160277081475</td><td>0.8224623695935326</td><td>30.224059047691718</td><td>27.72795609148564</td><td>-5.601407306425997</td><td>0.8750212788581848</td><td>-0.013376262889044099</td><td>-0.4838995982078433</td><td>-0.07984628679003236</td><td>-0.9899146088434846</td><td>-0.11701981748445146</td><td>0.4774539936375365</td><td>-0.14103241751233508</td><td>0.8672644009586659</td><td>10.214945917008441</td><td>9.040170727109352</td><td>3.5761973690223474</td><td>-0.8614780902862549</td><td>0.16273267542655348</td><td>0.4810130892316824</td><td>0.32598990654872145</td><td>0.9035240204531997</td><td>0.2781634866269143</td><td>-0.3893405918884474</td><td>0.39643715871825347</td><td>-0.8314153490857331</td><td>2.3269508673959876</td><td>2.003771712772918</td><td>1.3924634090971475</td><td>2.491318</td><td>0.0</td><td>298.01812744140625</td><td>2.0118546</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>1.4265923500061035</td><td>26.341442108154297</td><td>19.04120445251465</td><td>0.3918429756654348</td><td>0.3654932744319969</td><td>0.8443185114437765</td><td>0.011168222931565</td><td>0.9157482681432102</td><td>-0.4015972860830387</td><td>0.9199643217093848</td><td>-0.1667926129589919</td><td>-0.35474761598085974</td><td>0.062249062652563976</td><td>0.04554774917427718</td><td>0.038746376495772214</td><td>-0.9575876593589783</td><td>0.2240697145462036</td><td>0.34571993350982666</td></tr>\n",
       "<tr><td>6.0</td><td>0.014119734858107792</td><td>0.011844378746852749</td><td>0.007982621187688364</td><td>-0.22290355721159702</td><td>0.2984316518790147</td><td>-0.9280369353313361</td><td>0.9394537387980854</td><td>-0.18840792189093264</td><td>-0.2862326459840651</td><td>-0.26027039178653266</td><td>-0.9356500436196766</td><td>-0.238365935137095</td><td>-0.9105023145675659</td><td>-0.0988293550637392</td><td>0.40151996746038326</td><td>0.41098426612009176</td><td>-0.10925613157905309</td><td>0.9050718373223812</td><td>0.04557914757245563</td><td>-0.9890883966006611</td><td>-0.1400952712139218</td><td>335.3875231376624</td><td>294.41755624107986</td><td>-42.357889451741656</td><td>0.8969227075576782</td><td>0.3906390273337473</td><td>0.20719759058429238</td><td>-0.4393241654870583</td><td>0.8404695553081605</td><td>0.3171832344547916</td><td>0.05023911662531079</td><td>0.37551574790392966</td><td>-0.9254533776678657</td><td>54.147985187340716</td><td>50.0636499904205</td><td>27.867718833217868</td><td>0.9127143025398254</td><td>-0.27453073013282103</td><td>0.30263098525806825</td><td>-0.3416022979844858</td><td>-0.919069329745677</td><td>0.1965182870181554</td><td>0.22418854796186125</td><td>-0.2827444877156255</td><td>-0.9326280339069702</td><td>12.92535772244448</td><td>11.247231530013353</td><td>9.113070946326427</td><td>-0.8143582344055176</td><td>0.3390340504565971</td><td>0.4710377874775135</td><td>0.5378165677834383</td><td>0.745882999514194</td><td>0.39295278400000105</td><td>-0.21811470381054063</td><td>0.5733362570662202</td><td>-0.789754083443028</td><td>2.539618561436438</td><td>2.3344824030547318</td><td>1.80247327951178</td><td>2.8106654</td><td>0.0</td><td>206.8306121826172</td><td>1.7277424</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.9047555923461914</td><td>26.611112594604492</td><td>17.832447052001953</td><td>0.11291700663663792</td><td>-0.287765607234945</td><td>0.9510208751152229</td><td>0.978800736446974</td><td>-0.13239059141012582</td><td>-0.15627491685148434</td><td>0.17087676244323766</td><td>0.9485060287624851</td><td>0.26671603899678925</td><td>0.03274124656447151</td><td>0.02647658482026913</td><td>0.021759721217229116</td><td>-0.6157798171043396</td><td>0.1805431842803955</td><td>0.8364304900169373</td></tr>\n",
       "<tr><td>7.0</td><td>0.014264607282143125</td><td>0.012930660989750504</td><td>0.010384966439804553</td><td>-0.32911445366610587</td><td>0.2580640586333649</td><td>0.9083427866338432</td><td>0.9287466042611946</td><td>-0.08535148538246126</td><td>0.3607559687881924</td><td>-0.17062655555733508</td><td>-0.9623502821658456</td><td>0.2115857106563602</td><td>-0.5996925234794617</td><td>-0.7457408138106868</td><td>0.2902404295058079</td><td>0.09643116278398547</td><td>0.29270824713300736</td><td>0.9513269222010118</td><td>0.7943990805299755</td><td>-0.5984918718556634</td><td>0.10362229574678217</td><td>106.19326795531998</td><td>81.73599148245327</td><td>-10.20633132785529</td><td>0.778051495552063</td><td>0.5637893944151302</td><td>-0.2770873032260318</td><td>-0.2447324288495145</td><td>-0.13420077738774477</td><td>-0.960258397317067</td><td>-0.57856883180264</td><td>0.8149427403772278</td><td>0.0335624309754635</td><td>28.03821793521729</td><td>22.953089681994513</td><td>-3.2529257228048234</td><td>0.8548998832702637</td><td>0.47899385711488035</td><td>-0.19927645448558487</td><td>-0.37472315456571015</td><td>0.3044741498035521</td><td>-0.8757157355750365</td><td>-0.35878792886391675</td><td>0.8233227659597501</td><td>0.4397849987822771</td><td>9.766195040270658</td><td>8.692339146168745</td><td>5.119134099857715</td><td>0.8400160074234009</td><td>0.20962384116815538</td><td>0.5004307851097664</td><td>-0.5422037651656503</td><td>0.3578287100487087</td><td>0.7602458097911947</td><td>0.019702855424065132</td><td>0.9099540974570008</td><td>-0.41424067643020446</td><td>2.3679617471799905</td><td>2.1642857947847527</td><td>1.5947039668881267</td><td>2.4935412</td><td>0.0</td><td>137.8258819580078</td><td>1.4610503</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>1.4113867282867432</td><td>26.66329574584961</td><td>17.27666473388672</td><td>-0.6442798631117556</td><td>0.13761416014325387</td><td>0.7523069858221204</td><td>0.7423171156837277</td><td>-0.12418326258128562</td><td>0.6584404430604626</td><td>-0.18403466453820141</td><td>-0.9826702703464344</td><td>0.02214457101813702</td><td>0.03617064514392236</td><td>0.0322852986267902</td><td>0.028433050752495505</td><td>-0.487853080034256</td><td>-0.006067287176847458</td><td>0.6665339469909668</td></tr>\n",
       "<tr><td>8.0</td><td>0.012384133540565926</td><td>0.011480578764441723</td><td>0.008352121662922892</td><td>-0.09489200944034107</td><td>0.9845018255922565</td><td>-0.14748444646771572</td><td>-0.6070729329106967</td><td>-0.1746455075092532</td><td>-0.7752170024154785</td><td>0.7889600501112708</td><td>-0.015971916364344224</td><td>-0.6142368738655077</td><td>0.7868995070457458</td><td>0.6145145813034041</td><td>0.05622298256882669</td><td>0.6145898622580878</td><td>-0.7886416891792428</td><td>0.017988532405250793</td><td>0.05539400340305002</td><td>0.020398908157355643</td><td>-0.9982561740019295</td><td>297.00356951122933</td><td>154.70036110379576</td><td>106.36466728451252</td><td>-0.9773894548416138</td><td>-0.19233808125036161</td><td>-0.08784018444010429</td><td>0.20084339955830172</td><td>-0.9743790274684</td><td>-0.10122963836544814</td><td>-0.06611931907851548</td><td>-0.11658290325199669</td><td>0.9909776295729025</td><td>45.70994895845578</td><td>41.547306769246795</td><td>14.042110243323462</td><td>0.9484310746192932</td><td>-0.21409147544254528</td><td>0.23375905418496223</td><td>-0.25863785674287965</td><td>-0.9490173863896809</td><td>0.18020116367423786</td><td>0.1832618736400553</td><td>-0.2313673280152882</td><td>-0.9554497606870821</td><td>12.333956344572897</td><td>10.806718608133647</td><td>7.680217240448437</td><td>0.8317590355873108</td><td>-0.28411894710134616</td><td>0.4769206318809015</td><td>-0.5212707518563224</td><td>-0.695197018785786</td><td>0.4949524303713436</td><td>0.19092843809867752</td><td>-0.6602859448522115</td><td>-0.7263393164050955</td><td>2.5267100202069264</td><td>2.377367265966651</td><td>1.7441043594413514</td><td>2.6079419</td><td>0.0</td><td>118.54483032226562</td><td>1.3346524</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.8358985781669617</td><td>26.50518798828125</td><td>17.452577590942383</td><td>0.03019337371364592</td><td>-0.9973178994724258</td><td>0.06667358979158695</td><td>0.6088277874201042</td><td>0.0712522137829101</td><td>0.7900961000385804</td><td>0.7927276237453368</td><td>-0.016737067334008347</td><td>-0.6093461948089348</td><td>0.02507435545710483</td><td>0.023336459324641515</td><td>0.017664686905375673</td><td>-0.478249192237854</td><td>0.572478175163269</td><td>0.7418162822723389</td></tr>\n",
       "<tr><td>9.0</td><td>0.011825359956376158</td><td>0.010521218654665525</td><td>0.008978701775802936</td><td>-0.7609710684566479</td><td>-0.6463192607270818</td><td>0.05651943192518953</td><td>0.3723984291927872</td><td>-0.3637957790608959</td><td>0.8537985951453777</td><td>-0.5312649460548777</td><td>0.670763776862372</td><td>0.5175263401437973</td><td>0.411751389503479</td><td>-0.8524676327996239</td><td>0.3221175630061307</td><td>-0.4447841681946456</td><td>0.12051648053501705</td><td>0.8874924346961213</td><td>0.7953790499449934</td><td>0.5086990396574953</td><td>0.3295412780823392</td><td>267.7468038439064</td><td>172.7403347699184</td><td>-32.640283025697485</td><td>0.7494412660598755</td><td>0.6344754920526042</td><td>-0.18915235918686385</td><td>-0.43950295814616697</td><td>0.26309741857929136</td><td>-0.8588462598845511</td><td>0.49515140591754225</td><td>-0.7267878633559586</td><td>-0.47602992226997876</td><td>49.09953130616094</td><td>41.87076494607203</td><td>29.394175175203834</td><td>0.8856460452079773</td><td>0.3705045218806867</td><td>-0.2799240623581826</td><td>-0.29344113471123306</td><td>0.9137431830026912</td><td>0.28100835570405097</td><td>0.3598935702128169</td><td>-0.16673270459653186</td><td>0.9179743042903749</td><td>12.130659111205967</td><td>11.040164845071411</td><td>9.127062973829954</td><td>0.8107178211212158</td><td>0.3727685313728518</td><td>-0.4514201993941928</td><td>-0.5305426667923021</td><td>0.7938056589848028</td><td>-0.2973164214714939</td><td>0.24750970307419756</td><td>0.4805374051848933</td><td>0.8413220246150055</td><td>2.4672608847176476</td><td>2.348442180176855</td><td>1.8129556375128915</td><td>2.5268881</td><td>0.0</td><td>129.09141540527344</td><td>1.4090705</td><td>0.0</td><td>38878.03515625</td><td>849.0914306640625</td><td>26326.99609375</td><td>18306.93359375</td><td>0.4719037413597107</td><td>26.57210350036621</td><td>18.048410415649414</td><td>-0.010990762544732776</td><td>-0.937025356616335</td><td>0.34908836158874756</td><td>0.8748738199638653</td><td>0.16004706438643404</td><td>0.45714410892312607</td><td>-0.4842261891724978</td><td>0.31043263075797217</td><td>0.8180199138652835</td><td>0.03507434720214762</td><td>0.027104034426882407</td><td>0.02378750381252945</td><td>-0.7521599531173706</td><td>0.16906972229480743</td><td>0.4688679873943329</td></tr>\n",
       "<tr><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr>\n",
       "<tr><td>752194.0</td><td>0.0039049782377542867</td><td>0.0036617101517045876</td><td>0.00323593098287949</td><td>-0.8969425341511473</td><td>0.15923472866925656</td><td>0.41247835290612045</td><td>0.4385604363122114</td><td>0.20181889799140837</td><td>0.8757476098255571</td><td>-0.056203506404498144</td><td>-0.9663919668535962</td><td>0.25085400588525136</td><td>0.8813514113426208</td><td>-0.14239274624984932</td><td>-0.45049302620831444</td><td>-0.454290413917188</td><td>0.006483625748448637</td><td>-0.8908300524904308</td><td>0.1297685658002307</td><td>0.9897890019658634</td><td>-0.05897330682241867</td><td>5.970796756331006</td><td>3.5133344809792804</td><td>-2.9648340086428737</td><td>-0.87455153465271</td><td>-0.055496988388769694</td><td>-0.4817465506771193</td><td>0.45307553636173487</td><td>0.2606185793915024</td><td>-0.8525259611451617</td><td>-0.17286472503102496</td><td>0.9638454442262679</td><td>0.20278004459072446</td><td>3.5582254513528113</td><td>1.9449147384840677</td><td>-3.678877401717923</td><td>-0.8715538382530212</td><td>-0.09990647198693227</td><td>0.4800131555179455</td><td>0.40938610145545484</td><td>0.3904636323890103</td><td>0.8245854544658691</td><td>-0.2698091039054739</td><td>0.9151813200872844</td><td>-0.2994097506979705</td><td>2.484524160636765</td><td>1.2301506411263117</td><td>-1.137547397261705</td><td>0.7137887477874756</td><td>0.5192677404498717</td><td>-0.46996452723553467</td><td>-0.14948485572206624</td><td>-0.5426113854083702</td><td>-0.8265755635965549</td><td>0.684222128435925</td><td>-0.6602529046912965</td><td>0.30968723061391806</td><td>1.0579058050113466</td><td>0.6455042915192071</td><td>0.11860523604323951</td><td>-0.6245405</td><td>34218.0</td><td>2.0303726196289062</td><td>-1.3926811</td><td>1.0</td><td>2.135425329208374</td><td>43591.21875</td><td>66893.1015625</td><td>46197.33203125</td><td>43.591217041015625</td><td>66.89310455322266</td><td>46.197330474853516</td><td>-0.33428342418923335</td><td>-0.5364417851953097</td><td>0.7749095453075738</td><td>0.8781455882307229</td><td>-0.4758345354840794</td><td>0.049414782318335675</td><td>-0.3422205694917228</td><td>-0.69700194112866</td><td>-0.6301375848809855</td><td>0.009796140220580327</td><td>0.00909373617652982</td><td>0.008591940835305619</td><td>-0.9059203267097473</td><td>-0.27984654903411865</td><td>0.3170780837535858</td></tr>\n",
       "<tr><td>752818.0</td><td>0.0026186262605587193</td><td>0.0024932728331082256</td><td>0.002279924441617432</td><td>-0.9030752217638134</td><td>-0.42804347083864136</td><td>-0.03512735271335533</td><td>-0.37217532460122005</td><td>0.7391351649699659</td><td>0.561395347026316</td><td>-0.21433775121106394</td><td>0.5200557614114376</td><td>-0.8268018707214889</td><td>-0.20766738057136536</td><td>-0.6820985198038517</td><td>0.7011532425744401</td><td>0.5806113379797836</td><td>0.4909114048286273</td><td>0.6495355777927098</td><td>0.7872513794847126</td><td>-0.5419848723816729</td><td>-0.2940878501550836</td><td>11.468423455048686</td><td>10.651956184244154</td><td>-9.081636927805297</td><td>-0.2167843133211136</td><td>-0.9318600698414162</td><td>-0.2909319059122004</td><td>0.5927500302742935</td><td>-0.36244097476253795</td><td>0.7192245417274118</td><td>0.7756622752541074</td><td>0.016533301192812565</td><td>-0.6309316006504104</td><td>5.571626128709312</td><td>5.305849647593844</td><td>-1.665199122983732</td><td>-0.0744098424911499</td><td>-0.7523948560910872</td><td>0.6544961084557119</td><td>0.5978332116983757</td><td>0.49164441112732254</td><td>0.6331518174953567</td><td>0.798159524435062</td><td>-0.43839223708334535</td><td>-0.41322345047039083</td><td>1.7220040774830347</td><td>1.5588226742413658</td><td>0.7511200728909818</td><td>0.44135501980781555</td><td>-0.5377206255924155</td><td>0.7183747397692141</td><td>0.18492605354786965</td><td>0.8378870880291757</td><td>0.5135636108927497</td><td>0.8780706649496923</td><td>0.09381767704697398</td><td>-0.4692442336654916</td><td>0.38305216297663175</td><td>0.3019758838822295</td><td>0.19838676700390007</td><td>-1.0096263</td><td>34416.0</td><td>2.374236822128296</td><td>-1.2458875</td><td>1.0</td><td>2.6618595123291016</td><td>22884.365234375</td><td>24484.498046875</td><td>55708.71875</td><td>22.88436508178711</td><td>24.484498977661133</td><td>55.708717346191406</td><td>-0.74311389551055</td><td>-0.6587438549240829</td><td>-0.11763618447949593</td><td>0.4915361967780833</td><td>-0.41806821367418684</td><td>-0.7639444586959265</td><td>-0.4540637681604565</td><td>0.6255201853777647</td><td>-0.6344687479528792</td><td>0.009999610624272874</td><td>0.008407642147943567</td><td>0.00785384932863061</td><td>-0.7071279287338257</td><td>0.4234252870082855</td><td>-0.11567516624927521</td></tr>\n",
       "<tr><td>755536.0</td><td>0.0021613513097696055</td><td>0.00207422701044982</td><td>0.0019470556776861182</td><td>0.5680242291524903</td><td>-0.12896409205728354</td><td>-0.8128448425471861</td><td>-0.716978955399259</td><td>-0.5624660205781143</td><td>-0.4117926094645316</td><td>0.4040911439394667</td><td>-0.8167008256729397</td><td>0.41195886777059687</td><td>-0.3111162781715393</td><td>-0.9457294025660034</td><td>-0.093821912995568</td><td>0.700595806725478</td><td>-0.29493411861908686</td><td>0.6497533234028579</td><td>-0.6421621055735647</td><td>0.1364176044230847</td><td>0.7543328624478672</td><td>17.2170062071065</td><td>12.974222181701856</td><td>-16.882677835225092</td><td>0.3025914430618286</td><td>0.9404595212832804</td><td>-0.15483640782516903</td><td>-0.691771241037885</td><td>0.32845478490129515</td><td>0.6430940866998766</td><td>0.6556607159449793</td><td>-0.08748338758340188</td><td>0.749970454393671</td><td>10.921497738375654</td><td>7.986124265775706</td><td>-10.690819231498974</td><td>-0.4161469042301178</td><td>0.8774404260381898</td><td>-0.2385792142164934</td><td>0.6614251737142258</td><td>0.47214744851157725</td><td>0.5827465370477234</td><td>-0.6239699369996484</td><td>-0.08470587711562799</td><td>0.7768438917200332</td><td>6.391702243933468</td><td>5.435520623703876</td><td>-1.974914627062353</td><td>0.5831827521324158</td><td>-0.36280727781932565</td><td>-0.7268210102950337</td><td>-0.6401639153442851</td><td>0.3455485194849024</td><td>-0.6861387484852224</td><td>0.5000880555822523</td><td>0.8654288531376353</td><td>-0.03073819807404793</td><td>2.0490855478253582</td><td>1.9420036362058934</td><td>0.7419671860135247</td><td>-0.820687</td><td>35267.0</td><td>2.1864006519317627</td><td>-1.3186811</td><td>1.0</td><td>2.3315136432647705</td><td>2042.494140625</td><td>25050.94921875</td><td>18758.775390625</td><td>2.042494058609009</td><td>25.050949096679688</td><td>18.75877571105957</td><td>-0.3263830133088009</td><td>-0.8374746126016356</td><td>0.43830400622308613</td><td>0.8027756152309624</td><td>-0.0007882010487470872</td><td>0.5962807143700495</td><td>0.49902448859150866</td><td>-0.5464756645879546</td><td>-0.6725614528049734</td><td>0.005858898899447545</td><td>0.005688195098892562</td><td>0.0056594885769899106</td><td>-0.9588258862495422</td><td>0.663828432559967</td><td>-0.3421096205711365</td></tr>\n",
       "<tr><td>755674.0</td><td>0.004436804083140114</td><td>0.004264740110807067</td><td>0.0025533748600972766</td><td>0.04403848197702088</td><td>-0.1135546899290897</td><td>-0.9925552601746</td><td>0.9505702597764065</td><td>-0.30091247648105296</td><td>0.07660197599706549</td><td>-0.30737077501573706</td><td>-0.9468669462459592</td><td>0.0946899824325525</td><td>-0.11105625331401825</td><td>0.9927880217499921</td><td>0.045149222326187496</td><td>-0.3622912634031452</td><td>-0.08274732416115264</td><td>0.9283845759198729</td><td>-0.9254250638861452</td><td>-0.08674574464436825</td><td>-0.3688680345562874</td><td>13.95188110612573</td><td>11.187302298978205</td><td>-19.386835812314224</td><td>0.16068865358829498</td><td>-0.978925184570336</td><td>0.1260342872967807</td><td>0.37727176411351526</td><td>0.17891567899753882</td><td>0.9086557080718338</td><td>0.9120554668237838</td><td>0.09846147887769592</td><td>-0.3980705497949664</td><td>11.8258019368186</td><td>9.33140442632036</td><td>-11.99921107508282</td><td>0.2940555512905121</td><td>-0.9373061462802995</td><td>0.18705217230597665</td><td>0.3416856022786474</td><td>0.2858612009989778</td><td>0.895284492750154</td><td>0.8926266163490134</td><td>0.19935034965757129</td><td>-0.40432309095167346</td><td>6.167059663873896</td><td>5.286718570452355</td><td>-0.206161498032284</td><td>0.2879871726036072</td><td>-0.8555263496191945</td><td>0.4302767120814721</td><td>0.29795075684016564</td><td>0.5070595174306202</td><td>0.8087743766226767</td><td>0.9101036920872033</td><td>0.10471538039028457</td><td>-0.4009313641497244</td><td>1.6106905869774604</td><td>1.4610559910002954</td><td>0.8732351221659276</td><td>-1.2674142</td><td>35311.0</td><td>2.6895902156829834</td><td>-1.1459374</td><td>1.0</td><td>2.8207802772521973</td><td>41948.828125</td><td>67385.875</td><td>43157.11328125</td><td>41.948829650878906</td><td>67.38587188720703</td><td>43.15711212158203</td><td>-0.7932919083736172</td><td>-0.10996820093798011</td><td>-0.5988279743727813</td><td>0.5489113093915441</td><td>-0.5547038022661512</td><td>-0.6252999809479729</td><td>0.2634090403365174</td><td>0.8247488627054532</td><td>-0.5004048270501155</td><td>0.011458019378859392</td><td>0.01017745813650446</td><td>0.00940806951299503</td><td>-0.996257483959198</td><td>-0.08073781430721283</td><td>0.3829881250858307</td></tr>\n",
       "<tr><td>756143.0</td><td>0.00308361038690644</td><td>0.002863228393033827</td><td>0.0026574845332567217</td><td>-0.8820282465768073</td><td>-0.30503057593444066</td><td>-0.3591413649048887</td><td>0.37735646811831136</td><td>-0.913722593613067</td><td>-0.15070871869365546</td><td>-0.28218481215316443</td><td>-0.2684536639089007</td><td>0.9210343979048611</td><td>0.0015297271311283112</td><td>0.6829034495813674</td><td>0.7305070420501512</td><td>-0.5062166100096855</td><td>0.630523231567244</td><td>-0.5883750489307832</td><td>0.8624050113985278</td><td>0.3688947451562144</td><td>-0.34666159768690435</td><td>10.484103245714167</td><td>9.364777762883662</td><td>-10.484911472825559</td><td>0.0693657174706459</td><td>0.6393084073479804</td><td>0.7658153546009777</td><td>-0.5144573365815175</td><td>0.6806359637661176</td><td>-0.5216017002135052</td><td>0.8547058241792225</td><td>0.3577980507525953</td><td>-0.37610970340016375</td><td>9.155412621463178</td><td>7.249615957388292</td><td>-9.763320299917943</td><td>0.11723411083221436</td><td>-0.664608436684243</td><td>-0.7379375237218778</td><td>-0.45003975370358634</td><td>-0.6979383065317145</td><td>0.5570873722873798</td><td>0.8852798332251028</td><td>-0.26679157813500975</td><td>0.38092239461754607</td><td>5.8597562089345825</td><td>4.904736729885101</td><td>-3.0848186345156816</td><td>0.1956349015235901</td><td>-0.7960476172685534</td><td>-0.5727435517287517</td><td>-0.32617973240589426</td><td>-0.6035965775051984</td><td>0.7275149165423553</td><td>0.9248425634525186</td><td>-0.044490028827478094</td><td>0.3777391562460853</td><td>2.021503591285496</td><td>1.8367751552424527</td><td>0.21233276768052678</td><td>-1.173998</td><td>35460.0</td><td>2.484553813934326</td><td>-1.2053144</td><td>1.0</td><td>2.7357537746429443</td><td>12748.4169921875</td><td>49639.87890625</td><td>47643.5</td><td>12.748416900634766</td><td>49.63987731933594</td><td>47.64350128173828</td><td>-0.955188023683456</td><td>0.29030062106486715</td><td>-0.057804747391940074</td><td>0.26884812485837184</td><td>0.932570293718068</td><td>0.24090108558230686</td><td>-0.12384072501334462</td><td>-0.2145651338963279</td><td>0.9688267534209717</td><td>0.00968518309011414</td><td>0.008876370829769229</td><td>0.008005922377726408</td><td>-0.8064053058624268</td><td>0.02114914357662201</td><td>0.5130857229232788</td></tr>\n",
       "<tr><td>759874.0</td><td>0.002820744583448637</td><td>0.002662717462739102</td><td>0.002319225147659849</td><td>0.04319607310607816</td><td>-0.9530069040905351</td><td>0.2998531974550014</td><td>-0.6074539930611559</td><td>0.2132287320806834</td><td>0.7652013814214669</td><td>0.7931795166179414</td><td>0.2152007169389496</td><td>0.5696972054053719</td><td>-0.1481405794620514</td><td>-0.82326837452263</td><td>-0.5479813409441071</td><td>0.67024069836565</td><td>-0.49101434161756335</td><td>0.5564910804138702</td><td>0.7272082045510493</td><td>0.2848404813869292</td><td>-0.6245271230274163</td><td>7.30261618068809</td><td>6.609193025087626</td><td>-6.649047371244596</td><td>0.22049525380134583</td><td>-0.6521221977135423</td><td>0.7253402541486638</td><td>-0.6843301402915557</td><td>-0.6333280678775361</td><td>-0.3613693644002592</td><td>-0.6950353258128171</td><td>0.41669196979035555</td><td>0.5859127052594989</td><td>5.071516011979067</td><td>4.657164687021211</td><td>-4.639407350910298</td><td>0.22362379729747772</td><td>0.36972846819821875</td><td>0.9018277329587392</td><td>-0.7103837628876122</td><td>0.6953354972725315</td><td>-0.10891949163670296</td><td>-0.6673434719507917</td><td>-0.6162867887895698</td><td>0.4181426603542181</td><td>2.510227887296019</td><td>2.4446278380397635</td><td>-0.29550932599063673</td><td>-0.33222824335098267</td><td>0.0033420409483209644</td><td>0.9431930972938731</td><td>0.7715653706591196</td><td>-0.5742085519298509</td><td>0.27380908986057984</td><td>0.5425046237776824</td><td>0.8187022472504417</td><td>0.1881897009055279</td><td>0.7182822949657577</td><td>0.6025217404174937</td><td>0.2306177146392478</td><td>-1.1865146</td><td>36674.0</td><td>2.4078431129455566</td><td>-1.2269256</td><td>1.0</td><td>2.522681713104248</td><td>56744.86328125</td><td>8870.302734375</td><td>35271.48046875</td><td>56.7448616027832</td><td>8.8703031539917</td><td>35.271480560302734</td><td>-0.49254175439093084</td><td>-0.7410512703662206</td><td>-0.4563393856222735</td><td>-0.3426860771911116</td><td>-0.31684952479878653</td><td>0.8844052414669142</td><td>-0.7999805451889888</td><td>0.5919876631513475</td><td>-0.09788633201696292</td><td>0.010622851462192457</td><td>0.00982369881825927</td><td>0.009650462343434934</td><td>-0.8629167675971985</td><td>0.048090964555740356</td><td>-0.16218844056129456</td></tr>\n",
       "<tr><td>765609.0</td><td>0.004260902628739046</td><td>0.0034987491021631826</td><td>0.0034106950579482743</td><td>-0.7254811450662946</td><td>-0.29035294328760564</td><td>-0.6239970164011381</td><td>-0.6037416947992897</td><td>-0.1667834636273663</td><td>0.7795378388643749</td><td>0.33041348960685485</td><td>-0.9422730201935549</td><td>0.05429991989991641</td><td>0.39381176233291626</td><td>-0.6548285431766336</td><td>-0.6450673468188927</td><td>-0.2761178421602759</td><td>-0.7536412095371603</td><td>0.5964762061709743</td><td>0.8767389806159819</td><td>0.05678473675674944</td><td>0.47760261048272723</td><td>10.481565932624203</td><td>6.6920962085506615</td><td>-7.170262658610948</td><td>0.4483642280101776</td><td>-0.8762701314169516</td><td>-0.17640911969134018</td><td>-0.19613505366352707</td><td>-0.28899686470826963</td><td>0.9370228668037912</td><td>0.87206683313159</td><td>0.38552752038642946</td><td>0.30144314484912554</td><td>4.729470471816316</td><td>4.465591796277699</td><td>-0.3970924795654311</td><td>0.48078492283821106</td><td>0.7879133844221842</td><td>-0.38475751826720556</td><td>-0.000794233791429596</td><td>-0.4384093620450877</td><td>-0.8987750555416539</td><td>0.8768381939716388</td><td>-0.4324230913348613</td><td>0.21015482786021653</td><td>1.7567807726870621</td><td>1.426349191737206</td><td>0.2031151300820342</td><td>0.4790285527706146</td><td>0.7892295986942002</td><td>-0.3842502908566294</td><td>0.13989461687933577</td><td>-0.5007876366108205</td><td>-0.8541903998441644</td><td>0.8665801415025594</td><td>-0.35542704393833213</td><td>0.3503005492297478</td><td>0.682993241841505</td><td>0.39078392445802157</td><td>-0.28050173093942027</td><td>-1.0077474</td><td>38584.0</td><td>1.9691458940505981</td><td>-1.3810116</td><td>1.0</td><td>2.1477677822113037</td><td>69493.40625</td><td>24621.359375</td><td>14227.939453125</td><td>69.493408203125</td><td>24.62135887145996</td><td>14.22793960571289</td><td>-0.6743944499030639</td><td>-0.010403297096392141</td><td>-0.7382979732800758</td><td>-0.717860103919018</td><td>-0.22480331505028345</td><td>0.6588932696148744</td><td>0.17282649432688355</td><td>-0.9743486239287901</td><td>-0.1441379960550045</td><td>0.010279716595123521</td><td>0.008949812636168017</td><td>0.008544749853599294</td><td>-0.6449933052062988</td><td>-0.14330190420150757</td><td>0.025591935962438583</td></tr>\n",
       "<tr><td>768819.0</td><td>0.0021323759568082227</td><td>0.0019531829151296413</td><td>0.00168196992926417</td><td>0.22650844435540055</td><td>0.8825824324248946</td><td>0.4119977847157102</td><td>0.22960971113889792</td><td>0.36268625898340107</td><td>-0.9031821843323399</td><td>0.9465586644188557</td><td>-0.299177083881071</td><td>0.12049799705393582</td><td>0.5274119973182678</td><td>0.05384076766902656</td><td>-0.8479019808409981</td><td>-0.8137323925258569</td><td>0.3189620539130704</td><td>-0.4859041073274638</td><td>0.24428710717396201</td><td>0.9462369575853424</td><td>0.21203638689671114</td><td>19.25813556124239</td><td>5.157875753926299</td><td>-12.949636135332824</td><td>-0.30958840250968933</td><td>0.22900413062259264</td><td>0.9228825088615509</td><td>0.8808795210692636</td><td>-0.2964034887415335</td><td>0.3690477492447729</td><td>0.3580590542991266</td><td>0.9272012079476807</td><td>-0.1099619644003186</td><td>10.042317755658667</td><td>4.536471533194636</td><td>-2.1186116933800037</td><td>-0.13278178870677948</td><td>-0.28319771340635713</td><td>0.9498252751686096</td><td>0.5369329413552438</td><td>0.7849578247187383</td><td>0.3091022968216713</td><td>0.8331098455277792</td><td>-0.5510356327264482</td><td>-0.047830082066023676</td><td>4.374354019368073</td><td>2.568492555061436</td><td>1.041868943709825</td><td>-0.021763809025287628</td><td>-0.18826020958231326</td><td>0.9818780118442816</td><td>0.1488672411019617</td><td>0.9705517113362551</td><td>0.18938827880562378</td><td>0.9886176617792873</td><td>-0.1502912808857845</td><td>-0.006902876774547467</td><td>1.1737398506529195</td><td>1.0210257779570981</td><td>0.46605677495209324</td><td>-1.1968647</td><td>39684.0</td><td>1.8780118227005005</td><td>-1.4170407</td><td>1.0</td><td>2.0431861877441406</td><td>36158.20703125</td><td>74774.9453125</td><td>70913.0546875</td><td>36.158206939697266</td><td>74.77494812011719</td><td>70.91305541992188</td><td>-0.25880737812234933</td><td>0.5373878123052639</td><td>-0.8026413147945962</td><td>0.9001275221184105</td><td>-0.16727014441858473</td><td>-0.4022326972178621</td><td>0.35041287783756714</td><td>0.8265803275979584</td><td>0.4404268123918498</td><td>0.007447231116267198</td><td>0.006435803850254935</td><td>0.005884069490372308</td><td>-0.7623950839042664</td><td>-0.16326552629470825</td><td>-0.24580833315849304</td></tr>\n",
       "<tr><td>770944.0</td><td>0.003327517241708196</td><td>0.002747715967309195</td><td>0.002407341366642528</td><td>0.14292035662607</td><td>0.6694683238880966</td><td>0.72896223151295</td><td>-0.5992812437275111</td><td>-0.5276248767513952</td><td>0.6020581204082236</td><td>-0.7876774483113533</td><td>0.522899754009827</td><td>-0.3257914742257349</td><td>0.05435025691986084</td><td>0.7641332931650193</td><td>0.6427646223097467</td><td>0.9839356278544618</td><td>0.06864069457954995</td><td>-0.16480028909648037</td><td>0.1700491977483439</td><td>-0.6413959505051615</td><td>0.7481273320904169</td><td>16.031065234256847</td><td>13.097033127661414</td><td>-18.57526951137799</td><td>0.042064864188432693</td><td>0.5122840869476556</td><td>-0.8577852652925486</td><td>0.9745662965674106</td><td>0.16810868869776616</td><td>0.1481890764503211</td><td>0.22011606185756488</td><td>-0.8422021628115925</td><td>-0.49218333603220676</td><td>8.338396006391417</td><td>7.324287751073336</td><td>-2.9798515915973374</td><td>-0.08957286179065704</td><td>-0.5715592807471986</td><td>-0.8156572142414509</td><td>0.8270799596506596</td><td>-0.49895586369803363</td><td>0.25880839713118475</td><td>0.5549012911290102</td><td>0.6514315272329814</td><td>-0.5174181311379249</td><td>2.724508155446739</td><td>1.8929122368676128</td><td>1.1767791243203767</td><td>-0.5422424077987671</td><td>-0.4646034656564396</td><td>-0.7000834361301348</td><td>0.09872729271140354</td><td>-0.8626714405853982</td><td>0.4960351875343029</td><td>0.8344016535923378</td><td>-0.19985395992659125</td><td>-0.5136460602243853</td><td>0.7937468924310519</td><td>0.47057788766000547</td><td>0.06254334276224803</td><td>-1.1959074</td><td>40428.0</td><td>1.8275648355484009</td><td>-1.4227736</td><td>1.0</td><td>2.186333417892456</td><td>23255.451171875</td><td>67403.6328125</td><td>73682.65625</td><td>23.255451202392578</td><td>67.40363311767578</td><td>73.68265533447266</td><td>-0.029873393774191622</td><td>0.959348917237283</td><td>0.28063719521842584</td><td>0.9545157855163463</td><td>-0.055948303086403764</td><td>0.29286413672873035</td><td>0.2966600673237379</td><td>0.2766214785178561</td><td>-0.9140423196319035</td><td>0.010272358832846969</td><td>0.008207857856765633</td><td>0.007920067444851958</td><td>-0.7486265897750854</td><td>0.3115461468696594</td><td>0.6876575946807861</td></tr>\n",
       "<tr><td>803349.0</td><td>0.0031508121209186802</td><td>0.0030230735357601266</td><td>0.0021085432756289216</td><td>-0.32068781961542403</td><td>0.8583297912918535</td><td>0.4005362552019344</td><td>0.11450726335093839</td><td>0.4549004614612307</td><td>-0.8831498495738073</td><td>0.9402379533874308</td><td>0.23735108919750747</td><td>0.24416603258146458</td><td>-0.653459370136261</td><td>-0.6424877035768541</td><td>0.40025042264332955</td><td>0.5947728860507294</td><td>-0.10873011102233433</td><td>0.7965067337919722</td><td>-0.46822650938675986</td><td>0.7585428885103405</td><td>0.4531849756974288</td><td>6.6748121996964525</td><td>2.1377852490317872</td><td>-5.148943097278041</td><td>0.7105637788772583</td><td>-0.5531318755114786</td><td>-0.4349071783190374</td><td>-0.484342016331643</td><td>0.06385517200989986</td><td>-0.8725453158566515</td><td>0.5104036997084477</td><td>0.8306429108231407</td><td>-0.2225318359767618</td><td>4.223898005240051</td><td>2.4463229500904813</td><td>-2.6088281377861557</td><td>-0.7002483010292053</td><td>-0.4427910772855235</td><td>0.5599896365228617</td><td>0.42053104198821856</td><td>0.37802810777223383</td><td>0.8247717214226297</td><td>-0.5768933817101604</td><td>0.8130380136318779</td><td>-0.07850614325354002</td><td>2.4838324889453736</td><td>1.8398044371683278</td><td>-1.1980335771197033</td><td>-0.641701877117157</td><td>-0.4435216393631488</td><td>0.6257054349642287</td><td>0.3951264139991685</td><td>0.5080488462491738</td><td>0.7653505646336464</td><td>-0.65733846143924</td><td>0.7383596178296242</td><td>-0.15076876954855106</td><td>0.9848531553910784</td><td>0.7312008380597477</td><td>0.03994652212845551</td><td>-1.2654737</td><td>52574.0</td><td>1.3310412168502808</td><td>-1.6483018</td><td>1.0</td><td>1.3615388870239258</td><td>45466.94140625</td><td>36893.8515625</td><td>72491.796875</td><td>45.466941833496094</td><td>36.89385223388672</td><td>72.4917984008789</td><td>-0.38634791225497644</td><td>0.18658681399575996</td><td>0.9032832620718336</td><td>-0.34642264167820824</td><td>0.8782718458782803</td><td>-0.3295905309171525</td><td>0.8548255050177862</td><td>0.44025438734948485</td><td>0.27468059704066633</td><td>0.007025950808725896</td><td>0.006659610929452984</td><td>0.006428501890477896</td><td>-0.2274145483970642</td><td>-0.12203589081764221</td><td>0.053576335310935974</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=17457>\n",
       " gal_id            a           ...        mlp_av_y             mlp_av_z      \n",
       "float64         float64        ...        float64              float64       \n",
       "-------- --------------------- ... --------------------- --------------------\n",
       "     0.0   0.24562844247483243 ...      0.51895672082901   0.5298088788986206\n",
       "     1.0   0.08088470650058953 ...   -0.5722015500068665   0.8127713799476624\n",
       "     2.0  0.029657668413294556 ...   -0.1834503561258316   0.2371581792831421\n",
       "     3.0  0.018082525231503666 ...  -0.13807687163352966  0.38148462772369385\n",
       "     4.0   0.01801164473834766 ...   0.06337937712669373   0.3301573693752289\n",
       "     5.0  0.020714045769373805 ...    0.2240697145462036  0.34571993350982666\n",
       "     6.0  0.014119734858107792 ...    0.1805431842803955   0.8364304900169373\n",
       "     7.0  0.014264607282143125 ... -0.006067287176847458   0.6665339469909668\n",
       "     8.0  0.012384133540565926 ...     0.572478175163269   0.7418162822723389\n",
       "     9.0  0.011825359956376158 ...   0.16906972229480743   0.4688679873943329\n",
       "     ...                   ... ...                   ...                  ...\n",
       "752194.0 0.0039049782377542867 ...  -0.27984654903411865   0.3170780837535858\n",
       "752818.0 0.0026186262605587193 ...    0.4234252870082855 -0.11567516624927521\n",
       "755536.0 0.0021613513097696055 ...     0.663828432559967  -0.3421096205711365\n",
       "755674.0  0.004436804083140114 ...  -0.08073781430721283   0.3829881250858307\n",
       "756143.0   0.00308361038690644 ...   0.02114914357662201   0.5130857229232788\n",
       "759874.0  0.002820744583448637 ...  0.048090964555740356 -0.16218844056129456\n",
       "765609.0  0.004260902628739046 ...  -0.14330190420150757 0.025591935962438583\n",
       "768819.0 0.0021323759568082227 ...  -0.16326552629470825 -0.24580833315849304\n",
       "770944.0  0.003327517241708196 ...    0.3115461468696594   0.6876575946807861\n",
       "803349.0 0.0031508121209186802 ...  -0.12203589081764221 0.053576335310935974"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e296451",
   "metadata": {},
   "source": [
    "## Let's build the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a62847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_key='GroupID'\n",
    "pos_key=['gal_pos_x', 'gal_pos_y', 'gal_pos_z']\n",
    "scalar_key = ['mass', 'dm_mass']\n",
    "catalog = tng\n",
    "\n",
    "# It takes a minute but we precompute all the graphs and data\n",
    "# Identify the individual groups and pre-extract the relevant data\n",
    "\n",
    "group_ids = catalog[group_key].astype(jnp.int32)\n",
    "gids, idx = jnp.unique(group_ids, return_index=True) # gids are the unique group ids, in other words All the host halo IDS uniquely  extracted     idx = The indices of the first occurrences of the unique values in the original array. i.e index of the central galaxy\n",
    "Position = jnp.array(catalog[pos_key].to_pandas())\n",
    "Scalars = jnp.array(catalog[scalar_key].to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091471bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_radius = 1 #Mpc/h\n",
    "graphs_list = []\n",
    "node_features_list = []\n",
    "positions_list = []\n",
    "direction_to_COM_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de74b5ed-3dae-4a49-bbc6-c8f91b3e5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:56<00:00, 181.61it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for gid in tqdm(gids):\n",
    "    \n",
    "    g = np.where(group_ids == gid)[0]\n",
    "    Positions_for_group = Position[g]\n",
    "    Features_for_group = Scalars[g]\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    # Compute adjacency matrix for each entry\n",
    "    graph = radius_neighbors_graph(Positions_for_group, graph_radius, mode='connectivity',\n",
    "                               include_self=False)\n",
    "    \n",
    "    Positions_for_group_COM =  (Positions_for_group - Positions_for_group[0])\n",
    "    directions_to_COM = Positions_for_group_COM#/jnp.linalg.norm(Positions_for_group_COM, axis=-1)[..., jnp.newaxis]\n",
    "    \n",
    " \n",
    "    directions_to_COM = directions_to_COM.at[jnp.isnan(directions_to_COM)].set(0.0)\n",
    "    \n",
    "    graphs_list.append(graph)\n",
    "    node_features_list.append(Features_for_group)\n",
    "    \n",
    "    direction_to_COM_list.append(directions_to_COM)\n",
    "    positions_list.append(Positions_for_group_COM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f81f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_node = graphs_list[0].tocoo().shape[0]\n",
    "max_n_edge = graphs_list[0].tocoo().nnz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "babd3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "senders_list = []\n",
    "receivers_list = []\n",
    "n_node_list = []\n",
    "n_edge_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62db2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12261171",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:23<00:00, 436.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for single_graph in tqdm(graphs_list):\n",
    "    single_graph =  single_graph.tocoo()\n",
    "    \n",
    " \n",
    "    senders = single_graph.row.astype(jnp.int64)\n",
    "    receivers = single_graph.col.astype(jnp.int64)\n",
    "    values = single_graph.data.astype(jnp.int64)\n",
    "    n_node = jnp.asarray([single_graph.shape[0]])\n",
    " \n",
    "    values = np.array([0]) if values.size == 0 else values\n",
    "    n_edge = np.array([np.sum(values)])\n",
    "    senders = np.repeat(senders, values)\n",
    "    receivers = np.repeat(receivers, values)\n",
    "    \n",
    "    if len(senders) < max_n_edge:\n",
    "        senders = jnp.concatenate( [senders, -1*jnp.ones( max_n_edge - len(senders)  )] )\n",
    "        receivers = jnp.concatenate( [receivers, -1*jnp.ones(max_n_edge - len(receivers)   )] )\n",
    "        \n",
    "    \n",
    "     \n",
    "    senders_list.append( jnp.asarray(senders).astype(jnp.int32)  )\n",
    "    receivers_list.append( jnp.asarray(receivers).astype(jnp.int32) )\n",
    "    \n",
    "    n_node_list.append( jnp.asarray(n_node).astype(jnp.int32) )\n",
    "    n_edge_list.append( jnp.asarray(n_edge).astype(jnp.int32) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f9da14",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_list_padded = []\n",
    "positions_list_padded = []\n",
    "direction_to_COM_list_padded = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f638bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:07<00:00, 1368.22it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for direction_to_COM in tqdm(direction_to_COM_list ):\n",
    "    if len(direction_to_COM) < max_n_node:\n",
    "        \n",
    "        direction_to_COM = jnp.concatenate( [direction_to_COM, 0*jnp.ones( (max_n_node-len(direction_to_COM), jnp.squeeze(direction_to_COM ).shape[-1])  )] )\n",
    "        \n",
    "    direction_to_COM_list_padded.append( jnp.asarray(direction_to_COM).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "280d4bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:07<00:00, 1436.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for node_features in tqdm(node_features_list ):\n",
    "    if len(node_features) < max_n_node:\n",
    "        \n",
    "        node_features = jnp.concatenate( [node_features, 0*jnp.ones( (max_n_node-len(node_features), jnp.squeeze(node_features ).shape[-1])  )] )\n",
    "        \n",
    "    node_features_list_padded.append( jnp.asarray(node_features).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea28dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10292/10292 [00:04<00:00, 2155.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#here padding the node features with np.inf\n",
    "for positions in tqdm(positions_list ):\n",
    "    if len(positions) < max_n_node:\n",
    "        \n",
    "        positions = jnp.concatenate( [positions, 0*jnp.ones( (max_n_node-len(positions), jnp.squeeze(positions ).shape[-1])  )] )\n",
    "        \n",
    "    positions_list_padded.append( jnp.asarray(positions).astype(jnp.float32)  )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbda61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c75c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b238a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "senders_list = jnp.array(senders_list)\n",
    "receivers_list = jnp.array(receivers_list)\n",
    "\n",
    "n_node_list = jnp.array(n_node_list)\n",
    "n_edge_list = jnp.array(n_edge_list)\n",
    "\n",
    "node_features_list_padded = jnp.array(node_features_list_padded)\n",
    "positions_list_padded = jnp.array(positions_list_padded)\n",
    "direction_to_COM_list_padded = jnp.array(direction_to_COM_list_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9dac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_feats = jnp.concatenate([node_features_list_padded,   direction_to_COM_list_padded, positions_list_padded], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bad61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP =  gn_graph.GraphsTuple(\n",
    "      nodes=concat_feats,  \n",
    "      edges=-1*jnp.ones_like(n_node_list),\n",
    "      receivers= receivers_list   ,\n",
    "      senders= senders_list ,\n",
    "      globals=  -1*jnp.ones_like(n_node_list),\n",
    "      n_node=n_node_list,\n",
    "      n_edge=n_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfc49d1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dset = tf.data.Dataset.from_tensor_slices(GP)\n",
    "\n",
    "dset = dset.repeat()\n",
    "dset = dset.shuffle(buffer_size=10000)\n",
    "dset = dset.batch(batch_size)\n",
    "dset = dset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dset = dset.as_numpy_iterator()\n",
    "_ = next(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4613b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = next(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358071b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd22c261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbf1c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dset2 = tf.data.Dataset.from_tensor_slices(GP)\n",
    "\n",
    "dset2 = dset2.repeat()\n",
    "dset2 = dset2.shuffle(buffer_size=10000)\n",
    "dset2 = dset2.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "dset2 = dset2.as_numpy_iterator()\n",
    "g_init = next(dset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c058acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_e(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim , in_dim ], activation=jax.nn.silu)(feats)\n",
    "  return net\n",
    "\n",
    "def f_inf(m_ij: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP( [1,], activation=jax.nn.sigmoid)(m_ij)\n",
    "  return net\n",
    "\n",
    "def f_h(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim,  ], activation=jax.nn.silu)(feats)\n",
    "  net = hk.Linear(in_dim)(net)\n",
    "  return net\n",
    "\n",
    "\n",
    "def f_x(feats: jnp.ndarray, in_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP([in_dim, in_dim  ], activation=jax.nn.silu)(feats)\n",
    "  net = hk.Linear(1)(net) \n",
    "  return net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a86f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT modified\n",
    "def GNN(add_self_edges: bool = False ) -> Callable:\n",
    "  def _ApplyGNN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    " \n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    \n",
    "    \n",
    "    # Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    \n",
    "    \n",
    "    # position of the nodes are the last 3 entries in the features matrix\n",
    "    distances = jnp.linalg.norm( nodes[:,-3:][senders] - nodes[:,-3:][receivers], axis=-1)\n",
    "    node_feat = nodes[:,:n_scalar]\n",
    "    \n",
    "    input_dim = nodes[:,:n_scalar].shape[-1]\n",
    "    \n",
    "    \n",
    "    concat_feats = jnp.concatenate([ node_feat[senders], node_feat[receivers], distances.reshape((-1,1))**2 ], axis=-1)\n",
    "    m_ij = f_e(concat_feats, input_dim)\n",
    "    \n",
    "    \n",
    "    # predict edges\n",
    "    e_ij = f_inf(m_ij, input_dim)\n",
    "    \n",
    "    m_i = jraph_utils.segment_sum( e_ij*m_ij, receivers, num_segments=sum_n_node)\n",
    "\n",
    "    concat_for_hl1_i =  jnp.concatenate([node_feat,  m_i], axis=-1)\n",
    "    \n",
    "    \n",
    "    new_node_feat = node_feat + f_h(concat_for_hl1_i, input_dim) #h^(l+1)_i    f_h is the node update func\n",
    "\n",
    "     \n",
    "    position_mlp = f_x(concat_feats, input_dim)\n",
    "    prefac_xi_xj = (nodes[:,-3:][senders] - nodes[:,-3:][receivers])/ (distances.reshape((-1,1))+1)\n",
    "    \n",
    "    sum_xi_xj =  jraph_utils.segment_sum( prefac_xi_xj*position_mlp, receivers, num_segments=sum_n_node)\n",
    "    #sum_xi_xj = jnp.einsum('ij,ij ->i', prefac_xi_xj, position_mlp ) #f_x\n",
    "    \n",
    "    x_Lplus1_i = nodes[:,-3:] + sum_xi_xj\n",
    "    \n",
    "    #x_Lplus1_i = x_Lplus1_i/jnp.linalg.norm(x_Lplus1_i, axis=-1)[..., jnp.newaxis]\n",
    "    #x_Lplus1_i = x_Lplus1_i.at[jnp.isnan(x_Lplus1_i)].set(0.0)\n",
    "    \n",
    "    \n",
    "    nodes = jnp.concatenate([new_node_feat, x_Lplus1_i],axis=-1)\n",
    "    #print(nodes.shape)\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  # pylint: enable=g-long-lambda\n",
    "  return _ApplyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0fb32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_expand_MLP(feats: jnp.ndarray, out_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.nets.MLP( [out_dim] )(feats)\n",
    "  return net\n",
    "\n",
    "\n",
    "def dim_expand(graph: jraph.GraphsTuple, out_dim) -> jraph.GraphsTuple:\n",
    "\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    node_feat = nodes[:,:n_scalar]\n",
    " \n",
    "    new_node = dim_expand_MLP(node_feat, out_dim)\n",
    "    \n",
    "    nodes = jnp.concatenate([new_node, nodes[:,n_scalar:]],axis=-1)\n",
    "\n",
    "     \n",
    "    return graph._replace(nodes=nodes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f07ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c0598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569b2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced5484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afca7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def gcn_definition(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a GCN for the karate club task.\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  graph = dim_expand(graph,   64)\n",
    "    \n",
    "  gn = GNN( \n",
    "      add_self_edges=False )\n",
    "  graph = gn(graph)\n",
    "  \n",
    "  \n",
    "\n",
    "  gn = GNN(add_self_edges=False) # output dim is 2 because we have 2 scalars.\n",
    "  graph = gn(graph)\n",
    "    \n",
    "  graph = dim_expand(graph,   0)\n",
    "   \n",
    "\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85f80bce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "network = hk.without_apply_rng(hk.transform(gcn_definition ))\n",
    "\n",
    "\n",
    "#params = jax.vmap(lambda x: network.init( next(rng) , x))  (g)\n",
    "\n",
    "params =  network.init( next(rng) , g_init )\n",
    "\n",
    "#out_graph = jax.vmap(lambda x,y: network.apply(x,y))   (params, g)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c6144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e4294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d89a2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def prediction_loss(params: hk.Params, graph_batch: jraph.GraphsTuple ) -> jnp.ndarray:\n",
    "    \n",
    "    predicted_graph = jax.vmap ( lambda x: network.apply(params, x)) (graph_batch)\n",
    "\n",
    "    #print(predicted_graph.nodes.shape)\n",
    "    loss =  jnp.linalg.norm(predicted_graph.nodes   -  graph_batch.nodes[:,:,n_scalar:-3], axis=-1) **2\n",
    "    \n",
    "    return jnp.mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6d48a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_init, opt_update = optax.adam(1e-3)\n",
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69e1dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params: hk.Params, opt_state, graph_batch: jraph.GraphsTuple) -> Tuple[hk.Params, Any]:\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(prediction_loss)(params, graph_batch)\n",
    "\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    \n",
    "    return optax.apply_updates(params, updates), opt_state, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e3b231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "params_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4011be02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 123/1000 [02:02<14:31,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1_000\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     params, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39misnan(loss):\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(1_000)):\n",
    "    params, opt_state, loss = update(params, opt_state, next(dset))\n",
    "\n",
    "    #print(loss)\n",
    "    if jnp.isnan(loss):\n",
    "        break\n",
    "\n",
    "    losses.append(loss)\n",
    "    params_list.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b507138f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14d365ad8610>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBeUlEQVR4nO3de3icdZ3//9ecMjk0SQ/0kNJQCnIuoBbk4AkRqv0Bi+vKol/1qnvwu3ABK4u719r19/3C6lfL+l35qRdrFWRZEaXoCohysggtYC2U0kKBUlpa2vSQpk3bJE2amczM/ftj5r7nvieTzNwzd5Lmk+fjunIlmU7uuZtpOq+87/fn/QlZlmUJAAAgAOGxPgEAAGAOggUAAAgMwQIAAASGYAEAAAJDsAAAAIEhWAAAgMAQLAAAQGAIFgAAIDDR0X7ATCajPXv2qLGxUaFQaLQfHgAAVMCyLPX09Gj27NkKh4euS4x6sNizZ49aW1tH+2EBAEAA2traNGfOnCH/fNSDRWNjo6TsiTU1NY32wwMAgAp0d3ertbXVeR0fyqgHC/vyR1NTE8ECAIBxplQbA82bAAAgMAQLAAAQGIIFAAAIDMECAAAEhmABAAACQ7AAAACBIVgAAIDAECwAAEBgCBYAACAwBAsAABAYggUAAAgMwQIAAATGuGCx+p0D+uXatrE+DQAAJqRR3910pP3Tr17T7sNH9cFTjtPxk+vG+nQAAJhQjKtYHEmkJEl9ufcAAGD0GBcsMpaVez/GJwIAwARkXLCQZb8jWQAAMNqMCxZOxSIzxicCAMAEZFywsJz3VCwAABhtxgULu2JhkSsAABh1xgULO1AQLAAAGH3GBosMyQIAgFHnK1iceOKJCoVCg95uuOGGkTo/3+zeCmIFAACjz9fkzbVr1yqdTjufv/7667r88st1zTXXBH5ilcpQsQAAYMz4ChbTp0/3fH777bfr5JNP1kc/+tFAT6oaFs2bAACMmYr3Ckkmk7r//vt1yy23KBQKDXm/RCKhRCLhfN7d3V3pQ5Yl4zRvkiwAABhtFTdvPvLIIzp8+LC+9KUvDXu/pUuXqrm52XlrbW2t9CFLcocJYgUAAKOv4mBxzz33aNGiRZo9e/aw91uyZIm6urqct7a2kdvS3F2kyLBZCAAAo66iSyE7duzQ008/rYceeqjkfePxuOLxeCUP45s7SpArAAAYfRVVLO69917NmDFDV1xxRdDnU5WM51IIyQIAgNHmO1hkMhnde++9Wrx4saLRins/R4T7Ugi9mwAAjD7fweLpp5/Wzp079dd//dcjcT5V8VQsCBYAAIw63yWHhQsXjoulnAzIAgBg9Bm1V0iG5aYAAIwpo4KFZ7kpFQsAAEadUcHCEybIFQAAjDqjgoV3jgXJAgCA0WZWsMi4PiZXAAAw6swKFq6aBRULAABGn1HBIkOLBQAAY8qoYOHZ3ZSKBQAAo86oYJFhpDcAAGPKqGDhrlKwuykAAKPPrGDh+ZhkAQDAaDMqWGSoWAAAMKaMChbebdNJFgAAjDajggXbpgMAMLaMChberUJIFgAAjDZjg0UmM/T9AADAyDArWLiqFEHVK3YfPqquvoGAjgYAgNmMChbulSBB7BXSdXRAl/77Sl1715+qPhYAABOBUcHCsoLdLOTAkYQSqYx2Hzpa/cEAAJgAjAoWQVcs7KDCTqkAAJTHqGChgHss7DxBrAAAoDxGBYugKxb28ahYAABQHqOChXfyZvXHyziXQqo/FgAAE4FRwcI7eTOIHgv7g6oPBQDAhGBUsAh4UYirYkGyAACgHEYFC8/upgFcv7DosQAAwBejgoVbIKtCckchVgAAUB6jgoWnYhFI82b2vWWxDTsAAOUwKlh4V4UEsdyUbdgBAPDDqGARdBCwPBUQkgUAAKUYFSzcL/3BjPQufmwAAFCcWcHCXbEI4HhBT/IEAMB0hgWL/MfBjPSmxwIAAD+MChYZT/Nm9ccLekQ4AACmMypYWIGP9KZ5EwAAP3wHi927d+sLX/iCpk2bpvr6er33ve/VunXrRuLcfAu6YkGPBQAA/kT93PnQoUP64Ac/qI997GN64oknNGPGDL3zzjuaPHnyCJ2eP5bcFYZgj0esAACgNF/B4t/+7d/U2tqqe++917ntxBNPDPqcKuZdHhpE86breJmqDwcAgPF8XQp59NFHdd555+maa67RjBkz9L73vU933333sF+TSCTU3d3teRsp3lUh1R8vQ48FAAC++AoW27Zt07Jly3TKKafoqaee0nXXXae///u/13333Tfk1yxdulTNzc3OW2tra9UnPZRM0Ms46LEAAMAXX8Eik8no/e9/v7797W/rfe97n/7u7/5OX/7yl7Vs2bIhv2bJkiXq6upy3tra2qo+6aF4J29Wf7xMwAO3AAAwna9g0dLSojPPPNNz2xlnnKGdO3cO+TXxeFxNTU2et5HiDQLB9lhQsQAAoDRfweKDH/ygNm/e7Lnt7bff1ty5cwM9qYqNYI8FuQIAgNJ8BYt/+Id/0Jo1a/Ttb39bW7du1S9+8QvddddduuGGG0bq/HwJfnfT4h8DAIDifAWL888/Xw8//LAeeOABzZ8/X9/85jf1ve99T5///OdH6vx88QYBJm8CADDafM2xkKQrr7xSV1555UicS9WCbrakxwIAAH/M2ivE9XEmgCYLz+RNcgUAACWZFSxGsGJBsAAAoDTDgkX+4yAuXdBjAQCAP0YFi6ArDEEHFQAATGdUsPD2RAQxIIvJmwAA+GFUsPBULII+HhULAABKMipYBN0T4d3dtOrDAQBgPMOCRfGPKz9gwMcDAMBwZgULBVthyLAqBAAAX4wKFpmM+zN2NwUAYLQZFSy8kzeDOB6TNwEA8MOoYOFdHhpsxYJgAQBAaUYFC3kuXQRwOHosAADwxahg4alYBDx5k1gBAEBpRgUL94t/0JM3qVgAAFCaUcEi6BHcTN4EAMAfo4LFyO5uWvXhAAAwnmHBItggEPgkTwAADGdWsHB/TI8FAACjzqhgkckEuyqEyZsAAPhjVLDwVCwCaN/0HINcAQBASUYFC0+FIYiR3gEP3AIAwHRGBQsr6JHeGXosAADww7Bgkf84kFUhQ3wMAACKMytYBLwbKatCAADwx6hgEfSkTCZvAgDgj1HBIvBNw9wViwCaQQEAMJ1RwSLoSxcZVpsCAOCLUcHCCnjbdHosAADwx7Bgkf848IoFwQIAgJKMChZBD7EKepUJAACmMypYuINAMNum5z9m8iYAAKUZFSy8ly4COB6TNwEA8MWoYKGAmy2ZvAkAgD9GBYvAKxaeVSZECwAASvEVLG677TaFQiHP26xZs0bq3HwLutky6FUmAACYLur3C8466yw9/fTTzueRSCTQE6qGd6BVEM2bTN4EAMAP38EiGo0eU1UKt6BXcTB5EwAAf3z3WGzZskWzZ8/WvHnz9NnPflbbtm0b9v6JRELd3d2et5FiBdwTweRNAAD88RUsLrjgAt1333166qmndPfdd6u9vV0XX3yxOjs7h/yapUuXqrm52XlrbW2t+qSH4n7pD6Ji4VkVQrAAAKAkX8Fi0aJF+ou/+AudffbZuuyyy/TYY49Jkn76058O+TVLlixRV1eX89bW1lbdGQ/DPXciiBgQ9N4jAACYznePhVtDQ4POPvtsbdmyZcj7xONxxePxah6mbEFXGNwNm0zeBACgtKrmWCQSCW3atEktLS1BnU9VMuxuCgDAmPIVLP7xH/9Rq1at0vbt2/Xiiy/qM5/5jLq7u7V48eKROj9fgp47weRNAAD88XUpZNeuXfrc5z6nAwcOaPr06brwwgu1Zs0azZ07d6TOz5egeyKYvAkAgD++gsXy5ctH6jwC4V0VEvDupjRZAABQkmF7hQT74u+ZvEmuAACgJKOCRdA9FkzeBADAH6OCBbubAgAwtowKFu66QtCrQlhuCgBAaUYFC/dAq2C2TWfyJgAAfhgVLNxbpQeRA5i8CQCAP0YFi0zgzZtM3gQAwA+jgoUVcPMmUQIAAH8MCxYBN2+6j8e1EAAASjIrWLg/DmS5afGPAQBAcUYFi6DnTnhWhXBhBACAkowKFp4eiwCOR8UCAAB/jAoWQa/iYPImAAD+GBUsgu6xcGO5KQAApZkVLALejTTD5E0AAHwxLFh4Pqv6eEzeBADAH6OCRSbgioVnRDglCwAASjIqWHgnbwbRvOk6XtVHAwDAfEYFi6CXhzJ5EwAAf4wKFgr40gVzLAAA8MeoYOG5dBFwxYLJmwAAlGZUsPAGgeoFHVQAADCdUcHCe+ki4N1NSRYAAJRkVLAIevKm+xAECwAASjMrWIzoXiFVHw4AAOMZFixcHwdwPCZvAgDgj1HBIujdSL2XVkgWAACUYlSwsEZyuSm5AgCAkowKFpkR7LGgeRMAgNKMChbWEB9XfDwmbwIA4ItZwSLgSxcZJm8CAOCLYcGi8PPqwkDQPRsAAJjOqGBR2AdR7eULeiwAAPDHqGBR+NJfdcXC9TE9FgAAlGZUsCh88Q+yYsEcCwAASqsqWCxdulShUEg333xzQKdTpYIX/2obLt2TN8kVAACUVnGwWLt2re666y6dc845QZ5PVQorFEGGAXosAAAoraJgceTIEX3+85/X3XffrSlTpgR9ThUrrFBUGwbYhAwAAH8qChY33HCDrrjiCl122WVBn09V3JcupOrDAKtCAADwJ+r3C5YvX65XXnlFa9euLev+iURCiUTC+by7u9vvQ5at8KW/2jDA5E0AAPzxVbFoa2vTV77yFd1///2qra0t62uWLl2q5uZm5621tbWiEy1H4cqNarOAN0yQLAAAKMVXsFi3bp06Ojq0YMECRaNRRaNRrVq1Sj/4wQ8UjUaVTqcHfc2SJUvU1dXlvLW1tQV28oUGTd7MFL9f+cdzXwqp7lgAAEwEvi6FfPzjH9fGjRs9t/3VX/2VTj/9dP3zP/+zIpHIoK+Jx+OKx+PVnWWZCi99VL3clB4LAAB88RUsGhsbNX/+fM9tDQ0NmjZt2qDbx8LgHovgjkfFAgCA0gybvFlQsah2uWnGvdyUZAEAQCm+V4UUWrlyZQCnEZCAR3qzuykAAP6YXbGossfCeymEZAEAQCmGBQvv50EOyCJXAABQmlHBorBCweRNAABGl1HBonCkd5CTN8kVAACUZlSwKFRtFvAECyZvAgBQklHBorBCkalyWUiGyZsAAPhiVLAI+nIFq0IAAPDHqGAxqGJR7YAsVoUAAOCLUcGi8LW/mjBgWVZB8ybJAgCAUswKFgFWLAq/lB4LAABKMyxYFHxezbEKPqfHAgCA0owKFkFuQjb4WBUfCgCACcOoYBFkj0XQjaAAAEwERgWLwrkV1fRFDLqsQq4AAKAko4LFoIpFFV0Wg/s1SBYAAJRiVrAoXMmRKX6/cgy+FFL5sQAAmCgMCxYFDZfVVCwKPqfHAgCA0owKFoVVhSCbN7kSAgBAaUYFi8IKRVWTNwPegh0AgInAqGBRWLGoJgzQYwEAgH9GBQu7YBEJh9yfVnMoBxULAABKMypY2C/+kVDI83k1x7KRKwAAKM2oYGG/9odzf6sgmzfZ3RQAgNKMChaFFYuqwgC7mwIA4JtRwcLOEeGwfSmk8mMNWrrKelMAAEoyJli4qxNO8yarQgAAGFUGBYv8x/nmzSqON+j4JAsAAEoxJli4KwxhZ7lpFRWLDKtCAADwy5hg4X7dzzdvVnG8AIdtAQAwURgTLDJFeywqP15htYMeCwAASjMmWLhDhD3HoroBWYWfkywAACjFyGDhXAqp4njsbgoAgH/mBAsNbt6spspQuAqEigUAAKUZEywyRSoW1VQZBjdvVn4sAAAmCmOCRbEBWUH2WDB5EwCA0owJFu4gEA5guSmTNwEA8M9XsFi2bJnOOeccNTU1qampSRdddJGeeOKJkTo3f9yXQgLpsSj8nGQBAEApvoLFnDlzdPvtt+vll1/Wyy+/rEsvvVRXX3213njjjZE6v7IVn7wZzPEkJm8CAFCOqJ87X3XVVZ7Pv/Wtb2nZsmVas2aNzjrrrEBPzC/v5M3cbQFULKLhkFIZi1UhAACUwVewcEun0/rVr36l3t5eXXTRRUPeL5FIKJFIOJ93d3dX+pDDGqnJm+FwSMpY9FgAAFAG382bGzdu1KRJkxSPx3Xdddfp4Ycf1plnnjnk/ZcuXarm5mbnrbW1taoTHoo7RIQC2N3U/lpn6aroswAAoBTfweK0007Thg0btGbNGl1//fVavHix3nzzzSHvv2TJEnV1dTlvbW1tVZ3wUOwX/XBIsqNAVbub5o4XDbuDRcWHAwBgQvB9KaSmpkbvec97JEnnnXee1q5dq+9///v68Y9/XPT+8Xhc8Xi8urMsg/2aHwqFnOWm1VQs7KASieSDRcayFFZoqC8BAGDCq3qOhWVZnh6KsZJxVywCbN50XwqhzwIAgOH5qlj8y7/8ixYtWqTW1lb19PRo+fLlWrlypZ588smROr+y2UEgpFBAA7Ky78PuSyFM3wQAYFi+gsW+ffv0xS9+UXv37lVzc7POOeccPfnkk7r88stH6vzKZlcsQu6KBT0WAACMKl/B4p577hmp86iaU7EIuVaFZKo/Xjjk7bEAAABDM2avEHcQyK8KqeZ4ueZNKhYAAJTNnGCRixEhZRs4pWB2N3VfCqFiAQDA8IwJFhl3xSLfZFExz+TNgscAAADFGRMsrPyykEArFu7lpiwKAQBgeMYEi4yn2TK43U0jXAoBAKBsxgQLO0aEAqpY2KmEYAEAQPmMCRbeHovsx9UNyMpP8ix8DAAAUJwxwcLVYuGavFl9j0V275HcY9BkAQDAsIwJFvnJm66KRQDHC4cUyIhwAAAmAmOCRfHJm9VvQuYOKvRYAAAwPGOChWd3U+e2yo9nuY9HxQIAgLIYEyxsnt1NqzhOsR4LKhYAAAzPmGDhqVg4q0KquBTiGhFu7z5CrgAAYHjGBAvLU2GoPgi4l6+GA1i+CgDARGBMsMivCnH3WOSTwM7OPr29r6fs4zk9FuH8qhAuhQAAMDxjgoX9ku9eFeKOAdf8eLWuvvOP6kumyjuee0Q4PRYAAJTFnGDh9FgMXh6ayVja153Q0YG0uo+WFyzcISJfsQjwhAEAMJBBwSL7Pjt503tbypUIBtKZso5XrMeC7U0BABieMcHCs1eIvCO9065gkS6z7JApMseCigUAAMMzKFjkSxbh3N/Kvmkgk69SpMpNB8yxAADAN2OChafZUt4KQzqdDwSpTLmXQpi8CQCAXwYFC/emYbnbcmUHd5UilS73Ukj2PRULAADKZ06wyL0Pyb0qJPs+VcGlECZvAgDgnzHBwj0gKxzyLgtxVynSZV8Kyb5n8iYAAOUzJlh4tjnP3eb0WFRwKcQ9eTPE5E0AAMpiTLAo2mzp9FhUcCnEHVTosQAAoCzGBAvvSO/sx/keC/eqEH9zLLIDt5hjAQBAOcwJFq6R3oW7m7ovf6SYvAkAwIgxKFhk3+enWOTDRiUVC4vJmwAA+GZMsPDMnQh7dzd1rwQpd6R30R4LkgUAAMMyJlhYruWmhUFgwHUppPxNyAYvXyVWAAAwPGOCRdFNyHJ/VtkmZO7j2bcRLQAAGI4xwUJyr+LI3mIHAXeVouw5Fhq8KoRcAQDA8IwJFp4KQ8GkzHRFzZtDHw8AABRnTLBwXvTdPRFFVoWUPdI7k5+8GWbyJgAAZfEVLJYuXarzzz9fjY2NmjFjhj71qU9p8+bNI3Vuvngmb+Zus2NAytO8We6lEBuTNwEAKJevYLFq1SrdcMMNWrNmjVasWKFUKqWFCxeqt7d3pM6vbN7dTb0VhlQFy03dQYVVIQAAlCfq585PPvmk5/N7771XM2bM0Lp16/SRj3wk0BPzy7tpmH1b9r1n8mYFq0Lyu5sSLQAAGI6vYFGoq6tLkjR16tQh75NIJJRIJJzPu7u7q3nIIeUnb4YG7e3h3d20vB4LuSoWdlIpsz0DAIAJq+LmTcuydMstt+hDH/qQ5s+fP+T9li5dqubmZuettbW10occlnuglbO1hwY3b/qtWIRcFQt6LAAAGF7FweLGG2/Ua6+9pgceeGDY+y1ZskRdXV3OW1tbW6UPOSz3CG57pLddYfBum87kTQAARkpFl0JuuukmPfroo3ruuec0Z86cYe8bj8cVj8crOjk/3M2WNnvIVSU9Fva93JM36bEAAGB4voKFZVm66aab9PDDD2vlypWaN2/eSJ2Xb64xFoN6LDyrQspcbupULIocDwAAFOcrWNxwww36xS9+od/85jdqbGxUe3u7JKm5uVl1dXUjcoLlym9zPnjuRGXbpmffh8NM3gQAoFy+eiyWLVumrq4uXXLJJWppaXHeHnzwwZE6v7Lleyxcl0PsVSGeSyH+Jm96dkslWQAAMCzfl0KOVe5VHHZXhLMJmWe5qb8eC+/y1cr+/t39A/rGb9/Up957vD50ynEVHQMAgPGgqjkWxxL3bqTOpYvcn6U9q0Iqn7xZqefe3q//XrdL+7r7CRYAAKMZswmZd3fTguZNV5Wi3JHexXY3rbRicTSZ9rwHAMBUxgQLeeZO2DcNbt4cKHPypuXe1KzKyZv2xmflPjYAAOOVMcHCU7HI3WYXGNIZ/xUL524BTN60A0WyzP4OAADGK2OChZXfLMSZvGn3XbgrBeVumx7k7qb241OxAACYzphgUaxiYV+68FYsyrwUosHHq3RVTJJgAQCYIIwJFu7Jm6FQYcWikgFZ7lUm1U3eHEgNHi0OAICJzAkWnmbL7G35bdNdy03LvRSS+5JwOORqBq3s3OyhXEkqFgAAwxkULLLvs9uchzy3pYZo3rQsSzc9sF5f/eWrg47n2Ya9yuZNLoUAACYKY4KFJwjkbnOWmw4x0rv7aEq/fXWPfv3KLvUPeGdMFJu8WWmPhX0pZCBFsAAAmM2YYFE0CORuG2oTskQ6HyYSBS/6I7MqhB4LAIDZjAkW7iCggksXqSF6LJKuMJEsCBbuyZvO8Srs3szPscgc0/utAABQLWOChXd3U2+PRTpT/FKIu4JQ2P9geSZ5VrcqxN20We6qFAAAxiODgoVdsXDNsSjaY1FexcK9W2r1kzf9jxQHAGA8MihYZN+HQlK44G/lrlKkhwoW6dI9FpVyN23ajZwAAJjImGDhrjDYNYtMkU3IPD0W6WF6LHLvi1VA/HJXKZhlAQAwmTHBwp6ymZ2UmbutRI/FcBULyzPHIrgeCy6FAABMZkyw8OwVEvJWLDwjvcusWNj5w91jUemCjgGCBQBggjAmWMizisNzk3ekt6vs4Ol9KKxYqNiIcJo3AQAYjjHBwru7acFIb9cLu6d5c7iKhbMLewCTNz2PQ/MmAMBcxgQLyzUX06lYaHDz5lCXJYaaYxF29VhUeikkOUxlBAAAkxgTLLw9Ft7b0kNsQuYe4z14pPfQx/OLHgsAwERhTLBwz7EIFVy6GCiYfGnfPvxIb+daSKADslhuCgAwmUHBwnXpIndbsYqF+3NvJcF7H3fFIsgeixQbkenVtsPa2tEz1qcBABgB5gSL3PtQaPDupoWXH+yeC2/FwrtteqZIUKl2d9Ni5zLRdPcP6Jof/Un/4+4Xx/pUAAAjwJhgYe88Ggq5B2RlbyusWBQNFkO84Bebi+EXzZt5h3qTSqYz6uhJVLxbLADg2GVMsHAqFp7lodnbCncUTafLuRQS3O6mno3PJvilkEQZYQ4AMH4ZEyw8ly4Kmi0Lg8VAbmBWIj3MqpCRmryZmtgvpv0D6aIfAwDMYEywKL4qJHs5ZKjmzWFXhRSZvFlJ86ZlWUzedBluiS8AYPwzKFjYQSDkWR5aWK2Q8hWM4Zoqi03erKTHovASy4QPFgOZoh8DAMxgTrDIvQ9JnpHehdUKSUrlXtzLmWNR7eTNwiAx0XssPJdCUlwKAQDTGBMs8s2WIc9Ib3fFIh7N/nXzFYuhL1E4FYsqJ28WHnfCVyxSVCwAwGTGBAt3j4VcQSDleiGvjUUkldljUWS31EouhRSufJjozZsJV5UiQcUCAIxjTLAYalKmu2JRk6tY2FUDz2/PQ1Qs3MerBD0WXv2uKkU/FQsAMI4xwcLusnAVLGRZ+RHa0XBIsVzpoehI7+F6LHK3VdS8maLHwo2KBQCYzXeweO6553TVVVdp9uzZCoVCeuSRR0bgtPyz506EwyGFw/mR3qncH0TCIUUi2dvtKsJwkzftl/9qJ2/SY+HFclMAMJvvYNHb26tzzz1Xd95550icT8Us104e7gqDXZ2IRcKKhbN/XafHIj10j4UTIqqcvDmox2KCBwsGZAGA2aJ+v2DRokVatGjRSJxLVdw9Ee7loXZ1IhIOKZKrZNhVjGHnWNgVENeqkMqWm9Jj4UbFAgDM5jtY+JVIJJRIJJzPu7u7R+RxvJM3sx97KxYhRSO55abFLoUMmryZFXatCqlk8uagORapCd5j4RmQRcUCAEwz4s2bS5cuVXNzs/PW2to6Io/jbrYMeyoW+R6LaEHzZnKY356d5aaqsscixaUQN/dQrH4qFgBgnBEPFkuWLFFXV5fz1tbWNiKPk3EHgdxt7n1CouGwcynEfnFPDjvSu3hQ8YseCy9GegOA2Ub8Ukg8Hlc8Hh/ph8mP9HYHAeWnbEYjIcUiQ1csBq0KCWzyZmGPxQS/FMJyUwAwmjFzLIqP4LacyZve5s3Bq0IGCnofMkUmbwbRYzHRKxYMyAIAs/muWBw5ckRbt251Pt++fbs2bNigqVOn6oQTTgj05Pzwbhpm35avTsTCYcXs5k17VUgZFQvPJM8Kzotg4UXFAgDM5jtYvPzyy/rYxz7mfH7LLbdIkhYvXqz/+q//CuzE/HKNnXC6LDKWNJApstw0PbhiMdQcC7ta4b7Nj8LjEixYbgoAJvMdLC655JKKLgmMNHtAVnbyZv7WdK46EY3kV4WkMpYsy/L0Oww1eTPkqlhU02MRDmW/fsKP9GZAFgAYzZwei1wuKKxYuPcKiYbz26YXBolkKuMJTMV6LCqpWNiXXRpqshmO3U2pWACAyYwJFnbFIlthyN3m2t00Gg47e4Wk05lBlygk74oN7+RN+4D+z8t+nPp4JPcYE/vFlGABAGYzJljkV4XIszzUvdzUfSmk2LLPYi/64SorFvbjOBWLCR4s2CsEAMxmTLBwr+LI7xXiXW7quRSS+2055GrOdFcxMkFN3kwXViwmeI8FFQsAMJpBwcIOAnJN3nRVLApGetshoi4WcSoS7r4Ld49FdZuQ5YJFrmJR2Nsx0bibN9krBADMY06wyL0vnDvhjPSOhBWN5Ed62y/wsUh+vkUyldH/feot/ecL24vOsahm2/SGGnosJO/+IEFWLLr7B/TVX76qF7YcCOyYAAD/Rnyk92jJuAZZFJu8OVTFoiYaVsaylEhl1HaoT//x7DuKhkNqrotlD1ft5M3cRM/6OKtCUumME/SkYCsWz77VoV+/skvt3Uf1oVOOC+y4AAB/zKlYFJuU6WneDCuS67EYSOeXm9ZEwopHs7fv78lu757KWOpJpJzjhQKYvJmvWEzcHovCCkWQFYtDvUlJ0uG+gcCOCQDwz7iKRajgtrSrxyK/CVnGecG3KxaSdOBI0vlau6IRDuWPWVXzpqvHwrKs/BLWCaRwFUiQwaKnP+V5DwAYG8ZULGzhcHb6ppStMNgVgsJNyJxLIZGwanIVi84jiUHHc++WWlWPRW5ViP34E1FhkAhyuWl3/4DnPQBgbBgTLDzLQ3O3WVZ+pHfMPcfCfSkkGlZNxA4WSRUKhfIjwivb3TTXY1ETdd02MfssCoNFKpPvgamWu2JxLI6cR/nSGUt7Dh8d69MAUCFjgoXTu+mqMFiWt2IRjQyeYxGLhJxVIZ29g4NFOJSPKhUtN015eyyyt03MFz67QuH+XgS1/NYOFumMpaMsYx3Xvvv7zbr49mf09Jv7xvpUAFTAmGCRnzsR8qwKSbtHejsVi4xnVYh9KeRAkUsh4YJVJn7Z1Yk6d7DITOyKhb3iRpL6B4L5XrgvgYzHPovu/gHt6Owd69M4Jjy2ca8kadPe7jE+EwCVMCZY5FeFuAZayTsgK9+8aTkv+DF3j0VvkR4LuedYVLBtuvtxIvaqlAkaLHKVhNqaiPO9SKSCqS50u8JE99Hx12fxpf98SR//7iq1d/WP9amMqV2H+rSjs0+SdIgVPsC4ZFywCMl76cIZ6R0JOctN3ZdC4q4ei4NFeyyCmbyZHcSVG9A1US+FON/ziLPENxFQxaLHVbHoHocVi83tPUplLG3bf2SsT2VMrX6n0/n48NHBP48Ajn3mBIvclAn3pmFSvmIRC4ddm5BlvM2buRe53uTg357DYe9cDL/sHo9YJKxY7nEm6lhvp2IRCysey34v+gOqWLgvf/SMs5UhiVTa+bc30X9LX701Pzm1a4J/L4DxyqA5Ftn32QpDPlkk3ZuQRVyrQlL5SkJ6mOWfIVW7u6kdYPJNohP2UoirShSPZntORqJiMd56LNxDvQ72Tdzf0i3L0h9dFYtDE/h7AYxnxgQLy9W86a5Y2Ksy3CO9Uxnv5M3hxkpkqxWVT950B5iJ3mNhrwqJRyNOxSKIIVnJVMbTBDreZlkcdK1GOlRkZdJE8c7+I870W0k6PA57ZQAYdCnEqVhIrkkW+RfxaCRcdNv0mKvHohj3pZVqKhaeHosJGizsEFEby1csghiSVXjpY7xVLNy/mU/k39L/uDVbrZjRGJfEpRBgvDImWLh3Nw25/lZ2j0PUdSnEM9I7ElZNdJjx2lVO3nR6PCJhZ45GcoI2byZczZu1AVYsCoPEeOuxONQ74Pp44gaLV9sOS5IuP3OmpGzFgmFnwPhjTrBwLoV49wtJOhWL/EjvgXTxVSHFhKudvOkaHT7Reyzyl0LyG78Fsdy08NLHeKtYuPsqDk7g39Lbu7NLbc+Z0ywpuyzc3gwQwPhhULDIvnfvbiq5LoWEQ86lkOwci3wlwV4VUky1kzeT9uNEQ6rhUogkqTYWcV0KCb5iMd7mWBx2VSkOT+BLIR25/orWKfWqi2X/fXA5BBh/jAkWmfwgC7k3Dh1wVoWEPc2bCdfkzViJHosgJm/GqFg41Yl4NOy6FEKPhadiMYEvhXTkKhYzmuKaXJ+dzjqRe05MlAlwfyAcu4wJFkNWLHL9DNFISBFnuWnxkd7FeCdv+j8vdy+HHSzsKsZEYy8tjceCXW5aOBBrvAWLQ6wKUf9A2nkepzfWOmPfD1OxMMpf/vhPuuyOVYFN3MWxyZhgkd/d1CvpuhQS81wKGTzSW5Km1Mc8Xx8KuydvVlmxyD3OQAANi+OR/Z9JrWvyZhADsuxLH9MaarKfj7fmTdeLZ28yPSH/0+3ozl4GqY2F1VQb1ZT67HPJklNzdPUN6OUdh/RuZ5+2H2BfHJMZEyxsQ/ZYRMKu5s2CioXrUsjUhhqnTF94PL+5wrLcvRz0WLgrFrWx4CoWdoVi9uQ6z+fjRWG5fyL+lt7Rk7sM0lirUCjkXAqZyD0npmk71Jf/+ODRMTwTjDRjgkXGtSokXKTHonATsvyArJCnYtFYG1Njbb5qEVLlPRYDrksesSg9Fv0p14CsaPDLTWdPrpU0/ioWhX0VE7HPwm7ctGdY5IPF+HouMbRdrmDh/ngoA+mMvvCTF7XkoddG8rQwAowJFk7vZsFIb/vFPRLOLzdNuS6FFFYsmupiaqrNDyT1rArxeU7uABEL02NhVyc8e4UEOCDr+Mn1kqQjiZQylTTEjBH7xdMOuBOxz2Kfq3FTkibbl0IIFsZwVynKqVi8tbdHL2w9oAdealMvy47HFWOCRb7HIlS0YhGL5JebptKuVSGRiGdVSGNt1FuxqGLypidYRNgrxDMgy27eDKBiYVcojp+SvRRiWdKR5Pj4jyiRSutI7j/NedMaJE3MjcjyFYts1WlyHZdCTOO5FFJGxWLr/h7n42376ckYT4wJFvnJm4UVC9dy00i+YpHfw8N7KaSpNqpGV8UiFMrucCr577GwL7eEQtmKiT3hc6Iut/IMyAp0uWn2hfm4STVO9Wm89FnYv5FHwiGdMC1bcRntjcj6B9K68Rev6IGXdo7q47rZzZv5ikUuWNC8aYxdh44W/XgoWzuO5D92hQwc+8wJFs6lkFDuffZzp3nSswlZxnMpxFuxiKnJVbHIXgqxH6OyHotYJKxQKMSlkCIDsoJs3mx0hcLxMtbb7qeYUh9zVrWM9qWQP2zq0O9e26vvPPnWmI3QdjdvSu5LIVQsTNF20NVjcbCv5L81T7BwfYxjn0HBIvuP1L5sYYcBe2lndtv03HJT10jvmkh+vLSUrVg01RX0WFQ4x8I9zlsSl0JGaECWfSmkqTamplwJfbxULOwVIZPrazTFDhaj/GK6fueh3OMOaEdn6RL1SHAqFnbzJnMsjGJZlqdK0ZNIqatENYpgMX4ZEywyruZNKb9xmHuvkKLbphcMyCpcFVLN7qbu/o7s+2DmWLy4rVNf+MmL2rS3u6rjBOXX63bprufeKfkbSL+z3NRVsQhwVUj2ucuGwvEy1tvegGxqfY2m1o9NxWJ9bvMvSdrg+ng02RWLmU0FFYtx8jyabt2Og/rBH7ZU/EtRZ29SRwfSCoXkDD8b7nLIQDrjCbkEi/HFmGBhyV5uWngpxF5uGnatCsnkqwmDLoVE1Rh391hUPsci6RqOJSmQORYdPf26/uev6IWtB7Rs5TsVHyco+3sS+qf/flXffvwtvbqra9j7uisWzoCsKleFWJblXPbwXgoZHxULu59iSkPM6SsYzY3IkqmMXt+df97GIlgkUmmnYdWuWExxzbEYTyt8TGRZlm5+cIPuWPG2Hl6/u6Jj2JdBZjXVat5xDZ7bitnR2atUxnJ+qdvR2TdhK73jkTHBIpP7N2dfAim8fBFxbZvurlgUTt50/9Zrh5NKJ2+6eyzc7yvtschkLH31l6861+WfeavDcynhzT3d+vrDG7U/12E/Gp54fa/zPX7i9b3D3jffY+EakFVlxSKRyjjf56a6mBrj9qWQAb3V3n3MT7G0NyCb2lCjqQ2j31eQ/R7ln4P1YxAs7H+vNZGwE67sS1oZS+xwOsZe393tLA99YuPwP+NDactVJ+ZMqVPr1GyT8nAViy37shWK+cc3q6EmolTG0o5OVoaMFxUFix/+8IeaN2+eamtrtWDBAj3//PNBn1fF7OpC4WjvqGu5qWXly/KD5ljURp3/1OxjVbpXiLtBVFJ+pHeFyftna3bo+S0HFI+GNaU+piOJlFa/05n7O1m65Zcb9PMXd2rpE5t8Hfffn9qsy+9YpZ0VXF//3av5/2iefL192PCVXxXiGpBVZfOmfckjHJIaaiJOf8z9a3bqk997Xrc8+GpVxx9pB4v0WIzmgKz1Ow9Lkk6anv0tctOe0Q9j9lLT6Y1x5xeC2lhk1Hc4HUhndDR5bAfRcgT97+dx1y8ML2w9UNEAOnsgVuuUes3JLQsfbsmpfenjPTMm6eQZkzy34djnO1g8+OCDuvnmm/X1r39d69ev14c//GEtWrRIO3eO3VI1yTt5U5JnrLfkvRQiSX25OQc1kbCzDFTy9ljkqx/exyjXQGqIHosKgkVHd7/+71ObJUlfv+IMXXFOiyTp92+0S8p29r/Vnl2S9ZsNe8oOCRvaDuvOZ7dqS8cRfeN3b/g6p/aufq3dcVBS9vu4o7PPOYdClpWfHRJ3D8iq8kXM3rhqUjyqUCjkPHeb92XP47GNe7Uud47HIrufYmp9jbM/xmj2WNiXPq46Z7amNtQomc7ozT2V9+5UcmnLbtycnrsMYnMuhxwt/v149NU9+ocHN6i9q3/Ic7F3TC3lYG9Sn/j/ntOHv/PsuN7H4t+f2qz3f3OFvvm7NwM5nmVZTpUiEg5pIG3pD5v2+T6OXfGYM6VOrVPqc7cNEyz254PFe6ZXHyy27T8SyDA+lMd3sLjjjjv0N3/zN/rbv/1bnXHGGfre976n1tZWLVu2bCTOr2zu3U0l79bpknekt5SvPmQrFhHndvd1+nz1o7LJm0H2WHzr8U06kkjp3NbJ+sIFc/WJs2ZJkla8uU/pjKU7n92ae4yw0hlLy1aV7r/IZCzd9mg+TDy9qUMrN3eUfU6Pbdwry5LOmztFHzl1uiTpidfbi943mc44z1G2YuF/uem+7n4d7E16qiL5/opY7n100Nf92xObh6ykWFZ+hdBYsHsLpjTkmzfL2YhsX3e/dnaWXrJXir0i5H0nTNa5c5olVdZncbgvqa/9+jWd8b+f1PX3r/P1W+1+p3HTGyya7aBVpGLx2Gt79ZXl6/Xw+t1a/J8vDapqbG7v0WV3rNKFS/+gnzy/bdjvUyaT7SHYdqBXB44kdP3968bli9DTb+5z/h+454Xtuu9P71Z9zE17e/RuZ5/i0bC+dPGJkqQnNhb/GR+OXbGYM7VerVPrcrdlw8bb+3p0zY9W6y9//Cdt7cj+QuBULKZXV7FIpTP6xm/f1KXfXaVF339e7/oIjclUNmSPlyqWZVnHTD/S4P+Fh5FMJrVu3Tp97Wtf89y+cOFCrV69uujXJBIJJRL5a/7d3SOzkqGwmtDSXKt3XNPaopGQp2Jhq4mEPU2ZTbUxdddm/5Nyqh+5+HW4L6kv/ORF5/JJLPe+L5nSob6k4tGIJtfHlEpbnuVUhT0WG3d3eV7QhzOQzqi9q19/eKtDoZD0f66er3A4pAtPmqam2qgOHEnq+vvXaUPbYcWjYX3v2vfq+p+/ov9e16ZoOKS+ZFpv7OnSvu5+nTKzUafOnORcEjpwJKENbYfVUBPRJ86apYfW79b/+8jruuyMmZK8PSWF/1y7jg7oj1uzl2GuPKdFjbUxPb1pn365tk0HexNKZ7Jfn85YShe8eGd7LHLjq/uSJb8XA+mM1r57UG/nrrs21UZ14nENOmFqvTPq17585V7R80+fOE0/+MMWvfTuQX3hnhd1JJHW1PqYpjfG9W5nn7Z2HFHX0QGlM5ZOmt6gBSdMUUOucdf+u9t/b/e/kWQqo309/erpT2n6pLhmNMUHVcjK9WZuZc/UhmxvTyQcUjpj6dbfvOH0oRR6dddh5xLGjMa4zpzdpOMmxdVQE3EuJViWpT1d/Xp7X49ikbBOnt6gaZPiuVVO2ajck0jp3Vxl672tk/Vq6xQ9u3m/fv7iTm1u71EoFFIkPHhjP7d0xtKuQ31a33bYWRr6xOvtenNvtz58ynHO/exw7j6M/aHd9GvPsLDZS05/8vw2Pf3mvuy4fmV/KXhwbZssK1sN3LyvR1+450UtmDtFUvb/gl+v26Xe3AvC/3lsk9Zs69Sc3G/KhfYcPqrn3t6v2lhY9TVRvdXeo7+6d61OnZl9QSv2b8By/UQUyyxOf1bB37uwB8x2uC+pzt6k6msimjYp7rk8WxiKhnrp+M2GPZKk02c16q32Hv3rb9/Ur1/Zrcl12cbgxtqoLCv7/UlnLGWs7PmEQyGFw3ajevacM1b2z9/OVf4uOW26PrNgju55YbtWvr1fn7trjfqSKbU01ykaCWnLviPqTaZ06sxGtU6pUygU8py33SDcOqVes5qzz/OOg3362q9f00Prdzv/P1zxgxf0Z+fO1pZciDhlZqPz9139Tqf+1yOvO997+/De5yf/ZxnL0paOI87PyvYDvfrUD/+oq8+d7fn+Fwud3f0pPfNWh7qODqi+JqKPnT5D0yfFB92vUOGxjg6k9W5nnw72JtWa6y8p9jpUStfRAW3b36ujybROPK5es5pqPX+HXYf6tPbdQ0qmMnr/3Mn6wInT9LkLWgf9TI0WX8HiwIEDSqfTmjlzpuf2mTNnqr29eIpdunSp/vVf/7XyMyxTfvJm9pv9wP+8UL9Zv0crNu1TS3Otjs/tfHn85DrtPpxNynWxiOpqIopFw4qGsxM4J9VGczss5l+kpk+KK5orA76w9YDvc7Ob8uwldG0Hj+q/Vr/r+zh/dfE8nZ37rTIWCeuyM2fqoVd26/dvZkuTnz2/VYvObtEF86bqxe0H9bM1Ozxf/9L2g3pp++DLAjdeeoo+f+EJem7Lfu065O/c6mIR/T/ntCgeiagmGlZ7d7/uXzP0ZbHJ9THVRMJqrqtRKJRtviz38cKh7ItKd39Kr+3q0muuVSizcr/tzs79p3XqzEm67qMnq7t/QD9etc0JQUPZtr93TMcGtzTXKRwOqaW5VrsOHdXytW3D3j8UylbhOnoS6ti8v+Txh/tt78yWJk2ur9EH5k117lvJb4fvmTFJ//MjJ+n7T2/Rjs4+7ej0d3n0hKneF357t9rntxzQ81sG/9x98qxZuunj79Fn71qjjbu7tNG1ukWSLjxpqj58ynR99/eb9fSm0pW4b149Xy3Ndfrif76oP23r1J+2Df9v5lh07pxm/fK6i/S/Hnldv3x5l14NqBn3inNm6/RZjTppeoO27e91vjeFK8GGa8gMhaR5xzVoakON6mIRHR1IO//OP3badKUylp7fckC/WrdLUrb62DqlzhkT0NGTGPR/WjnqYhH976vO1PKXdurVXV366Z/KP0ZNNKy+ZFqPvVZZ06pbUD0i9mXeofxxa6f+uLVTnzlvTiCPV4mQ5aOWumfPHh1//PFavXq1LrroIuf2b33rW/rZz36mt956a9DXFKtYtLa2qqurS01NTVWeft6Da3eqL5nW1e893nkhL+Zgb1LrdhzS7kN9Or2lSReeNE1StoxYVxPRB9+T/S3rydf3ampD3PnPdsu+Hr2974iS6bQGUpYS6eyS1WQ6o/qaiJrrYkqmMjrcN6BYJKRJtTH1D6TVm0jpk/Nnae60BiVTGT3w0k4dOFL+qo1QKKTpjXGddFyDLj55mieldvT068GX2pRIZVRXE9Hii0/UpHhUew4f1cPrdyuRyigWDum0WY1qaa7TW+3dgwYgTZtUoy9eOFfRSFiv7+7S799o9/xG5MnWrseur4nohKn1Ord1shPant+yXy9uO6hwOKSI/duu83H2t94PzJuq+cdnw9ETG/fqjTKv558yc5IuOXWG4rGwdh7s07sHerXzYJ/6B9IKhUK68pwWzZ3WoFQ6o1++vEuXnDZdsyfXqX8grQde2qlYJKzjJsV1qC+pju6E5kyp02mzGjW9MVtteLXtsDbu7lI64+3V8XwPcjdGwyHNaIyrqS6mju5+HTiS9PwG69fcaQ36y/NaJWUvTTzzVvZFMP8bmffYs5pq9YmzZqmpLqYNbYe1s7NP+48kPOV7y8oG2tNnNWogY+mdjiPZzdly5dKMJdXHI5rWUKOPnjpDs5prZVmWfrNhj3YfPpqrNmV/67OsbNUpNKglOvstaWmuy1Z85k5RLBLW4b6k/nvdLmcPlMLfLFWkEjYpHtX/uOAET8Vpb1fu3/FAJns/K/udsKzsZZNrzmtVbSyirR09+u2re53nTpJmNdfq2vNbFYuEtW7HQT371v5hn6Nz5kx2Li++uK1Tz285MKjKYN/g/i4UViayf6fiv1FrmCpYU11UU+prdHQgrQNHvEtsi/5bLPwDy1J9PKprz2vVlIYaWZaljbu7tK87ocN9SXUdHVB3f0rhkBQJhRTO/TxasmRZuTJ67rf8jJW7T247g+Mm1eiaBa0Kh0PatLdbz7zVoZbmWjXEo2rv6lcildYpMxpVXxPR5n096uhOFD3nM2c36ZPzs71hq7ce0JptnbIknTarUVecnb39d6/t1daOI4rHwrropGl63wnZKtTjG/c6/Vvu3rehK0LKTTsO6eNnzNTJ0yfpaDKt5Wt3qvNIvmensBBnfxoOZ/+v+sCJU/Xa7i49//aBsi9hu88lGgnrhKn1mjapRjsP9mnv4f6K/q+oi0V00vRJqquJaPv+XnX2el9DptTX6PwTpyoeC2vtu4f0dnuPvvmp+b4fp5Tu7m41NzeXfP32FSySyaTq6+v1q1/9Sn/+53/u3P6Vr3xFGzZs0KpVqwI7MQAAcOwo9/XbV/NmTU2NFixYoBUrVnhuX7FihS6++OLKzhQAABjDV4+FJN1yyy364he/qPPOO08XXXSR7rrrLu3cuVPXXXfdSJwfAAAYR3wHi2uvvVadnZ36xje+ob1792r+/Pl6/PHHNXfu3JE4PwAAMI746rEIAj0WAACMPyPSYwEAADAcggUAAAgMwQIAAASGYAEAAAJDsAAAAIEhWAAAgMAQLAAAQGAIFgAAIDAECwAAEBjfI72rZQ/67O4ub7tsAAAw9uzX7VIDu0c9WPT09EiSWltbR/uhAQBAlXp6etTc3Dzkn4/6XiGZTEZ79uxRY2OjQqFQYMft7u5Wa2ur2tra2INkjPFcHDt4Lo4dPBfHDp6LyliWpZ6eHs2ePVvh8NCdFKNesQiHw5ozZ86IHb+pqYl/KMcInotjB8/FsYPn4tjBc+HfcJUKG82bAAAgMAQLAAAQGGOCRTwe16233qp4PD7WpzLh8VwcO3gujh08F8cOnouRNerNmwAAwFzGVCwAAMDYI1gAAIDAECwAAEBgCBYAACAwxgSLH/7wh5o3b55qa2u1YMECPf/882N9Ska77bbbFAqFPG+zZs1y/tyyLN12222aPXu26urqdMkll+iNN94YwzM2x3PPPaerrrpKs2fPVigU0iOPPOL583K+94lEQjfddJOOO+44NTQ06M/+7M+0a9euUfxbmKHUc/GlL31p0M/JhRde6LkPz0X1li5dqvPPP1+NjY2aMWOGPvWpT2nz5s2e+/BzMXqMCBYPPvigbr75Zn3961/X+vXr9eEPf1iLFi3Szp07x/rUjHbWWWdp7969ztvGjRudP/vOd76jO+64Q3feeafWrl2rWbNm6fLLL3f2ikHlent7de655+rOO+8s+uflfO9vvvlmPfzww1q+fLleeOEFHTlyRFdeeaXS6fRo/TWMUOq5kKRPfvKTnp+Txx9/3PPnPBfVW7VqlW644QatWbNGK1asUCqV0sKFC9Xb2+vch5+LUWQZ4AMf+IB13XXXeW47/fTTra997WtjdEbmu/XWW61zzz236J9lMhlr1qxZ1u233+7c1t/fbzU3N1s/+tGPRukMJwZJ1sMPP+x8Xs73/vDhw1YsFrOWL1/u3Gf37t1WOBy2nnzyyVE7d9MUPheWZVmLFy+2rr766iG/hudiZHR0dFiSrFWrVlmWxc/FaBv3FYtkMql169Zp4cKFntsXLlyo1atXj9FZTQxbtmzR7NmzNW/ePH32s5/Vtm3bJEnbt29Xe3u75zmJx+P66Ec/ynMywsr53q9bt04DAwOe+8yePVvz58/n+RkBK1eu1IwZM3Tqqafqy1/+sjo6Opw/47kYGV1dXZKkqVOnSuLnYrSN+2Bx4MABpdNpzZw503P7zJkz1d7ePkZnZb4LLrhA9913n5566indfffdam9v18UXX6zOzk7n+85zMvrK+d63t7erpqZGU6ZMGfI+CMaiRYv085//XM8884y++93vau3atbr00kuVSCQk8VyMBMuydMstt+hDH/qQ5s+fL4mfi9E26rubjpTCLdgtywp0W3Z4LVq0yPn47LPP1kUXXaSTTz5ZP/3pT53mNJ6TsVPJ957nJ3jXXnut8/H8+fN13nnnae7cuXrsscf06U9/esiv47mo3I033qjXXntNL7zwwqA/4+didIz7isVxxx2nSCQyKFF2dHQMSqcYOQ0NDTr77LO1ZcsWZ3UIz8noK+d7P2vWLCWTSR06dGjI+2BktLS0aO7cudqyZYsknoug3XTTTXr00Uf17LPPas6cOc7t/FyMrnEfLGpqarRgwQKtWLHCc/uKFSt08cUXj9FZTTyJREKbNm1SS0uL5s2bp1mzZnmek2QyqVWrVvGcjLByvvcLFixQLBbz3Gfv3r16/fXXeX5GWGdnp9ra2tTS0iKJ5yIolmXpxhtv1EMPPaRnnnlG8+bN8/w5PxejbMzaRgO0fPlyKxaLWffcc4/15ptvWjfffLPV0NBgvfvuu2N9asb66le/aq1cudLatm2btWbNGuvKK6+0Ghsbne/57bffbjU3N1sPPfSQtXHjRutzn/uc1dLSYnV3d4/xmY9/PT091vr1663169dbkqw77rjDWr9+vbVjxw7Lssr73l933XXWnDlzrKefftp65ZVXrEsvvdQ699xzrVQqNVZ/rXFpuOeip6fH+upXv2qtXr3a2r59u/Xss89aF110kXX88cfzXATs+uuvt5qbm62VK1dae/fudd76+vqc+/BzMXqMCBaWZVn/8R//Yc2dO9eqqamx3v/+9zvLjDAyrr32WqulpcWKxWLW7NmzrU9/+tPWG2+84fx5JpOxbr31VmvWrFlWPB63PvKRj1gbN24cwzM2x7PPPmtJGvS2ePFiy7LK+94fPXrUuvHGG62pU6dadXV11pVXXmnt3LlzDP4249twz0VfX5+1cOFCa/r06VYsFrNOOOEEa/HixYO+zzwX1Sv2HEiy7r33Xuc+/FyMHrZNBwAAgRn3PRYAAODYQbAAAACBIVgAAIDAECwAAEBgCBYAACAwBAsAABAYggUAAAgMwQIAAASGYAEAAAJDsAAAAIEhWAAAgMAQLAAAQGD+fyw7l2RgjTQYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10c96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    " @jax.jit\n",
    "  def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "    \"\"\"Returns updated params and state.\"\"\"\n",
    "    g = jax.grad(prediction_loss)(params)\n",
    "    updates, opt_state = opt_update(g, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "  @jax.jit\n",
    "  def accuracy(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "    params, opt_state = update(params, opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_club(network: hk.Transformed, num_steps: int, graph_batch: jraph.GraphsTuple) -> jnp.ndarray:\n",
    "  \"\"\"Solves the karate club problem by optimizing the assignments of students.\"\"\"\n",
    "  graph_batch = get_zacharys_karate_club()\n",
    "  labels = get_ground_truth_assignments_for_zacharys_karate_club()\n",
    "  params = network.init(jax.random.PRNGKey(42), zacharys_karate_club)\n",
    "\n",
    "  @jax.jit\n",
    "  def predict(params: hk.Params) -> jnp.ndarray:\n",
    "    predicted_graph = network.apply(params, graph_batch)\n",
    "    return predicted_graph\n",
    "\n",
    "  @jax.jit\n",
    "  def prediction_loss(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, graph_batch)\n",
    "    # We interpret the decoded nodes as a pair of logits for each node.\n",
    "    log_prob = jax.nn.log_softmax(decoded_graph.nodes)\n",
    "    # The only two assignments we know a-priori are those of Mr. Hi (Node 0)\n",
    "    # and John A (Node 33).\n",
    "    return -(log_prob[0, 0] + log_prob[33, 1])\n",
    "\n",
    "  opt_init, opt_update = optax.adam(1e-2)\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def update(params: hk.Params, opt_state) -> Tuple[hk.Params, Any]:\n",
    "    \"\"\"Returns updated params and state.\"\"\"\n",
    "    g = jax.grad(prediction_loss)(params)\n",
    "    updates, opt_state = opt_update(g, opt_state)\n",
    "    return optax.apply_updates(params, updates), opt_state\n",
    "\n",
    "  @jax.jit\n",
    "  def accuracy(params: hk.Params) -> jnp.ndarray:\n",
    "    decoded_graph = network.apply(params, zacharys_karate_club)\n",
    "    return jnp.mean(jnp.argmax(decoded_graph.nodes, axis=1) == labels)\n",
    "\n",
    "  for step in range(num_steps):\n",
    "    print(f\"step {step} accuracy {accuracy(params).item():.2f}\")\n",
    "    params, opt_state = update(params, opt_state)\n",
    "\n",
    "  return predict(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0252a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab6ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911af68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39801e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fd4fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2143e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c248f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe60d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd9711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfd166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca68d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45d602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27412dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d999a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2aee7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c213cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592482e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jraph.concatenated_args\n",
    "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.Sequential(\n",
    "      [hk.Linear(128), jax.nn.relu,\n",
    "       hk.Linear(128)])\n",
    "  return net(feats)\n",
    "\n",
    "\n",
    "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  # Add a global paramater for graph classification.\n",
    "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
    "  embedder = jraph.GraphMapFeatures(\n",
    "      hk.Linear(128), hk.Linear(128), hk.Linear(128))\n",
    "  net = jraph.GraphNetwork(\n",
    "      update_node_fn=node_update_fn,\n",
    "      update_edge_fn=edge_update_fn,\n",
    "      update_global_fn=update_global_fn)\n",
    "  return net(embedder(graph)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d4f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(params: hk.Params, graph: jraph.GraphsTuple, label: jnp.ndarray,\n",
    "                 net: jraph.GraphsTuple) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
    "  \"\"\"Computes loss and accuracy.\"\"\"\n",
    "  pred_graph = net.apply(params, graph)\n",
    "  preds = jax.nn.log_softmax(pred_graph.globals)\n",
    "  targets = jax.nn.one_hot(label, 2)\n",
    "\n",
    "  # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
    "  # to mask out any loss associated with the dummy graph.\n",
    "  # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
    "  # get_graph_padding_mask.\n",
    "  mask = jraph.get_graph_padding_mask(pred_graph)\n",
    "\n",
    "  # Cross entropy loss.\n",
    "  loss = -jnp.mean(preds * targets * mask[:, None])\n",
    "\n",
    "  # Accuracy taking into account the mask.\n",
    "  accuracy = jnp.sum(\n",
    "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask) / jnp.sum(mask)\n",
    "  return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6f114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
    "def train(dataset: List[Dict[str, Any]], num_train_steps: int) -> hk.Params:\n",
    "  \"\"\"Training loop.\"\"\"\n",
    "\n",
    "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
    "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
    "  # Get a candidate graph and label to initialize the network.\n",
    "  graph = dataset[0]['input_graph']\n",
    "\n",
    "  # Initialize the network.\n",
    "  params = net.init(jax.random.PRNGKey(42), graph)\n",
    "  # Initialize the optimizer.\n",
    "  opt_init, opt_update = optax.adam(1e-4)\n",
    "  opt_state = opt_init(params)\n",
    "\n",
    "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
    "  # We jit the computation of our loss, since this is the main computation.\n",
    "  # Using jax.jit means that we will use a single accelerator. If you want\n",
    "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
    "  # found in the jax documentation.\n",
    "  compute_loss_fn = jax.jit(jax.value_and_grad(\n",
    "      compute_loss_fn, has_aux=True))\n",
    "\n",
    "  for idx in range(num_train_steps):\n",
    "    graph = dataset[idx % len(dataset)]['input_graph']\n",
    "    label = dataset[idx % len(dataset)]['target']\n",
    "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
    "    # In the limit, this means a new compilation every training step, which\n",
    "    # will result in *extremely* slow training. To prevent this, pad each\n",
    "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
    "    # of compiled programs, the compilation cost is amortized.\n",
    "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
    "\n",
    "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
    "    # been added to the batch, which means there should be an extra label.\n",
    "    label = jnp.concatenate([label, jnp.array([0])])\n",
    "\n",
    "    (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
    "    updates, opt_state = opt_update(grad, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    if idx % 50 == 0:\n",
    "      print(f'step: {idx}, loss: {loss}, acc: {acc}')\n",
    "  print('Training finished')\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = train(train_mutag_ds, num_train_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfa6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28355cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(hk.Module):\n",
    "  def __init__(self, features: jnp.ndarray):\n",
    "    super().__init__()\n",
    "    self.features = features\n",
    "\n",
    "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "    layers = []\n",
    "    for feat in self.features[:-1]:\n",
    "      layers.append(hk.Linear(feat))\n",
    "      layers.append(jax.nn.relu)\n",
    "    layers.append(hk.Linear(self.features[-1]))\n",
    "\n",
    "    mlp = hk.Sequential(layers)\n",
    "    return mlp(x)\n",
    "\n",
    "# Use MLP block to define the update node function\n",
    "update_node_fn = lambda x: MLP(features=[8, 4])(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18233c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = False) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    # First pass nodes through the node updater.\n",
    "    nodes = update_node_fn(nodes)\n",
    "    # Equivalent to jnp.sum(n_node), but jittable\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
    "      # this case it is not required since a GCN is agnostic to whether\n",
    "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
    "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
    "                                                       total_num_nodes)\n",
    "    else:\n",
    "      conv_senders = senders\n",
    "      conv_receivers = receivers\n",
    "\n",
    "    # pylint: disable=g-long-lambda\n",
    "    if symmetric_normalization:\n",
    "      # Calculate the normalization values.\n",
    "      count_edges = lambda x: jax.ops.segment_sum(\n",
    "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
    "      sender_degree = count_edges(conv_senders)\n",
    "      receiver_degree = count_edges(conv_receivers)\n",
    "\n",
    "      # Pre normalize by sqrt sender degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
    "          nodes,\n",
    "      )\n",
    "      # Aggregate the pre-normalized nodes.\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "      # Post normalize by sqrt receiver degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x:\n",
    "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
    "          nodes,\n",
    "      )\n",
    "    else:\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "    # pylint: enable=g-long-lambda\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "  \"\"\"Defines a graph neural network with 3 GCN layers.\n",
    "  Args:\n",
    "    graph: GraphsTuple the network processes.\n",
    "\n",
    "  Returns:\n",
    "    output graph with updated node values.\n",
    "  \"\"\"\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(8)(n)),\n",
    "      add_self_edges=False)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=lambda n: jax.nn.relu(hk.Linear(4)(n)),\n",
    "      add_self_edges=False)\n",
    "  graph = gn(graph)\n",
    "\n",
    "  gn = GraphConvolution(\n",
    "      update_node_fn=hk.Linear(2))\n",
    "  graph = gn(graph)\n",
    "  return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525be495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21324b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/deepmind/jraph/blob/master/jraph/_src/models.py#L506\n",
    "def GraphConvolution(update_node_fn: Callable,\n",
    "                     aggregate_nodes_fn: Callable = jax.ops.segment_sum,\n",
    "                     add_self_edges: bool = False,\n",
    "                     symmetric_normalization: bool = False) -> Callable:\n",
    "  \"\"\"Returns a method that applies a Graph Convolution layer.\n",
    "\n",
    "  Graph Convolutional layer as in https://arxiv.org/abs/1609.02907,\n",
    "  NOTE: This implementation does not add an activation after aggregation.\n",
    "  If you are stacking layers, you may want to add an activation between\n",
    "  each layer.\n",
    "  Args:\n",
    "    update_node_fn: function used to update the nodes. In the paper a single\n",
    "      layer MLP is used.\n",
    "    aggregate_nodes_fn: function used to aggregates the sender nodes.\n",
    "    add_self_edges: whether to add self edges to nodes in the graph as in the\n",
    "      paper definition of GCN. Defaults to False.\n",
    "    symmetric_normalization: whether to use symmetric normalization. Defaults to\n",
    "      True.\n",
    "\n",
    "  Returns:\n",
    "    A method that applies a Graph Convolution layer.\n",
    "  \"\"\"\n",
    "\n",
    "  def _ApplyGCN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "    \"\"\"Applies a Graph Convolution layer.\"\"\"\n",
    "    nodes, _, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    # First pass nodes through the node updater.\n",
    "    nodes = update_node_fn(nodes)\n",
    "    # Equivalent to jnp.sum(n_node), but jittable\n",
    "    total_num_nodes = tree.tree_leaves(nodes)[0].shape[0]\n",
    "    if add_self_edges:\n",
    "      # We add self edges to the senders and receivers so that each node\n",
    "      # includes itself in aggregation.\n",
    "      # In principle, a `GraphsTuple` should partition by n_edge, but in\n",
    "      # this case it is not required since a GCN is agnostic to whether\n",
    "      # the `GraphsTuple` is a batch of graphs or a single large graph.\n",
    "      conv_receivers, conv_senders = add_self_edges_fn(receivers, senders,\n",
    "                                                       total_num_nodes)\n",
    "    else:\n",
    "      conv_senders = senders\n",
    "      conv_receivers = receivers\n",
    "\n",
    "    # pylint: disable=g-long-lambda\n",
    "    if symmetric_normalization:\n",
    "      # Calculate the normalization values.\n",
    "      count_edges = lambda x: jax.ops.segment_sum(\n",
    "          jnp.ones_like(conv_senders), x, total_num_nodes)\n",
    "      sender_degree = count_edges(conv_senders)\n",
    "      receiver_degree = count_edges(conv_receivers)\n",
    "\n",
    "      # Pre normalize by sqrt sender degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: x * jax.lax.rsqrt(jnp.maximum(sender_degree, 1.0))[:, None],\n",
    "          nodes,\n",
    "      )\n",
    "      # Aggregate the pre-normalized nodes.\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "      # Post normalize by sqrt receiver degree.\n",
    "      # Avoid dividing by 0 by taking maximum of (degree, 1).\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x:\n",
    "          (x * jax.lax.rsqrt(jnp.maximum(receiver_degree, 1.0))[:, None]),\n",
    "          nodes,\n",
    "      )\n",
    "    else:\n",
    "      nodes = tree.tree_map(\n",
    "          lambda x: aggregate_nodes_fn(x[conv_senders], conv_receivers,\n",
    "                                       total_num_nodes), nodes)\n",
    "    # pylint: enable=g-long-lambda\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  return _ApplyGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c53717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c9b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT modified\n",
    "def GNN(add_self_edges: bool = False ) -> Callable:\n",
    " \n",
    " \n",
    "\n",
    "  def _ApplyGNN(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    " \n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "    \n",
    "    \n",
    "    #Equivalent to the sum of n_node, but statically known.\n",
    "    try:\n",
    "      sum_n_node = nodes.shape[0]\n",
    "    except IndexError:\n",
    "      raise IndexError('GAT requires node features')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # position of the nodes are the last 3 entries in the features matrix\n",
    "    \n",
    "    distances = jnp.linalg.norm( nodes[:,:3][senders] - nodes[:,:3][receivers], axis=-1)\n",
    "    node_feat = nodes[:,3:]\n",
    "    \n",
    "    input_dim = nodes[:,3:].shape[-1]\n",
    "    \n",
    "    \n",
    "    concat_feats = jnp.concatenate([ node_feat[senders], node_feat[receivers], distances.reshape((-1,1))**2 ], axis=-1)\n",
    "    m_ij = f_e(concat_feats, input_dim)\n",
    "    \n",
    "    \n",
    "    # predict edges\n",
    "    e_ij = f_inf(m_ij, input_dim)\n",
    "    \n",
    "    m_i = jraph_utils.segment_sum( e_ij*m_ij, receivers, num_segments=sum_n_node)\n",
    "\n",
    "    concat_for_hl1_i =  jnp.concatenate([node_feat,  m_i], axis=-1)\n",
    "    \n",
    "    \n",
    "    new_node_feat = node_feat + f_h(concat_for_hl1_i, input_dim) #h^(l+1)_i    f_h is the node update func\n",
    "\n",
    "     \n",
    "    position_mlp = f_x(concat_feats, input_dim)\n",
    "    prefac_xi_xj = (nodes[:,:3][senders] - nodes[:,:3][receivers])/ (distances.reshape((-1,1))+1)\n",
    "    \n",
    "    sum_xi_xj =  jraph_utils.segment_sum( prefac_xi_xj*position_mlp, receivers, num_segments=sum_n_node)\n",
    "    #sum_xi_xj = jnp.einsum('ij,ij ->i', prefac_xi_xj, position_mlp ) #f_x\n",
    "    \n",
    "    x_Lplus1_i = nodes[:,:3] + sum_xi_xj\n",
    "    \n",
    "    nodes = jnp.concatenate([x_Lplus1_i, new_node_feat],axis=-1)\n",
    "    #print(nodes.shape)\n",
    "    return graph._replace(nodes=nodes)\n",
    "\n",
    "  # pylint: enable=g-long-lambda\n",
    "  return _ApplyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_expand_MLP(feats: jnp.ndarray, out_dim) -> jnp.ndarray:\n",
    "  \"\"\"Node update function for graph net.\"\"\"\n",
    "  net = hk.Linear( out_dim )(feats)\n",
    "  return net\n",
    "\n",
    "\n",
    "def dim_expand(graph: jraph.GraphsTuple, out_dim) -> jraph.GraphsTuple:\n",
    "\n",
    "    nodes, edges, receivers, senders, _, _, _ = graph\n",
    "\n",
    "    node_feat = nodes[:,3:]\n",
    "    in_dim = nodes[:,3:].shape[-1]\n",
    "    new_node = dim_expand_MLP(node_feat, out_dim)\n",
    "    \n",
    "    nodes = jnp.concatenate([nodes[:,:3], new_node],axis=-1)\n",
    "\n",
    "     \n",
    "    return graph._replace(nodes=nodes)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d91ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
